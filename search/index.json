[{"content":"Proximal Policy Optimization Proximal Policy Optimization (PPO) 的 objective function:\n$$\\max_{\\theta}\\left( \\mathbb{E}_t\\left[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} A^{\\pi_{\\text{old}}}_t\\right] - \\beta \\mathbb{E}_t\\left[\\textbf{KL}[\\pi_{\\theta_{\\text{old}}}(\\cdot|s_t), \\pi_{\\theta}(\\cdot|s_t)]\\right]\\right)$$\n其中第二项是为了保证 $\\pi$ 在更新时不要发生太大的变化，这样可以让训练更稳定。\nAdaptive KL Penalty 系数 $\\beta$ 是可以调节的，用于调节 KL penalty 的比重，也可以 adaptively 调节。比如令\n$$d = \\mathbb{E}_t\\left[\\textbf{KL}[\\pi_{\\theta_{\\text{old}}}(\\cdot|s_t), \\pi_{\\theta}(\\cdot|s_t)]\\right]$$\n再设定一个目标 $d_{\\text{targ}}$，\n当 $d\u0026lt;d_{\\text{targ}}/1.5$ 时，$\\beta\\leftarrow \\beta/2$。 当 $d\u0026gt;d_{\\text{targ}}\\times 1.5$ 时，$\\beta\\leftarrow \\beta\\times 2$。 PPO with Clipped Objective 为了让训练更加稳定，对 $r_t=\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}}$ 进行 clip：\n$$L^{\\text{CLIP}}=\\mathbb{E}_t\\left[\\min \\left(r_t A^{\\pi_{\\text{old}}}_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A^{\\pi_{\\text{old}}}_t\\right)\\right]$$\nPPO in Practice 一般在使用时，会在 CLIP 项后额外加两项，而 KL divergence 则作为调节 learning rate 的判据 （KL小就调大 lr，KL大就调小 lr）。\n","date":"2024-08-31T07:00:00Z","permalink":"https://suz-tsinghua.github.io/p/drl-notes-6/","title":"Deep Reinforcement Learning Lecture 6"},{"content":"Actor-Critic Off-Policy Policy Gradient 上节中我们提到，REINFORCE 中：\n$$\\nabla_{\\theta}J(\\theta)=\\mathbb{E}_{\\tau}\\left[R(\\tau)\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi(a_t|s_t,\\theta)\\right]$$\n等式右边的期望 $\\mathbb{E}_{\\tau}$ 是对当前 policy $\\pi_{\\theta}$ 而言的。因此在更新 policy 时只能采用当前 policy 跑出来的 trajectory 数据，不能用过往 policy 或者任意别的 policy 跑出来的 trajectory。而这使得 REINFORCE 的训练样本量较小，训练速度较慢。我们现在希望用某种方法使得 REINFORCE 能够使用别的 trajectory 数据。\nImportance Sampling 假设现在有一个随机变量 $x$，其概率密度函数 $p(x)$，我们希望估计函数 $f(x)$ 的均值 $\\mathbb{E}_{x\\sim p(x)}[f(x)]$。如果我们拥有一些符合概率密度函数 $p(x)$ 的 samples $\\{x_i\\}$ 则可以通过 Monte-Carlo 的办法进行直接估计：\n$$\\mathbb{E}_{x\\sim p(x)}[f(x)]\\approx \\frac{1}{|{x_i}|} \\sum_{i} f(x_i)$$\n但实际情况是，我们拥有的 samples $\\{x_i\\}$ 并不符合 $p(x)$，而是符合另一概率密度函数 $q(x)$，则可以用以下 Monte-Carlo 的办法估计：\n$$\\mathbb{E}_{x\\sim p(x)}[f(x)]=\\int f(x) p(x) dx =\\int f(x) \\frac{p(x)}{q(x)} q(x) dx \\approx \\frac{1}{|{x_i}|} \\sum_{i} f(x_i) \\frac{p(x_i)}{q(x_i)}$$\n这种方法称为 Importance Sampling。\nOff-Policy PG 利用 Importance Sampling，在更新策略时我们可以使用别的 policy 跑出来的 trajectory，假设我们利用 $\\pi_{\\theta^{\\prime}}$ 跑出来的数据来更新 $\\pi_{\\theta}$。\n$$\\begin{align*} \\nabla_{\\theta}J(\\theta)\u0026amp;=\\mathbb{E}_{\\tau\\sim p_{\\theta}}\\left[R(\\tau)\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi_{\\theta}(a_t|s_t)\\right]\\\\ \u0026amp;=\\mathbb{E}_{\\tau\\sim p^{\\prime}_{\\theta}}\\left[\\frac{p_{\\theta}(\\tau)}{p_{\\theta}^{\\prime}(\\tau)}R(\\tau)\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi_{\\theta}(a_t|s_t)\\right]\\\\ \\end{align*}$$\n其中：\n$$\\frac{p_{\\theta}(\\tau)}{p_{\\theta}^{\\prime}(\\tau)}=\\frac{\\mu(s_0)\\prod_{t=0}^{T-1}\\left[\\pi_{\\theta}(a_t|s_t)\\mathbb{P}(s_{t+1},r_t|s_t, a_t)\\right]}{\\mu(s_0)\\prod_{t=0}^{T-1}\\left[\\pi_{\\theta^{\\prime}}(a_t|s_t)\\mathbb{P}(s_{t+1},r_t|s_t, a_t)\\right]}=\\frac{\\prod_{t=0}^{T-1}\\left[\\pi_{\\theta}(a_t|s_t)\\right]}{\\prod_{t=0}^{T-1}\\left[\\pi_{\\theta^{\\prime}}(a_t|s_t)\\right]}$$\nActor-Critic 事实上，我们除了可以选取 $\\mathbb{E}_{\\tau}\\left[R(\\tau)\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi(a_t|s_t,\\theta)\\right]$ 来作为 $\\nabla_{\\theta}J(\\theta)$，还有许多别的选择：\n我们接下来考虑形式：\n$$\\nabla_{\\theta}J(\\theta)=\\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^{T-1}(Q^{\\pi}(s_t, a_t)-V^{\\pi}(s_t))\\nabla_{\\theta}\\log\\pi(a_t|s_t,\\theta)\\right]$$\n其中 $V^{\\pi}(s_t)$ 即为上一讲说到的 baseline，可以起到降低 variance 的作用。\n由于 $V^{\\pi}$ 并不能轻易得到，我们在原先已有神经网络 $\\pi_{\\theta}$ 的基础上再加一个神经网络 $V^{\\pi}_{\\phi}$。用新的神经网络 $V^{\\pi}_{\\phi}$ 去拟合 $V^{\\pi}$。我们需要同时学习这两个神经网络。直观上来看，由于新的神经网络表征的是 value function，它相当于对原有的 policy 网络 $\\pi_{\\theta}$ 进行评判，因此我们把这类方法称为 actor-critic，$\\pi_{\\theta}$ 是 actor，$V^{\\pi}_{\\phi}$ 是 critic。\nBatch Actor-Critic Algorithm Batch Actor-Critic Algorithm (Without Discount):\n在机器人或别的智能体上跑 $\\pi_{\\theta}$，得到很多 trajectories 数据。 用收集到的数据更新 $V^{\\pi}_{\\phi}$ 网络。 对收集到的每个 transition $(s, a, r, s^{\\prime})$， 计算出估计的 advantage function $A^{\\pi}(s,a)=r(s,a)+V^{\\pi}_{\\phi}(s^{\\prime})-V^{\\pi}_{\\phi}(s)$。 算出梯度 $\\nabla_{\\theta}J(\\theta)\\approx \\frac{1}{N} \\sum_i A^{\\pi}(s_i,a_i)\\nabla_{\\theta} \\log\\pi_{\\theta}(a_i|s_i)$。 更新参数 $\\theta\\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$。 但如果我们不想收集完一整个 episode 的数据再更新，而是想要每个 step 都能更新一次怎么办？我们可以用 TD update 来更新 $V^{\\pi}_{\\phi}$，并且再加入 discount。\nBatch Actor-Critic Algorithm:\n在机器人或别的智能体上跑 $\\pi_{\\theta}$，得到一个 step 的数据 $(s, a, r, s^{\\prime})$。 以 $r+\\gamma V^{\\pi}_{\\phi}(s^{\\prime})$ 为目标更新 $V^{\\pi}_{\\phi}(s)$。 计算出估计的 advantage function $A^{\\pi}(s,a)=r(s,a)+V^{\\pi}_{\\phi}(s^{\\prime})-V^{\\pi}_{\\phi}(s)$。 算出梯度 $\\nabla_{\\theta}J(\\theta)\\approx \\frac{1}{N} \\sum_i A^{\\pi}(s_i,a_i)\\nabla_{\\theta} \\log\\pi_{\\theta}(a_i|s_i)$。 更新参数 $\\theta\\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$。 DDPG 我们知道，在 DQN 中，$Q$ 的更新需要用到 target: $r+\\gamma\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})$。由于要对 $a^{\\prime}$ 取最大值，DQN 不能直接被应用到 continuous action 的情况。那是否有办法将其应用到 continuous action 的情况呢？提供几种可能的解决方法：\nSolution 1: 取 $N$ 个 $a$ 的samples，取其中的最大值，即认为 $\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})\\approx \\max{Q(s^{\\prime},a^{\\prime}_1), Q(s^{\\prime},a^{\\prime}_2), \u0026hellip;, Q(s^{\\prime},a^{\\prime}_N)}$。但这样取出来的值可能和真正的最大值差的非常大，尤其是 action space 很大的时候。 Solution 2：参数化 $Q$ 时用 $Q_{\\phi}(s,a)=-\\frac{1}{2}(a-\\mu_{\\phi}(s))^{T}P_{\\phi}(s)(a-\\mu_{\\phi}(s))+V_{\\phi}(s)$。这样可以直接得出 $\\arg\\max Q$ 和 $\\max Q$，但由于限制了表达形式，会让 $Q$ less expressive。 Solution 3：每次求 $\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})$ 都做 gradient ascent。这样太慢了。 Solution 4：直接学习一个 $\\mu_{\\theta}(s)$ s.t. $\\mu_{\\theta}(s)\\approx\\argmax_a Q(s,a)$。 顺着 solution 4，我们得到一个新的算法 Deep Deterministic Policy Gradient (DDPG)。\nDDPG:\n每一步的 action 从 $\\mu_{\\theta}(s)$+noise 中 sample，得到很多 transitions $(s_i, a_i, s_i^{\\prime}, r_i)$，将他们存到 buffer $\\mathcal{B}$ 中。 随机从 $\\mathcal{B}$ 中 sample 得到 $N$ 个 transitions $(s_j, a_j, s_j^{\\prime}, r_j)$。 用 target network 得到 $y_j=r_j+\\gamma Q_{\\phi^{\\prime}}(s_j^{\\prime},\\mu_{\\theta^{\\prime}}(s_j^{\\prime}))$。 Update $Q_{\\phi}$ to minimize the loss funciton $L=\\frac{1}{N}\\sum_j(y_j-Q_{\\phi}(s_j, a_j))^2$。 Update the actor network $\\mu_{\\theta}(s)$ by $\\theta\\leftarrow \\theta+\\beta\\frac{1}{N}\\sum_j\\frac{d\\mu}{d\\theta}(s_j)\\frac{dQ}{da}(s_j, \\mu_{\\theta}(s_j))$。 用不管什么方法，更新 target network。 TD3 Twin Delayed DDPG (TD3) 通过在 DDPG 的基础上进行改进得来。它多加了 3 个技巧：\nClipped Double Q-learning 这是说，采用两套 networks $(Q_{\\phi_1}, \\mu_{\\theta_1}), (Q_{\\phi_2}, \\mu_{\\theta_2})$。在更新两套 networks 时，分别采用：\n$$y_1=r+\\gamma \\min_i Q_{\\phi_i}(s^{\\prime}, \\mu_{\\theta_1}(s^{\\prime}))$$ $$y_2=r+\\gamma \\min_i Q_{\\phi_i}(s^{\\prime}, \\mu_{\\theta_2}(s^{\\prime}))$$\n这有助于解决 overestimation of $Q$。\nDelayed Policy Updates 因为 $Q$ network 和 $\\mu$ network 是耦合的，也就是说当 policy 较差的时候， $Q$ 由于 overestimation 也会变差；而 $Q$ 变差了又会得到更差的 policy。因此降低 $\\mu$ network 的更新频率，每次先固定 $\\mu$，更新多次 $Q$ 直到其较为稳定，这时再去更新 $\\mu$。\nTarget Policy Smoothing 计算 $y$ 时，将 $\\mu_{\\theta^{\\prime}}(s^{\\prime})$ 替换为 $clip(\\mu_{\\theta^{\\prime}}(s^{\\prime})+clip(\\epsilon, -c, c), a_{low}, a_{high})$。这样可以让 policy 函数更加 smooth，这是基于相近的 state 本就应该有相近的 action 的假设的。\n","date":"2024-07-21T07:00:00Z","permalink":"https://suz-tsinghua.github.io/p/drl-notes-5/","title":"Deep Reinforcement Learning Lecture 5"},{"content":"Advanced DQN and Policy Gradient DQN Variants 本节我们先来介绍一下 DQN 的几种 variants。\nDouble DQN 在加入 Target Network 后，DQN 的 loss function 为：\n$$\\mathcal{L}(w)=\\left(r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w^-)-Q(s_t, a_t; w)\\right)^2$$\n在 $Q(s, a; w)$ 收敛的过程中，我们可以将其视为一个随机变量，其值随着 $w$ 的变化而上下浮动，以真值 $\\hat{Q}(s,a)$ 为期望。由于采用了 Experience Replay，同一个 transition 会在 $w$ 不同的时候被多次计算，我们实际上在以：\n$$\\mathbb{E}_{w^-}\\left[r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w^-)\\right]$$\n为 target 优化 $Q(s_t, a_t; w)$。根据不等式：\n$$\\mathbb{E}(\\max(X_1, X_2, \u0026hellip;))\\geq \\max(\\mathbb{E}(X_1), \\mathbb{E}(X_2), \u0026hellip;)$$\n我们能得到：\n$$\\mathbb{E}_{w^-}\\left[r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w^-)\\right]\\geq r_t+\\gamma \\max_{a^{\\prime}} \\hat{Q}(s_{t+1}, a^{\\prime})$$\n不等式右边才是我们想要的 target，因此可见 DQN 往往会存在 overestimation 的问题。我们使用的 target 会比实际的 target 大。\n解决方法是我们用当前的 $Q$-network $w$ 来选择最好的 action，但用之前的 $Q$-network $w^-$ 来 evaluate action，即：\n$$\\mathcal{L}(w)=\\left(r_t+\\gamma Q\\left(s_{t+1}, \\argmax_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w); w^-\\right)-Q(s_t, a_t; w)\\right)^2$$\n这个方法被称为 Double DQN。\n显然，$Q\\left(s_{t+1}, \\argmax_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w); w^-\\right)\\leq \\max_{a^{\\prime}}Q\\left(s_{t+1}, a^{\\prime}; w^-\\right)$。因此相较于 DQN，Double DQN 可以在一定程度上缓解 overestimation 带来的影响。并且当 $w^-$ 和 $w$ 收敛之后，不等式会取等。\nPrioritized Experience Replay 由于 DQN 对 replay buffer 里的 samples 学习好坏程度并不相同，有些 samples 学习得较好，有些则学习得较差。如果每次从 buffer 中取 samples 都完全随机的话，有可能会 overfit 到一些 samples 上，而另一些 samples 则几乎没学到。这样就会让训练效果变差，训练速度变慢。\n解决方法是每次根据学习得好坏来对 replay buffer 进行加权采样。Store experience in priority queue according to DQN error：\n$$\\left|r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w^-)-Q(s_t, a_t; w)\\right|$$\n对 error 进行一些函数变换之后作为 sample 的权重。这个方法被称作 Prioritized Experience Replay。\nDueling DQN 我们定义 advantage function：\n$$A^{\\pi}(s,a)=Q^{\\pi}(s,a)-V^{\\pi}(s)$$\n用来衡量 state $s$ 下，action $a$ 相较于其他的 actions 好多少。\nDueling DQN 将 $Q$-network 分为两个 channel，一个 channel 用来学习只与 state 有关的 value function $V(s;w)$；另一个 channel 学习 advantage function $A(s,a;w)$。计算 $Q$ 时将二者加起来：\n$$Q(s,a;w)=V(s;w)+A(s,a;w)$$\n其中 $V(s;w)$ 与 $A(s,a;w)$ 共享部分结构，如下图所示。网络的输入是 $s$ 的 vector representation，输出是大小为 $|A|$ 的向量。$V(s;w)$ 的输出在图中呈现为单个数。\n$n$-Step Return 我们先前一直用的都是 1-step return 作为 target，当然也可以用 $n$-step return：\n$$\\mathcal{L}(w)=\\left(R_t^{n}+\\gamma^n\\max_{a^{\\prime}}Q(s_{t+n}, a^{\\prime}; w^-)-Q(s_t, a_t; w)\\right)^2$$\nRainbow Rainbow 这篇工作总结比较了 DQN 的许多变种，并同时运用了这些 tricks 来得到较好的效果。\nPolicy Gradient 相较于 $Q$-learning，DQN 已经在一定程度上解决了 $|S|$ 与 $|A|$ 较大的问题，但还不够，从 $Q$ function 得到 policy 需要对 $Q(s,a) \\forall a$ 进行比较，这相当耗时。为了在更大的空间里进行 RL，我们希望直接参数化 policy：\n$$\\pi_{\\theta}(s,a)=\\mathbb{P}[a|s,\\theta]$$\nThree Types of RL Methods RL 可以根据是否学习参数化的 value/$Q$ function，是否学习参数化的 policy 分为三大类\nValue Based: Learn Value/$Q$ Function, Implicit Policy. Policy Based: No Value Function, Learn Policy. Actor-Critic: Learn Value Function, Learn Policy. 我们之前讲的方法都属于 Value Based，这部分会介绍 Policy Based Method，而下一节会讲 Actor-Critic。\nPolicy Parameterization 先讲如何 parameterize $\\pi_{\\theta}$。\nSoftmax Policy $$\\pi_{\\theta}(s,a)\\propto e^{\\phi(s,a)^{\\top}\\theta}$$\n其中 $\\phi(s,a)$ 是 $s,a$ 的 features，也可以用 neural networks 表示。\n定义 score function 为 $\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)$ （后面的推导中会看到为何要这么定义）。那么 softmax policy 的 score function 为：\n$$\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)=\\phi(s,a)-\\mathbb{E}_{\\pi_{\\theta}}[\\phi(s,\\cdot)]$$\nGaussian Policy 一个更广泛运用的 parameterized policy 是 Gaussian Policy，其只适用于 continuous actions。\nAction 的均值为 $\\mu(s)=\\phi(s)^{\\top}\\theta$，其中 $\\phi(s)$ 是 $s$ 的 features，也可以用 neural networks 表示。\nAction 的方差为 $\\sigma^2$，可以是固定的，也可以是 parameterized 的。\nAction 从高斯分布中采样，$a\\sim \\mathcal{N}(\\mu(s), \\sigma^2)$。\nScore function:\n$$\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)=\\frac{(a-\\mu(s))\\phi(s)}{\\sigma^2}$$\nPolicy Objective Functions 既然要学习参数化的 policy $\\pi_{\\theta}(s,a)$，那么该如何衡量 policy 的好坏，如何定义 objective function to maximize。一种可行的定义方法是 average reward per time-step：\n$$J_{avR}(\\theta)=\\sum_{s}d^{\\pi_{\\theta}}(s)\\sum_a\\pi_{\\theta}(s,a)r(s,a)$$\n这里，$d^{\\pi_{\\theta}}$ 是 MDP 在 $\\pi_{\\theta}$ 下的稳定分布。\n不过，在 model-free 的情况下我们无法得知 $d^{\\pi_{\\theta}}$，另一种较为直接的办法就是 maximize expected return of trajectories：\n$$J(\\theta)=\\mathbb{E}_{\\tau}[R(\\tau)]$$\nGradient of Objective Functions 为了更新 policy，我们需要对 objective funcion 进行 gradient ascent：\n$$\\Delta \\theta = \\alpha \\nabla_{\\theta}J(\\theta)$$\n接下来就对这个导数进行推导。\n$$\\begin{align*} \\nabla_{\\theta}J(\\theta)\u0026amp;=\\nabla_{\\theta}\\sum_{\\tau}\\mathbb{P}(\\tau|\\theta)R(\\tau)\\\\ \u0026amp;=\\sum_{\\tau}R(\\tau)\\nabla_{\\theta}\\mathbb{P}(\\tau|\\theta)\\\\ \u0026amp;=\\sum_{\\tau}R(\\tau)\\mathbb{P}(\\tau|\\theta)\\frac{\\nabla_{\\theta}\\mathbb{P}(\\tau|\\theta)}{\\mathbb{P}(\\tau|\\theta)}\\\\ \u0026amp;=\\sum_{\\tau}R(\\tau)\\mathbb{P}(\\tau|\\theta)\\nabla_{\\theta}\\log\\mathbb{P}(\\tau|\\theta)\\\\ \u0026amp;=\\mathbb{E}_{\\tau}\\left[R(\\tau)\\nabla_{\\theta}\\log\\mathbb{P}(\\tau|\\theta)\\right] \\end{align*}$$\n假设 $\\tau = s_0, a_0, r_0, s_1, a_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}, s_T$，那么有：\n$$\\mathbb{P}(\\tau|\\theta)=\\mu(s_0)\\prod_{t=0}^{T-1}\\left[\\pi(a_t|s_t,\\theta)\\mathbb{P}(s_{t+1},r_t|s_t, a_t)\\right]$$\n其中 $\\mu(s_0)$ 是 initial distribution。取对数：\n$$\\log\\mathbb{P}(\\tau|\\theta)=\\log\\mu(s_0)+\\sum_{t=0}^{T-1}\\left[\\log\\pi(a_t|s_t,\\theta)+\\log\\mathbb{P}(s_{t+1},r_t|s_t, a_t)\\right]$$\n求导：\n$$\\nabla_{\\theta}\\log\\mathbb{P}(\\tau|\\theta)=\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi(a_t|s_t,\\theta)$$\n代入原式中，得到：\n$$\\nabla_{\\theta}J(\\theta)=\\mathbb{E}_{\\tau}\\left[R(\\tau)\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi(a_t|s_t,\\theta)\\right]$$\n在已知 $\\tau$ 以及 $\\pi$ 的情况下，括号中的内容可以直接算出。Expectation 则可以用 MC 来 estimate。\nREINFORCE 根据以上的推导，得出了一个 MC-based, policy-based RL method：\nREINFORCE:\nsample {$\\tau^i$} from $\\pi_{\\theta}$ (run the policy) $\\nabla_{\\theta}J(\\theta)\\approx\\frac{1}{|\\{\\tau^i\\}|}\\sum_{i}\\left[R(\\tau_i)\\sum_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t^i|s_t^i)\\right]$ $\\theta\\leftarrow \\theta+\\alpha\\nabla_{\\theta}J(\\theta)$ REINFORCE 的一个问题是，其只能用当前 policy 跑出来的 trajectory 进行策略的更新，即它是 on-policy 的。用别的 policy 跑出来的 trajectory 进行更新会导致 MC estimate 错误。这样一种性质使得 REINFORCE 的训练非常慢。下一节课会讲如何改进得到 off-policy Policy Gradient。\nBaseline with REINFORCE REINFORCE 中采用 MC 的方法进行 estimate，会导致 high variance，我们试图减小 variance。先不考虑整个 trajectory，只考虑一个 step。即减小 $r(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)$ 的方差。\n注意到，$\\forall B(s)$ irrelevant with $a$：\n$$\\begin{align*} \\mathbb{E}_{\\pi_{\\theta}}\\left[B(s)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]\u0026amp;=\\sum_{a}\\pi_{\\theta}(s,a)B(s)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\\\ \u0026amp;=\\sum_{a}B(s)\\nabla_{\\theta}\\pi_{\\theta}(s,a)\\\\ \u0026amp;=B(s)\\nabla_{\\theta}\\sum_{a}\\pi_{\\theta}(s,a)\\\\ \u0026amp;=B(s)\\nabla_{\\theta}1\\\\ \u0026amp;=0 \\end{align*}$$\n故而用 $\\mathbb{E}_{\\pi_{\\theta}}\\left[\\left(r(s,a)-B(s)\\right)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]$ 来代替 $\\mathbb{E}_{\\pi_{\\theta}}\\left[r(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]$ 并不会影响 expectation。并且我们可以证明，当 $B(s)$ 合适的时候，$\\text{Var}_{\\pi_{\\theta}}\\left[\\left(r(s,a)-B(s)\\right)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right] \u0026lt; \\text{Var}_{\\pi_{\\theta}}\\left[r(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]$：\n$$\\begin{align*} \u0026amp;\\text{Var}_{\\pi_{\\theta}}\\left[\\left(r(s,a)-B(s)\\right)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right] - \\text{Var}_{\\pi_{\\theta}}\\left[r(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]\\\\ =\u0026amp;\\mathbb{E}_{\\pi_{\\theta}}\\left[\\left(r(s,a)-B(s)\\right)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]^2 - \\mathbb{E}_{\\pi_{\\theta}}\\left[r(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]^2\\\\ =\u0026amp;\\mathbb{E}_{\\pi_{\\theta}}\\left[\\left(B(s)^2-2r(s,a)B(s)\\right)\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right)^2\\right]\\\\ =\u0026amp;B(s)^2\\mathbb{E}_{\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]^2-2B(s)\\mathbb{E}_{\\pi_{\\theta}}\\left[r(s,a)\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right)^2\\right] \\end{align*}$$\n这是个二次函数，当 $B(s)=\\frac{\\mathbb{E}_{\\pi_{\\theta}}\\left[r(s,a)\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right)^2\\right]}{\\mathbb{E}_{\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)\\right]^2}$ 时，两个 Variance 的差取最小值，小于0。\n$B(s)$ 称为 baseline。在 REINFORCE 中，选择一个合适的 constant baseline 也可以 reduce variance：\n$$\\nabla_{\\theta}J(\\theta)=\\mathbb{E}_{\\tau}\\left[(R(\\tau)-B)\\nabla_{\\theta}\\sum_{t=0}^{T-1}\\log\\pi(a_t|s_t,\\theta)\\right]$$\n","date":"2024-07-19T08:00:00Z","permalink":"https://suz-tsinghua.github.io/p/drl-notes-4/","title":"Deep Reinforcement Learning Lecture 4"},{"content":"Q-Learning and Deep Q-Learning From Estimation to Policy Policy Evaluation for $Q(s,a)$ 在上一节中，我们学习了在 model-free 的情况下，如何进行 Policy Evaluation。我们知道，Policy Iteration 除了 Policy Evaluation，还有一步 Policy Improvement。这里先来看下如何在 model-free 的情况下进行 Policy Improvement。最初的 Policy Iteration 中 Policy Improvement 的方法是：\n$$\\pi(s)=\\argmax_a \\sum_{s^{\\prime}}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V(s^{\\prime})]$$\n但这种方法要求知道 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$。另一种方法是，在 Policy Evaluation 的时候，我们并不去计算 $V(s)$，而是计算 $Q(s,a)$。这样就可以直接通过：\n$$\\pi(s)=\\argmax_a Q(s,a)$$\n来进行 Policy Improvement。$Q(s,a)$ 也满足 Bellman Equation：\n$$Q(s,a)=\\sum_{s^{\\prime}}T(s,a,s^{\\prime})(R(s,a,s^{\\prime})+\\gamma\\max_{a^{\\prime}}Q(s^{\\prime}, a^{\\prime}))$$\n因此用 MC、TD 对 $Q$ 进行 Policy Evaluation 的方法与对 $V$ 进行 Policy Evaluation 的方法类似，这里不再详细阐述。\nExploration v.s. Exploitation 当然，完全根据以上得到的 $Q$ 进行 Policy Iteration 是不可取的。我们来看这样一个简单的例子：\n假设一个 MDP 只有一个 state $s$，在这个 state 上有两个 actions $a_1, a_2$。$R(s,a_1)$ 有 100 个样本，均取值为 1；而 $R(s,a_2)$ 仅有一个样本，取值为 0。那么根据以上的 Policy Iteration，得出的 policy 会永远选择 $a_1$。但实际上由于选择 $a_2$ 的样本数较少，我们不能确定 $\\mathbb{E}(R(s,a_2))\u0026lt;\\mathbb{E}(R(s,a_1))$。所以只用以上的方法进行 RL，很可能无法学到 optimal policy。有时候也需要选择当前看来并不是最优的 action 来拓展眼界。\n永远选择当前最优策略的行动方法称为 exploitation，而在上面所说的例子中，我们可以在某些时候选择当前看来并不是最好的 solution $a_2$，这种行为叫做 exploration。\nExploration: Get more information about the world. 可能有些更好地选择 agent 之前并不知道，它需要勇于尝试未知的事物才能得到更好的 reward。 Exploitation: To try to get reward. RL 的目的毕竟还是得到更高的 reward，在大多数情况下，我们更希望是一个“保守派”，选择目前最优的策略以期得到较高的 reward。 剩下的问题就是，该在什么时候选择 exploration，什么时候选择 exploitation，这种决定何时选择 exploration 何时选择 exploitation 的策略被称为 exploration/exploitation policy。 一个数学上十分简单的 exploration/exploitaton policy 是 $\\epsilon$-greedy exploration，即有 $1-\\epsilon$ 的概率选择 greedy action (exploitation)，有 $\\epsilon$ 的概率 act randomly (exploration)。由于 exploration 的时候也有可能选到 greedy action，policy 可以被写成：\n$$\\pi(a|s)=\\begin{cases} \\epsilon/|A|+1-\\epsilon \u0026amp;\\text{if } a=\\argmax_{a^{\\prime}\\in A}Q(s,a^{\\prime})\\\\ \\epsilon/|A| \u0026amp; \\text{otherwise}\\end{cases}$$\n有些 exploration/exploitation policy 具有一种特殊的性质，这种性质被称为 Greedy in the Limit with Infinite Exploration (GLIE)。顾名思义，这类 exploration/exploitation policy 当 explore 得足够多的时候会收敛到 fully exploitation policy。更严谨地说，当所有的 state-action pair 都被 explore 了无数遍时，\n$$\\lim_{k\\to\\infty} N_k(s,a)=\\infty$$\n要求 exploration/exploitation policy 收敛到 greedy policy，\n$$\\lim_{k\\to\\infty}\\pi_k(a|s)=\\mathbf{1}\\left(a=\\argmax_{a^{\\prime}\\in A}Q_k(s,a^{\\prime})\\right)$$\n这里，$k$ 代表走了多少个 step。For example, $\\epsilon$-greedy policy is GLIE if $\\epsilon_k=\\frac{1}{k}$. 另一个 GLIE 的例子是 Boltzman Exploration：\n$$\\pi(a|s)=\\frac{\\exp(Q(s,a)/T)}{\\sum_{a^{\\prime}\\in A}\\exp(Q(s,a^{\\prime})/T)}$$\n要求 $T\\to 0$。\n$Q$ Learning 总结一下之前所学的内容：\n我们先学了 Value Iteration, Policy Iteration 等。 但是由于大部分情况下我们并不知道 transition/reward functions，我们需要 MC, TD 等方法来 estimate value function。 用 value function 来得到 policy 还是需要 transition/reward functions，所以我们用 TD 来 estimate $Q$ 而非 value function。 即使采用了 $Q$ function，我们的学习过程仍然是 passive 的，有些未曾涉足的区域会永远被忽视。因此需要 explore，我们采用某种 exploration/exploitation policy。 通过以上的过程，我们几乎已经创造出了 $Q$ Learning。Formally:\n$Q$-Learning:\nStart with initial $Q$-function (e.g. all zeros). Every time take an action from an exploration/exploitation GLIE policy. This action gives a transition $s_t,a_t,r_t,s_{t+1}$. Perform TD update for the $Q$ function: $$Q(s_t,a_t)\\leftarrow Q(s_t, a_t)+\\alpha\\left(r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime})-Q(s_t, a_t)\\right)$$ $Q$-learning converges to the optimal $Q$-value function in the limit with probability 1, if every state-action pair is visited infinitely often. 即 $Q$-learning 在某条件下一定能收敛到最优解。\n由于每一步的 update 并不依赖真实的 $a_{t+1}$，我们称 $Q$ Learning 是 off-policy 的。$Q$ Learning 的一种 on-policy 的变种是 State-Action-Reward-State-Action (SARSA)，即把每步 update 替换为：\n$$Q(s_t,a_t)\\leftarrow Q(s_t, a_t)+\\alpha\\left(r_t+\\gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)\\right)$$\nFunction Approximation 尽管 $Q$ Learning 非常强大，但它也有其局限性。$Q$ Learning 中，我们需要存储 $Q$ function，其定义域大小正比于 $|S|\\times|A|$，因此 $Q$ Learning 只能适用于 $|S|$ 和 $|A|$ 很小的情况，不能适用于围棋 ($\\sim 10^{170}$ states), 机器人 (continuous state space) 等领域。\n一个解决方法是，我们并不去计算 $V$ 或是 $Q$ 在定义域上每个点的值，而是用一个参数化的函数去 approximate $V, Q$。这样我们就只需要储存函数的参数，而不用存 $V, Q$ 的每一个函数值。\n有许多办法可以 approximate functions，比如 Decision Tree, Linear combinations of features 等。我们着重考虑 differentiable 的方法，比如用 neural networks。\n我们先以 $V$ 为例子，我们的目标是找到一个参数 vector $w$, 使得以下 loss 尽可能小：\n$$J(w)=\\mathbb{E}_{\\pi} \\left[(v_{\\pi}(S)-\\hat{v}(S,w))^2\\right]$$\n其中，$v_{\\pi}$ 是要近似的函数，$\\hat{v}$ 是我们的参数化函数。\nGradient Descent:\n$$\\Delta w = -\\frac{1}{2}\\alpha\\nabla_{w}J(w) = \\alpha \\mathbb{E}_{\\pi}\\left[(v_{\\pi}(S)-\\hat{v}(S,w)) \\nabla_{w} \\hat{v}(S,w)\\right]$$\n添加的系数 $\\frac{1}{2}$ 只是为了把求导出来的 2 给约掉。如果我们只用一个 sample，即 SGD，则式子变为：\n$$\\Delta w = \\alpha (v_{\\pi}(S)-\\hat{v}(S,w)) \\nabla_{w} \\hat{v}(S,w)$$\n其中，$v_{\\pi}(S)$ 可以用上一节中讲的 MC、TD 表示，比如：\n$$v_{\\pi}(S)=G_t\\quad MC$$\n$$v_{\\pi}(S)=R_{t}+\\gamma \\hat{v}(S_{t+1},w)\\quad TD(0)$$\n$$v_{\\pi}(S)=G_t^{\\lambda}\\quad TD(\\lambda)$$\nDeep $Q$ Learning 与 $V$ 一样，$Q$ 也可以被 neural nets approximate。Deep $Q$ Learning 就是 $Q$-learning with non-linear approximators：\nDeep $Q$-Learning:\nStart with initial parameter values. Every time take an action from an exploration/exploitation GLIE policy. This action gives a transition $s_t,a_t,r_t,s_{t+1}$. Perform TD update for the parameters. Do gradient descent on the following loss function: $$\\mathcal{L}(w)=\\left(r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w)-Q(s_t, a_t; w)\\right)^2$$ 与 $Q$-learning 不同，deep $Q$-learning 并没有收敛保证，但 empirically 它是能用的。为了 DQN (Deep Q Network) 能训得更好，还需要加一些其他的 tricks。\nTrick 1. Experience Replay 由于每几个 step 的 states 是类似的，比如 $s_t, s_{t+1}, \u0026hellip;, s_{t+n}$，DQN 在用这些 transition 进行更新时很可能会 overfit 到 state space 的这个区域。我们可以用 Experience Replay 来解决这个问题，即每次得到一个 transition $(s_t, a_t, r_t, s_{t+1})$，都将其存到 replay memory $D$ 中，每次要更新 network 时就从 $D$ 中取一个 mini-batch 出来进行更新。\nTrick 2. Target $Q$ Network 在 DQN 的 loss function 中，我们可以发现每次更新 $w$ 都会同时更新 target $(r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w))$ 和 $Q$-network $(Q(s_t, a_t; w))$。但是同时更新二者容易造成训练不稳定，我们可以让 target 更新得慢一点，即每次更新时用 loss：\n$$\\mathcal{L}(w)=\\left(r_t+\\gamma\\max_{a^{\\prime}}Q(s_{t+1}, a^{\\prime}; w^-)-Q(s_t, a_t; w)\\right)^2$$\n其中，$w^-$ 是较早几个 steps 的参数，等到 $w$ 更新了几个 steps 之后再去更新 $w^-$，而不是每个 step 都去更新 $w^-$。\nTrick 3. Reward Clipping 有些 step 的 reward 非常大，使得 gradient 也很大，容易造成训练不稳定。我们实际计算时，将 reward clip 到某个合适的范围，比如 [-1, 1]。\n","date":"2024-07-18T09:00:00Z","permalink":"https://suz-tsinghua.github.io/p/drl-notes-3/","title":"Deep Reinforcement Learning Lecture 3"},{"content":"Model-Free Estimation: Monte-Carlo and Temporal Difference 前一节我们讲过如何进行 Policy Evaluation，但这是基于我们能知道 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$ 的前提的，这种方法被称为是 model-based 的。由于大部分情况下我们并不知道 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$，我们需要 model-free 的方法来进行 Policy Evaluation。但因为 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$ 往往是带有概率的，model-free 的方法通常只能 estimate value function，而不能精确地进行 Policy Evaluation。显然，在 model-free 的情况下，agent 只能通过与环境交互来得到信息，再利用这些信息得到 value function。\nModel-free Estimation: Monte-Carlo Learning 我们想要做的事情是在已知很多 episodes of experience under policy $\\pi$ 的情况下，学出 $v_{\\pi}$。Episodes of experience 用 trajectories 来表示：\n$$s_1, a_1, r_1, s_2, a_2, r_2, \u0026hellip;, s_T, a_T, r_T, s_{T+1} \\sim \\pi$$\n即在 $s_t$ 状态时做出 action $a_t$，转移到 $s_{t+1}$ 的同时得到 reward $r_t$。定义时刻 $t$ 的 return 为：\n$$G_t=r_{t}+\\gamma r_{t+1} + \\gamma^2 r_{t+2} + \u0026hellip; +\\gamma^{T-t} r_T$$\n那么就可以用 Monte-Carlo 的方法估计出 $v_{\\pi}(s)$：\n$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]$$\n即对任意一个状态 $s$，需要在样本中找到每一个 $s$，计算每一个 $s$ 对应的 return $G_t$，再将其进行平均。实际计算中，可以存这么几个量：\nIncrement counter $N(s)\\leftarrow N(s)+1$ Increment total return $S(s)\\leftarrow S(s)+G_t$ Estimated Value $V(s)=S(s)/N(s)$ 当 $N(s)\\to \\infty$ 的时候，$V(s)\\to v_{\\pi}(s)$。\n存了以上 $N(s)$ 和 $V(s)$ 之后，当获取到新的样本时，就可以通过以下式子直接得出新的 $N(s)$ 和 $V(s)$:\n$$N(s)\\leftarrow N(s)+1$$ $$V(s)\\leftarrow \\frac{(N(s)-1)V(s)+G_t}{N(s)}$$\nMC Learning 的好处在于它非常简单，但是它要求每个 episode sample 都必须要终止，否则无法得到 $G_t$。并且它由于没有利用 Bellman Equation，对样本的利用效率非常低，需要大量样本才能得到比较好的近似值。\nModel-free Estimation: Temporal Difference Learning TD 与 MC 不同，它并不要求每个 episode sample 都是终止的。TD 每得到一个 transition 就会进行一次更新：\n$$V(S_t)\\leftarrow V(S_t)+\\alpha(R_t+\\gamma V(S_{t+1})-V(S_t))$$\n这种更新方法被称为 TD(0)（后面还会介绍 TD($\\lambda$)）。式子中 $\\alpha$ 被称为 learning rate，$R_t+\\gamma V(S_{t+1})$ 被称为 TD target，$\\delta_t=R_t+\\gamma V(S_{t+1})-V(S_t)$ 被称为 TD error。\n相较于 MC，TD 利用了 Bellman Equation。因为当 $\\alpha$ 很小，样本量很大的时候，$V(s)$ 会趋向于平衡点 $\\sum_{s^{\\prime}}T(s,\\pi(s),s^{\\prime})[R(s,\\pi(s),s^{\\prime})+\\gamma V(s^{\\prime})]$，这就是 Bellman Equation 中给定的 $V(s)$。\n对比 MC 与 TD Bias and Variance 先来比较两种方法的 bias 和 variance。MC 中 $G_t$ 是实际值 $v_{\\pi}(s_t)$ 的 unbiased estimate。而 TD 中，尽管 $R_{t+1}+\\gamma v_{\\pi}(s_{t+1})$ 也是 $v_{\\pi}(s_t)$ 的 unbiased estimate，但我们实际计算时不知道 $v_{\\pi}(s_{t+1})$，而是用的当前值 $V(s_{t+1})$。$R_{t+1}+\\gamma V(s_{t+1})$ 是 $v_{\\pi}(s_t)$ 的 biased estimate。\n但另一方面，TD 的 variance 是比 MC 要小的。所以 MC has high variance, zero bias；TD has low variance, some bias。\nInitial Value TD(0) 在计算时需要给定 initial value $V(s)$，由于之后的更新是依赖于当前的 $V(s)$ 的，所以 initial value 如果给得不好会影响 TD(0) 的收敛速度。而 MC 中 initial value 的影响则较小。\nSample Efficiency 由于利用了 Bellman Equation，TD 的对样本的利用较 MC 来说更为充分，所以在样本比较少的时候可以选择用 TD。\nVisualization 三种方法 MC，TD，Value Iteration 的 visualization 如上图所示。可以看出，MC 每次更新利用了一个 trajectory 从头到尾的数据，而 TD 则利用只利用了一个 step 的数据。我们将 MC 与 TD 的更新式子写成类似的形式：\nMC: $V(S_t)\\leftarrow V(S_t)+\\alpha(G_t-V(S_t))$，其中 $G_t=R_{t}+\\gamma R_{t+1} + \\gamma^2 R_{t+2} + \u0026hellip; +\\gamma^{T-t} R_T$, $\\alpha = \\frac{1}{N(S_t)}$。 TD: $V(S_t)\\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_t))$。 我们定义从 $S_t$ 开始走过 $n$ 步的 return，$n$-step return 为： $$G_t^{(n)}=R_{t}+\\gamma R_{t+1} + \u0026hellip; + \\gamma^{n-1} R_{t+n-1} + \\gamma^{n} V(S_{t+n})$$\n那么 MC 可以被写成： $$V(S_t)\\leftarrow V(S_t)+\\alpha(G_t^{(\\infty)}-V(S_t))$$\nTD 可以被写成： $$V(S_t)\\leftarrow V(S_t)+\\alpha(G_t^{(1)}-V(S_t))$$\n当然 $n$ 除了取 1 和 $\\infty$ 还可以取别的值，即 $n$-step TD： $$V(S_t)\\leftarrow V(S_t)+\\alpha(G_t^{(n)}-V(S_t))$$\nTD($\\lambda$): Combination of TD and MC 我们并不好决定在何时用 TD，何时用 MC。如果当前估计的 $V(s)$ 已经很好了，可以用 TD；但如果当前的值不好，用 MC 更合适。但是否有一种方法，可以将 MC 与 TD 的优点结合起来呢？可以采用 TD($\\lambda$)。\n先介绍 $\\lambda$-return $G_{t}^{\\lambda}$，其将所有的 $n$-step return 结合起来：\n$$G_t^{\\lambda}=(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_t^{(n)}$$\n基于 $G_t^{\\lambda}$ 提出 TD($\\lambda$)： $$V(S_t)\\leftarrow V(S_t)+\\alpha(G_t^{\\lambda}-V(S_t))$$\n通过整合所有的 $n$-step return，TD($\\lambda$) 在 bias 和 variance 之间做出了一个折中，使得二者均不会太大。\n可以看出，$\\lambda=0$ 时，TD($\\lambda$) 退化为了 TD(0)。\nComputation Trick 看起来，TD($\\lambda$) 似乎丢失了 TD(0) 一个很好的特性。TD(0) 并不需要一整个 episode 的数据，只需要得知当前的一个 transition 就可以进行一次更新，而 TD($\\lambda$) 似乎需要整个 episode 的数据才能得到 $G_t^{\\lambda}$ 从而完成一次更新。事实上，这个问题可以通过计算上的 trick 来解决。\n正常的计算思路是，在 update $V(S_t)$ 时，去查看所有 $\\tau\u0026gt;t$ 时刻的 transition，然后对 $t$ 时刻的 value function 进行更新。这种方法是 forward 的，但事实上也可以采取 backward 的计算方法，即得到 $\\tau$ 时刻的 transition 时，去更新所有 $t\u0026lt;\\tau$ 时刻的 value function。先重写 $G_t^{\\lambda}-V(S_t)$：\n$$\\begin{align*} G_t^{\\lambda}-V(S_t)=-V(S_t)\u0026amp;+(1-\\lambda)\\lambda^0(R_t+\\gamma V(S_{t+1}))\\\\ \u0026amp;+(1-\\lambda)\\lambda^1(R_t+\\gamma R_{t+1}+\\gamma^2 V(S_{t+2}))\\\\ \u0026amp;+(1-\\lambda)\\lambda^2(R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\gamma^3 V(S_{t+3}))\\\\ \u0026amp;+ \u0026hellip; \\\\ =-V(S_t)\u0026amp;+(\\gamma\\lambda)^0(R_t+\\gamma V(S_{t+1})-\\gamma\\lambda V(S_{t+1}))\\\\ \u0026amp;+(\\gamma\\lambda)^1(R_{t+1}+\\gamma V(S_{t+2})-\\gamma\\lambda V(S_{t+2}))\\\\ \u0026amp;+(\\gamma\\lambda)^2(R_{t+2}+\\gamma V(S_{t+3})-\\gamma\\lambda V(S_{t+3}))\\\\ \u0026amp;+ \u0026hellip; \\\\ =(\\gamma\\lambda)^0\u0026amp;(R_t+\\gamma V(S_{t+1})-V(S_t))\\\\ \u0026amp;+(\\gamma\\lambda)^1(R_{t+1}+\\gamma V(S_{t+2})-V(S_{t+1}))\\\\ \u0026amp;+(\\gamma\\lambda)^2(R_{t+2}+\\gamma V(S_{t+3})-V(S_{t+2}))\\\\ \u0026amp;+ \u0026hellip; \\\\ =\\delta_t\u0026amp;+\\gamma\\lambda\\delta_{t+1}+(\\gamma\\lambda)^2\\delta_{t+2}+ \u0026hellip; \\end{align*}$$\n所以 backward 的计算方法是，在得到 $\\tau$ 时刻的 transition 时，先计算出相应的 $\\delta_{\\tau}$，再对所有 $t\u0026lt;\\tau$ 时刻的 value function 进行一次更新： $$V(S_t)\\leftarrow V(S_t)+\\alpha\\delta_{\\tau}(\\gamma\\lambda)^{\\tau-t}$$\n","date":"2024-07-16T10:00:00Z","permalink":"https://suz-tsinghua.github.io/p/drl-notes-2/","title":"Deep Reinforcement Learning Lecture 2"},{"content":"2024春季学期选了xhz老师的限选课深度强化学习，恰巧也在研究这方面的内容，于是打算写个笔记。\nMDP, Value Iteration and Policy Iteration MDP Basics Definition of MDP Markov Decision Process (MDP) 是一个决策过程数学模型，直观地来说：一个智能体 (agent) 处在一个环境中，环境处于不同的状态 (state)；每一步，agent 可以得知环境的部分或全部 state 信息，这部分信息称为 agent 的观测 (observation)；通过 observation，agent 每一步会作出决策，给出一个动作 (action)；这个动作会影响环境，环境有概率转移到另一个 state；同时，环境根据潜在的奖励函数 (rewards) 来给 agent 提供奖励。Formally:\nDefinition:\nAn MDP is a 4-tuple $(S, A, T, R)$:\n$S$ is a set of states called the state space. $A$ is a set of actions called the action space. $T(s, a, s^{\\prime})=Pr(s_{t+1}=s^{\\prime}|s_t=s, a_t=a)$ is the probability that action $a$ at $s$ leads to $s^{\\prime}$, called the transition fuction (also model or dynamics). $R(s, a, s^{\\prime})$ is the immediate reward received after transitioning from state $s$ to state $s^{\\prime}$, due to action $a$. 当然这只是最简单的定义，还可以根据情况的不同引入额外的东西。比如系统初始状态的分布函数 (initial state distribution)，一个一旦到达就直接停止的结束状态 (terminal state)。再比如某些情况下，环境可能是 partially observable 的，agent 无法观测到环境的整个 state (比如打扑克的时候，你无法看到对方手上的牌，这是 partially observable 的，但是下围棋的时候，你可以看到棋盘上所有的东西，这是 fully observable 的)，此时需要在定义中引入 a set of observations $O$，此时的 MDP 称为 Partially Observable MDP (POMDP)。\n定义中 Markov 的意思是：给定当前状态之后，未来与过去就无关了，即 $Pr(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \u0026hellip;, s_0)=Pr(s_{t+1}|s_t, a_t)$。可以认为过去的信息都被浓缩到当前的 state 中了。\nAn Example 用一个简单的例子来加深理解:\n比如一个 agent 位于这样一个 grid world 中\n每一时刻的 state 就是 agent 的位置。 action 是上下左右。 $T$ 我们定义为，有 80% 的可能，agent 的 transition 与其 action 一致；有 10% 的可能，无论什么 action，agent 都往左；有 10% 的可能，无论什么 action，agent 都往右。 只有在 agent 吃到钻石的时候才会有 reward。 Policy 以上我们主要关注环境，接下来我们看 agent，我们将 agent 从 state/observation 得到 action 的决策称为 policy:\nDefinition:\nPolicy $\\pi$ 是一个条件概率密度函数 $\\pi(a|s)$，表示 agent 在 state $s$ 时采取 action $a$ 的概率。\nUtility RL 的目标是学出一个好的 policy，那么这个 “好的” 该如何进行评价。直观来看，我们可以将 agent 放在某个 initial state，让其根据自己的 policy 进行运动固定的步数，或是等到最后结束。那么问题就到了，agent 跑出的这个序列 $(s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_t, a_t, r_t)$ (称为 trajectory) 该如何评价。我们引入 Reward Hypothesis, 即MDP中所有的目标都应被 reward 定义，而不牵扯到其他的量。那么可以将 trajectory 中的 rewards 单独抽出来 $(r_0, r_1, r_2, \u0026hellip;, r_t)$。我们希望有一个函数 (Utility) 能够将这个 rewards 序列映射成一个标量，这样有助于比较不同 trajectories 的优劣。\n一种方法是直接加起来，即 additive utilities: $U([r_0, r_1, r_2, \u0026hellip;]) = r_0+r_1+r_2 + \u0026hellip;$\n考虑到现实生活中，当下的 reward 往往比之后的 reward 更具有价值 (当下给你一块钱往往优于两天后给你一块钱)，一个更常用的 utility 是 discounted utilities: $U([r_0, r_1, r_2, \u0026hellip;]) = r_0+\\gamma r_1+\\gamma^2 r_2 + \u0026hellip;$。其中 $\\gamma$ 称为 discounted factor。\nOptimal Quantities 直观上定义 optimal quantities:\nOptimal policy: $\\pi^*(s)$ = optimal action from state $s$. Optimal value/utility of a state $s$: $V^*(s)$ = expected utility starting from $s$ and acting optimally. Optimal Q value: $Q^*(s,a)$ = expected utility taking action $a$ from state $s$ and acting optimally. Formally 可以递归地定义这些量：\n$$\\pi^{*}(s)=\\argmax_a Q^{*}(s,a)$$ $$V^{*}(s)=\\max_a Q^{*}(s,a)$$ $$Q^{*}(s,a)=\\sum_{s^{\\prime}}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V^{*}(s^{\\prime})]$$\n从以上两个式子我们可以消去 $Q^{*}$ 得到 $V^{*}$ 满足的等式：\n$$V^{*}(s)=\\max_a \\sum_{s^{\\prime}}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V^{*}(s^{\\prime})]$$\n称为 Bellman Equation。\nValue Iteration RL 的最终目标是得到 $\\pi^{*}$，我们可以利用 $V^{*}$ 来得到 $\\pi^{*}$。一种可行的用来得到 $V^{*}$ 的方法称为 Value Iteration，其利用了 Bellman Equation。\n假设 MDP 在 $k$ 步后结束，定义 $s$ 的 optimal value 为 $V^{*}_k(s)$，那么有：\n$$V_0^{*}(s)=0$$ $$V_{k+1}^{*}(s)=\\max_a \\sum_{s^{\\prime}}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_k^{*}(s^{\\prime})]$$\n迭代计算直至收敛，即可得到 $V_{\\infty}^{*}=V^{*}$。\nVI 有两个问题：\nVI 每一步的时间复杂度为 $O(S^2A)$，因此仅仅适用于 discrete case，并且要求 $S$ 和 $A$ 均比较小，无法适用于连续空间。 Policy 往往会比 Value 收敛得更早，如果能够提前发现 policy 已经收敛会更好。 Policy Iteration Policy Evaluation 上面介绍的 $V^{*}$ 是 optimal policy 的 value function，我们也可以给定一个 policy $\\pi$，计算其对应的 value function $V^{\\pi}$，这个计算过程称为 Policy Evaluation。\n$$V^{\\pi}(s) = \\text{expected total discounted rewards starting in } s \\text{ and following } \\pi$$\n计算过程类似于 Value Iteration，也是从相应的 Bellman Equation 入手进行迭代计算：\n$$V^{\\pi}(s)=\\sum_{s^{\\prime}}T(s,\\pi(s),s^{\\prime})[R(s,\\pi(s),s^{\\prime})+\\gamma V^{\\pi}(s^{\\prime})]$$\nPolicy Evaluation 一步花费时间 $O(S^2)$。\nPolicy Improvement 假设我们知道一个 MDP 的 value function, 如何得到在这个 value function 下的 optimal policy。显然可以在某个状态 $s$ 遍历所有的 action $a$，看哪个 $a$ 的收益最大，即：\n$$\\pi(s)=\\argmax_a \\sum_{s^{\\prime}}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V(s^{\\prime})]$$\n这个过程就是 policy improvement。\nPolicy Iteration 结合 Policy Evaluation 和 Policy Improvement，我们可以用另一种方法（不同于 Value Iteration）来得到 $\\pi^*$。\n循环以下两步直至 policy 收敛：\nStep 1: Policy Evaluation。对当前的 policy 进行 policy evaluation。 Step 2: Policy Improvement。对 Step 1 中得到的 value function 进行 policy improvement，得到新的 policy。 这就是 Policy Iteration。PI 也可以得到 optimal $\\pi^*$，并且在一些情况下比 VI 收敛得更快。\nMDP to Reinforcement Learning 不管是 VI 还是 PI，都要求我们知道 MDP 的 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$。但是在现实中的大部分情况，我们并不能准确地知道 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$，尤其是 $T$，因此需要引入 RL。\nRL 的主要思想是：\n环境会为 agent 的 action 提供 reward 进行反馈。 Agent 的所有 utility 都被 reward 定义。 Agent 的目标是 maximize expected rewards。 学习只能基于 agent 获取到的 observations, actions, rewards 等信息（不知道真实的 $T(s,a,s^{\\prime})$ 和 $R(s,a,s^{\\prime})$）。 ","date":"2024-07-15T17:00:00Z","permalink":"https://suz-tsinghua.github.io/p/drl-notes-1/","title":"Deep Reinforcement Learning Lecture 1"},{"content":"2024春 深度强化学习 课程项目 —— Cyberdog2 Quadrupedal and Bipedal Dribbling 我们用强化学习在 Cyberdog2 上实现了四足与二足的运球，其中四足运球被部署到了真机上。\nYour browser doesn't support HTML5 video. Here is a link to the video instead. ","date":"2024-06-25T00:00:00Z","image":"https://suz-tsinghua.github.io/p/2024-spring-deep-reinforcement-learning-course-project-demo/cover_hu6555395394926631864.png","permalink":"https://suz-tsinghua.github.io/p/2024-spring-deep-reinforcement-learning-course-project-demo/","title":"2024 Spring Deep Reinforcement Learning Course Project Demo"},{"content":"本 blog 主要以 TidyBot 这篇工作为例子，简要介绍一下 Language-Conditioned Mobile Manipulation 这个研究领域。本文采取一个从下而上的方式，先从具体的 TidyBot 出发，再去介绍更广的领域。\nTidyBot 简单来说，TidyBot 是一个能 个性化、自动化 地帮助人们整理家中杂乱物品的机器人。\n论文地址：https://arxiv.org/abs/2305.05658 项目官网：https://tidybot.cs.princeton.edu/ 开源代码：https://github.com/jimmyyhwu/tidybot Your browser doesn't support HTML5 video. Here is a link to the video instead. TL;DR 这篇工作的主要目的就是设计一个能够 个性化、自动化 地帮助人们整理家中杂物的机器人。它需要能够识别地上的物体，判断该物体需要被 如何 收纳到 何处（如扔到垃圾桶里、放到抽屉里、放到沙发上等）。注意两点要求：\n个性化：由于不同的人可能有不同的收纳习惯，可能有些人喜欢把衣服放在架子上，有些人则可能喜欢放在抽屉里。 这意味着机器人不能给出一个广泛的策略（对于所有人来说，都把衣服放到架子上），它必须学习到其主人的喜好，从而指定专门的策略。 自动化：一旦设定完成，机器人收纳杂物的过程必须是全自动化的，不能让它的主人在旁边告诉它某物应该收纳到某处。 运用 LLM 的总结推理能力可以很好地解决这两个问题。这篇文章的 methods 非常直接，分为两步：\n在机器人第一次开始工作之前，先让主人提供几个例子，比如“黄衬衫要被放在抽屉里、深紫色衬衫要被放在柜子里、白色袜子要被放在抽屉里、黑色衬衫要被放在柜子里”。将这些例子告诉 LLM，让其总结出规则，LLM 就会总结出：“浅色的东西需要放在抽屉里，深色的东西需要放在柜子里。” 机器人工作过程中，先识别地上的某个物体，将第一步中得到的规则和这个物体是什么告诉 LLM，LLM 就可以告诉机器人这个物体需要被放在什么地方。 对于每个物体该 如何 被放置也是同理，先给 LLM 提供一些例子，如 \u0026ldquo;pick and place yellow shirt, pick and place dark purple shirt, pick and toss white socks\u0026rdquo;。LLM 可以总结出 \u0026ldquo;pick and place shirts, pick and toss socks\u0026rdquo;，再将 LLM 的 summarization 用于新物体即可。\n再加上一些物体识别，以及让机器人执行对应的收纳动作，这个个性化、自动化的收纳系统就可以被运用于真实世界中。\nMethod # TL;DR 中已经简略介绍了本工作的 methods，接下来 formally 展示下这样一个收纳系统的 pipeline：\n$E_{receptacle}$ 和 $E_{primitive}$ 都是用户的个性化输入，分别代表了每个物品 $o_i$ 需要被收纳到何处 $r_i$，以及需要被如何收纳 $p_i$。 接着运用 LLM 将 $E_{receptacle}$ 和 $E_{primitive}$ 总结成 $S_{receptacle}$ 和 $S_{primitive}$。 此时需要将 $S_{receptacle}$ 中 LLM 总结出的物体类别 （如浅色衣服、深色衣服）提取出来，以便于视觉系统进行分类。此处 pipeline 中只写了 $S_{receptacle}$，而没写 $S_{primitive}$，或许是默认了二者提取出来的物体类别是一致的，但严谨来说，同时考虑 $S_{receptacle}$ 和 $S_{primitive}$ 应该更合理。将物体类别 $C$ 提取出来的好处在于，后面进行物体分类的时候就可以只考虑较少的类别，不容易分类错误，而且不同的用户的 $C$ 也可以不同，更加 flexible。 做好了前置工作，就可以将系统部署到真实的机器人上了，系统会进入以下收纳循环，每一循环收纳一个物品，直到没有物品可以收纳： 利用外置摄像头得到地板的俯视图，通过 ViLD 识别出距离机器人最近的物体。 机器人移动到此物体旁，通过其自身的摄像头得到物体的近距离照片，将近距离照片与 $C$ 告诉 CLIP，让其对物体进行分类，得到类别 $c$。 让 LLM 根据 $c, S_{receptacle}, S_{primitive}$ 总结出物体该 如何 被放置到 何处。 机器人执行相应的收纳动作。 涉及到 LLM 的部分，具体 prompt 可以参阅原论文 Appendix A。\nExperiments Benchmark Dataset 为了评估所提出方法的可靠性，作者专门做了一个 benchmark dataset，其中共包含 96 个个性化场景，每个场景里都有一些容器和一些物品，其中有些物品被标注了应该被放到什么容器里，而另一些物品并没被标注。注意，每个场景可能代表了不同的收纳喜好，所以对于同一个物品，不同场景的收纳容器可能大不相同。任务的目的就是根据被标注的物品来预测未被标注的物体应该被放到哪里。\n在这个数据集上，作者做了一些实验：# Baseline Comparisons, # Ablation Studies, # Human Evaluation。\nBaseline Comparisons 这部分，作者将自己的方法与一些 baseline 作比较，比如只给 LLM 提供标注物体，直接让其预测为被标注物体该被放到哪里，而不经过 summarization 过程；再比如利用 pre-trained text embedding，对于未标注的物体，直接找到与其 embedding 距离最近的标注物体，认为二者应该被收纳到同一个地方。结论就是，作者的方法胜过其他 baseline。\nAblation Studies 作者一共做了三个方面的 ablation studies：\n不利用 user specific preference，直接让 LLM 依据 commonsense 来推断物品应该被放到哪里。 让人类来进行 summarization，不用 LLM 做。 比较采用不同 LLM 的准确率。 结论是，让 LLM 进行 summarization 会比直接用 commonsense 有非常大的提升，但相较于直接让人类进行 summarization 仍有不足。这也说明通过提升 LLM 的总结能力还能进一步提升此系统的能力。\n另一方面，在不同的 LLM 中，text-davinci-003 有较好的效果。\nHuman Evaluation 作者还招募了一些志愿者，向他们提供 user preference、baseline 给出的收纳建议、自己方法给出的收纳建议，让他们比较自己的方法与 baseline 的结果，哪个更符合 user preference。题目形式如下图所示：\n结果显示，作者的方法有 46.9% 的情况被认为更好，而 baseline 只有 19.1% 的情况被认为更好。\nReal-world Experiments 正如 # Method 中说的，作者还搭建了一个真实的机器人平台，让文章中提出的个性化收纳方法能够落地。作者构造了 8 个真实场景，每个场景包含一些散落在地上的物品以及几个收纳容器，然后让系统根据 # Method 中的 pipeline 运行。结果显示，系统在 85.0% 情况下都能正确完成收纳任务。\nLanguage-Conditioned Mobile Manipulation TidyBot 属于一个更广的研究领域 Language-Conditioned Mobile Manipulation。这个领域将 CV、NLP、Robotics 结合了起来，要求机器人能够根据人类的自然语言指令去做出相应的行为。\n这篇文章为 Language-Conditioned Mobile Manipulation 领域做了个详细的调查：\n论文地址：https://arxiv.org/pdf/2312.10807 开源代码：https://github.com/hk-zh/language-conditioned-robot-manipulation-models Architecture Framework 此领域工作的总体框架如上图所示。主要包括三个模块\n语言模块。其主要作用是理解用户的语言输入，并转化为机器人动作指导。如 TidyBot 中，用户告诉系统收纳 preference，语言模块就会进行处理，进行 preference 的总结等。 感知模块。其主要作用是感知周围环境，例如 TidyBot 中机器人利用自身的相机去识别物体进行分类。 控制模块。其主要作用是让机器人执行需要执行的指令。对应到 TidyBot 中，就是机器人去执行 “移动到某处”、“拿起地上的物品”、“把物品放置到某处” 等。在 TidyBot 中，这样的动作是 hard-coded 的，当然也可以采用 reinforcement learning (RL), imitation learing (IL) 等方法得到。 Approaches Categorization Language-Conditioned Mobile Manipulation 的工作主要可以被粗略分为以下几类：\nLanguage-conditioned Reinforcement Learning Language-conditioned Imitation Learning Empowered by LLMs \u0026amp; VLMs 当然，有些工作可能可以被同时划分到多种类别中。其中，前两种方法较为传统，没有采用大语言模型等现成工具。第三种方法利用现成的 LLMs 与 VLMs，简化了系统，提高了能力。\nLanguage-conditioned Reinforcement Learning 此类工作利用强化学习，通过人为设计等方法，建立一个从自然语言到 reward 的一个映射，当 agent 达到自然语言描述的目标时，它就能得到对应的 reward。Agent 在这个过程中可以学习到一个从自然语言到具体动作的映射。具体工作有：\nLancon-learn: Learning with language to enable generalization in multi-task manipulation [paper] [code] Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards [paper][code] Learning from symmetry: Meta-reinforcement learning with symmetrical behaviors and language instructions [paper][website] Meta-reinforcement learning via language instructions [paper][code][website] Learning language-conditioned robot behavior from offline data and crowd-sourced annotation [paper] Concept2robot: Learning manipulation concepts from instructions and human demonstrations [paper] Language-conditioned Imitation Learning 此类工作利用模仿学习的范式，其不像强化学习那样要求提供 reward，但是需要提供一些正确的行为例子 (expert demonstrations)，agent 会根据这些正确的行为进行学习。具体可以再被细分为 behavior cloning (BC) 和 inverse reinforcement learning (IRL)。\nBC 就是直接依样画葫芦，expert demonstrations 里怎么做，agent 就怎么做。具体工作有：\nLanguage conditioned imitation learning over unstructured data [paper] [code] [website] Bc-z: Zero-shot task generalization with robotic imitation learning [paper] What matters in language-conditioned robotic imitation learning over unstructured data [paper] [code][website] Grounding language with visual affordances over unstructured data [paper] [code][website] Language-conditioned imitation learning with base skill priors under unstructured data [paper] [code] [website] Pay attention!- robustifying a deep visuomotor policy through task-focused visual attention [paper] Language-conditioned imitation learning for robot manipulation tasks [paper] IRL 则需要经历两个步骤，第一步先从 expert demonstrations 和语言命令中学习一个从自然语言命令到 reward 的映射，再通过 RL 的方式学习行为策略（这样看来，此部分与 # Language-conditioned Reinforcement Learning 也有交集）。具体工作：\nGrounding english commands to reward function [paper] From language to goals: Inverse reinforcement learning for vision-based instruction following [paper] Empowered by LLMs \u0026amp; VLMs 前两类方法均需要对文本信息进行学习，而有了 LLM 这样强有力的工具，就可以对系统进行简化。具体而言，可以利用好大语言模型的 planning 和 reasoning 能力。\n大语言模型的 planning 能力指的是其将复杂任务转化为一系列简单的、机器人能够执行的任务的能力。譬如要求机器人炒菜，直接学习一个炒菜的策略是非常难的，但可以先让 LLM 将炒菜的动作拆分成 洗菜、放油、放菜 等一系列简单的、机器人能够学会的动作，此时再让机器人去执行这些动作就能完成炒菜的任务了。具体工作有：\nSayplan: Grounding large language models using 3d scene graphs for scalable task planning [paper] Language models as zero-shot planners: Extracting actionable knowledge for embodied agents [paper] Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents [paper] Progprompt: Generating situated robot task plans using large language models [paper] Robots that ask for help: Uncertainty alignment for large language model planners [paper] Task and motion planning with large language models for object rearrangement [paper] Do as i can, not as i say: Grounding language in robotic affordances [paper] The 2014 international planning competition: Progress and trends [paper] Robot task planning via deep reinforcement learning: a tabletop object sorting application [paper] Robot task planning and situation handling in open worlds [paper] [code] [website] Embodied Task Planning with Large Language Models [paper] [code] [website] Text2motion: From natural language instructions to feasible plans [paper] [website] Large language models as commonsense knowledge for large-scale task planning [paper] [code] [website] Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation [paper] Learning to reason over scene graphs: a case study of finetuning gpt-2 into a robot language model for grounded task planning [paper] [code] Scaling up and distilling down: Language-guided robot skill acquisition [paper][code] [website] Stap: Sequencing task-agnostic policies [paper] [code][website] Inner monologue: Embodied reasoning through planning with language models [paper] [website] 大语言模型的 reasoning 能力就像 TidyBot 中展示的那样，利用 LLM 去推理某个物品应该被放置到何处，再让机器人去执行特定的策略。具体工作有：\nRearrangement:A challenge for embodied ai [paper] The threedworld transport challenge: A visually guided task and motion planning benchmark for physically realistic embodied ai [paper] Tidy up my room: Multi-agent cooperation for service tasks in smart environments [paper] A quantifiable stratification strategy for tidy-up in service robotics [paper] Tidybot: Personalized robot assistance with large language models [paper] Housekeep: Tidying virtual households using commonsense reasoning [paper] Building cooperative embodied agents modularly with large language models [paper] Socratic models: Composing zero-shot multimodal reasoning with language [paper] Voyager: An open-ended embodied agent with large language models [paper] Translating natural language to planning goals with large-language models [paper] 在自然语言的基础上，可以再加上视觉工具，比如 TidyBot 中识别物体的部分。利用了 VLMs 的具体工作有：\nCliport: What and where pathways for robotic manipulation [paper] [code] [website] Transporter networks: Rearranging the visual world for robotic manipulation [paper] [code] [website] Simple but effective: Clip embeddings for embodied ai [paper] Instruct2act: Mapping multi-modality instructions to robotic actions with large language model [paper] [code] Latte: Language trajectory transformer [paper] [code] Embodied Task Planning with Large Language Models [paper] [code] [website] Palm-e: An embodied multimodal language model [paper] [website] Socratic models: Composing zero-shot multimodal reasoning with language [paper] Pretrained language models as visual planners for human assistance [paper] [code] Open-world object manipulation using pre-trained vision-language models [paper] [website] Robotic skill acquisition via instruction augmentation with vision-language models [paper] [website] Language reward modulation for pretraining reinforcement learning [paper] [code] Vision-language models as success detectors [paper] ","date":"2024-05-19T14:00:00+08:00","image":"https://suz-tsinghua.github.io/p/tidybot/cover_hu14442273580889646987.png","permalink":"https://suz-tsinghua.github.io/p/tidybot/","title":"Language-Conditioned Mobile Manipulation: 以 TidyBot 为例"},{"content":"Lecture 4 Context-Free Grammar Context-Free Grammar We have already known that REGEXPs can generate languages. Now we introduce another way to generate languages: Context-free grammar (CFG).\nDefinition:\nA CFG is a 4-tuple $(V, \\Sigma, R, S)$:\n$V$ is a set of variables. $\\Sigma$ is a set of terminals. $R$ is a set of rules (a rule consists of a variable and a sting in $(V\\cup \\Sigma)^*$). $S$ is the start variable. We say $uAv$ yields $uwv$ ($uAv \\Rightarrow uwv$) if $A\\to w$ is a rule in $R$.\nWe say $u$ derives $v$ ($u \\Rightarrow^* v$) if $\\exists u_1, u_2, \\cdots, u_k$ such that $u\\Rightarrow u_1\\Rightarrow u_2\\Rightarrow\\cdots\\Rightarrow u_k \\Rightarrow v$.\n总的来说，CFG 就是从 $S$ 开始，每次从现在的字符串中选择一个 variable，依据某个 rule 将其替换为一个 string，直到没有 variable 为止。\nDefinition:\nThe language of a CFG $G$ is defined as:\n$$L(G)=\\{w\\in \\Sigma^* | S\\Rightarrow^* w\\}$$\nCFG is more powerful than DFA 要证明 CFG is strictly more powerful than DFA，要从两方面入手：\n(1) 存在 language，可以用 CFG 表达，但不是 regular language。 (2) 任意 regular language 都可以用 CFG 表达。 对于 (1)，上节课已知 $L=\\{0^n1^n | n\\geq 0\\}$ is nonregular，但可以用 CFG $S\\to 0S1|\\epsilon$ 得到。\n对于 (2)，即要证明：\nTheorem:\nGiven a DFA $A=(Q,\\Sigma,\\delta,q_0,F)$, we can find a CFG $A^{\\prime}=(V,\\Sigma^{\\prime},R,S)$, such that $L(A)=L(A^{\\prime})$.\nProof:\n对 $A$ 的每个 state $q_i$，在 $V$ 中引入一个变量 $R_i$。 如果 $\\delta(q_i,a)=q_j$，就在 $R$ 中引入 rule $R_i\\to a R_j$。 $q_0$ 是 starting state，那么 $R_0$ 就是 start variable。 如果 $q_i$ 是一个 accepting state，就在 $R$ 中加入 rule $R_i\\to\\epsilon$。 这样构造出来的 $A^{\\prime}$ 满足 $L(A)=L(A^{\\prime})$。\nAn example:\nParse Trees and Ambiguity 可以用一个 parse tree 来表示从 start variable 生成一个 string of terminals 的过程：\nDefinition:\nParse tree:\nEach internal node labeled with a variable. Each leaf labeled with a terminal. The children of a node represent the rule that was applied. An example:\nDefinition:\nA CFG is called ambiguous if a string has two distinct parse trees.\n直观上来说，ambiguous 指的就是存在歧义，比如 $a+a\\times a$，如果不定义 + 和 $\\times$ 的计算顺序，就有可能先算 +，也可能先算 $\\times$。\nambiguous 并不等同于 “存在两种 derivations”，因为两种 derivations 可能对应同一个 parse tree。\nDefinition:\nLeftmost derivation: 每一步替换当前 string 最左边的那个 variable。\nTheorem:\nambiguous 等价于存在两种 leftmost derivations。\nDefinition:\nA CFL (context free language, 即可以用某个 CFG 生成的 language) is called inherently ambiguous if every CFG of it is ambiguous.\nAn example of inherently ambiguous CFL:\n$$L=\\{w | w=a^ib^jc^k, i,j,k\\geq 1 \\text{ and } i=j \\text{ or } i=k\\}$$\nClosure Properties Theorem:\nCFLs are closed under Union, Concatenation, Kleene Star.\nProof:\n只需要在原先的 CFG 之上引入新的 start variable，用新的 start variable 去生成旧的 start variables即可。\nTheorem:\nCFLs are NOT closed under intersection. But CFLs are closed under intersection with REGULAR languages.\nProof:\n$$L = \\{w : w \\in \\{0, 1\\}^*, w \\text{ is a palindrome, and } w \\text{ contains fewer 0s than 1s }\\}$$\n这是两个 CFL 的 intersection，可以用后面将要介绍的 pumping lemma 证明它不是 CFL。\nPushdown Automaton (PDA) Regular language 在句法上可以用 REGEXP 生成，在计算模型上可以被 DFA、NFA 识别。CFL 在句法上可以用 CFG 生成，现引入新的计算模型 PDA 来识别 CFL。\n由于 CFL 是 regular language 的扩充，PDA 也应该是 NFA 的扩充。事实上，PDA 就是为 NFA 多提供了一个无穷大的 stack，PDA 读取 input 的时候，可以从 stack 中 push 或 pop 一个 symbol。\nPDA 做的事情就是，每次读取一个 input（可能是 $\\epsilon$），并从 stack 顶部 pop 出一个 symbol（可能是 $\\epsilon$），然后 transit to a new state and push a symbol to the top of the stack（可能是 $\\epsilon$）。\nDefinition:\nA pushdown automaton (PDA) is a 6-tuple $(Q, \\Sigma, \\Gamma, \\delta, q_0, F)$:\n$Q$ is a finite set of states. $\\Sigma$ is the input alphabet. $\\Gamma$ is the stack alphabet. $q_0$ in $Q$ is the initial state. $F \\subseteq Q$ is a set of final states. $\\delta$ is the transition function. $\\delta : Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\times (\\Gamma \\cup \\{\\epsilon\\}) \\to 2^{Q \\times (\\Gamma \\cup \\{\\epsilon\\})}$ Definition:\nPDA $(Q, \\Sigma, \\Gamma, \\delta, q_0, F)$ accepts string $w \\in \\Sigma^*$ if:\n$w$ can be written as $x_1 x_2 \\cdots x_m$, each $x_i \\in \\Sigma \\cup \\{\\epsilon\\}$. $\\exists r_0, r_1, \\cdots, r_m \\in Q$, a sequence of $m+1$ states. $\\exists s_0, s_1, \\cdots, s_m \\in \\Gamma^*$, stack contents. such that $r_0 = q, s_0=\\epsilon$. $(r_i, b) \\in \\delta(r_{i-1},x_i, a)$, where $s_{i-1}=ta$ and $s_i=tb$ for $1 \\leq i \\leq m$. $r_m\\in F$. 可以通过构造 PDA 的方法证明 $$L=\\{ww^R\\}$$ $$L=\\{w:w=w^R\\}$$ $$L=\\{w: w \\text{ has the same number of 0s and 1s}\\}$$ $$L=\\{w: w \\text{ has two 0-blocks with same number of 0s}\\}$$ 都可以被 PDA 识别。\nEquivalence of CFG and PDA Theorem:\nA language L is context-free if and only if it is accepted by some pushdown automaton.\nProof:\n(1) From CFG to PDA. 思路就是用 PDA 来模拟 leftmost derivation。将每时每刻的 string 倒序放在 stack中，如果 stack 的顶部是 terminal，就和 input 比较，相同则 pop out，不同则 reject；如果 stack 顶部是 variable，就做 leftmost derivation。\n例子： Formally: 用图来表示 PDA 就是： (2) From PDA to CFG. 总的思路就是，先令 PDA 只有一个 accept state，并且 accept 之前会清空 stack。然后 CFG 的一个 variable 就是一个字符串的集合，使得 PDA 读取集合中的字符串能从某个 state 转移到另一个 state，同时保持 stack 不变。\nFormally: CFG 的 rules 如下： 并不给出详细的证明，直观理解即可。\nChomsky Normal Form and CYK algorithm Definition:\nA context-free grammar is in Chomsky normal form if every rule is of the form:\n$A \\to BC$ $A \\to a$ 其中，$A, B, C$ 是任意 variables，$a$ 是任意非 $\\epsilon$ 的 terminal。$B, C$ 不能是 start variable。允许 $S\\to \\epsilon$。\n任意 CFG 都可以转换成 CNF 的形式，课件的 91-98 页讲得很清楚，还举了个例子。\n将任意 CFG 转成 CNF 之后，我们就可以用 Cocke–Younger–Kasami algorithm 来判断某个特定的 string 能否被某个特定的 CFG 生成。假设 string 为 $s[1:n]$，CFG 有 $r$ 个 variables $R$，其中 $R_1$ 是 start variable。总的思路是，构造一个 $n\\times n\\times r$ 的表格 $T$，其中 $T[i,j,k]$ 表示 $s[i:j]$ 能否由 $R_k$ 生成，然后依据 $j-i$ 从小到大的顺序 DP 地判断 $s[i:j]$ 能否被 $R_k$ 生成。最后，$s$ 能被生成当且仅当 $T[1,n,1]$ 为真。\nPumping Lemma for CFL Theorem:\nIf $A$ is a context-free language, then there is a number $p$ (the pumping length) where, if $s$ is any string in $A$ of length at least $p$, then $s$ may be divides into 5 pieces $s=uvxyz$, satisfying:\nFor each $i\\geq 0, uv^ixy^iz \\in A$ $|vy|\u0026gt; 0$ $|vxy|\\leq p$ Proof:\n用 CFG 来证明，假设 CFG 有 $a$ 个 variables，rules 右手边的 symbols 最长为 $b$ 个。考察 parse tree，当 parse tree 从 root 到 leaf 最深的 path 长度为 $v+2$ 时，至少有一个 variable $N$ 出现了两次，假设 $S\\Rightarrow^* uNz\\Rightarrow uvNyz\\Rightarrow uvxyz$，那么显然也可以有 $S\\Rightarrow^* uNz\\Rightarrow uvNyz\\Rightarrow uvvNyyz\\Rightarrow uvvxyyz$。可以算出 $p$ 是 $b,v$ 的一个函数。 可以用 pumping lemma 证明，$\\{a^n b^n c^n | n\\geq 0\\}, \\{ww|w\\in\\{0,1\\}^*\\}, \\{a^i b^j c^k | 0\\leq i\\leq j\\leq k\\}$ 都不是 context-free 的。\n","date":"2024-04-17T15:00:00+08:00","image":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-4/cover_hu12441510513087462890.png","permalink":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-4/","title":"Theory of Computation Lecture 4"},{"content":"Lecture 3 Finite Automata 计算理论剩下的笔记会是 exam-oriented 的，因为系统地写一篇笔记确实太过耗时。\nDeterministic Finite Automaton (DFA) DFA and Regular Languages Definition:\nA DFA is a 5-tuple $(Q, \\Sigma, \\delta, q, F)$:\n$Q$ is a finite set of states. $\\Sigma$, the alphabet is a finite set of symbols. $\\delta: Q\\times \\Sigma \\to Q$ is the transition function. $q\\in Q$ is the start state. $F\\subset Q$ is the set of accepting states. DFA 做的事情就是从 start state 开始，每次从输出序列中获取下一个 symbol，依据 transition function 转成另一个state，直到 inputs 被获取完，判断此时的 state 是否为 accepting state。\nDefinition:\nA DFA $M$ accepts the input $w$ if:\n$M=(Q,\\Sigma,\\delta,q,F)$. $w=w_1 w_2\\cdots w_n$, each $w_i\\in \\Sigma$ for $1\\leq i\\leq n$. $\\exists (r_0, r_1, \\cdots, r_n)$, each $r_i\\in Q$ for $0\\leq i\\leq n$ s.t. $r_0 = q$, the start state. $r_i = \\delta (r_{i-1}, w_i)$, for $1\\leq i\\leq n$. $r_n\\in F$. 每个 DFA 都存在一些 inputs 的集合，使其 accepts，定义这个集合为 DFA 的 language。\nDefinition:\nA Language is a (possibly infinite) set of strings over some alphabet.\n$L(M) = \\{w | M \\text{ accepts } w\\}$ is the language recognized by M.\nA DFA always recognizes one language!\nIf it accepts no strings, it recognize the “empty language” $\\emptyset$.\nDFA 可以做一些简单的任务，比如判断一个 binary number 是否可以被 3 整除（只需要用 states 存目前被 3 除的余数即可），比如判断一个字母序列是否包含 b（检测到 b 就进入 accepting state）。由此我们可以定义 regular languages：\nDefinition:\nRegular languages is the set of all languages recognized by some DFA.\n所以只要一个 language 能被某个 DFA 识别，那它就是一个 regular language。\nRegular Languages are Closed under Complementation, Intersection and Union 对于一个或几个 regular language(s)，我们可以由它们构造新的 language，使得新的 language 也是 regular language。比如 complementation: $L^{\\prime}=\\bar{L}$. 即我们现在已知存在某个 DFA $M$ 使得 $L=L(M)$, 我们是否能找到一个 DFA $M^{\\prime}$ 使得 $L(M^{\\prime})=\\bar(L(M))$。\nTheorem:\nThe class of regular languages is closed under complementation.\nProof:\n根据 $M$, 构造一新的 DFA $M^{\\prime}$，二者完全一样，除了 $M^{\\prime}$ 的 accepting states 是 $M$ 的 accepting states 的补集。这样的话一个 input 能被 $M$ 识别当且仅当其不能被 $M^{\\prime}$ 识别。\nTheorem:\nThe class of regular languages is closed under intersection.\nProof:\n假设我们现在有两个 DFAs $M_1=(Q_1, \\Sigma, \\delta_1, s_1, F_1), M_2=(Q_2, \\Sigma, \\delta_2, s_2, F_2)$，构造一新 DFA $M_3=(Q_1\\times Q_2, \\Sigma, \\delta_1\\times \\delta_2, (s_1,s_2), F_3)$。即 $M_3$ 的状态空间是 $Q_1$ 和 $Q_2$ 的直积，在两个分量空间中分别作 transition。$(q_1, q_2)\\in F_3$ 当且仅当 $q_1\\in F_1, q_2\\in F_2$。\nTheorem:\nThe class of regular languages is closed under uniton.\nProof:\n可以用 DeMorgan’s Law $X\\cup Y = \\overline{(\\bar{X}\\cap\\bar{Y})}$，也可以用与上一 theorem 类似的构造。\n定义 regular operations on languages:\nDefinition: A \u0026amp; B are two (possibly infinite) sets of strings. Define regular operations on them:\nUnion: $A \\cup B = \\{x | x\\in A \\text{ or } x\\in B\\}$. Concatenation: $A\\circ B = \\{xy | x\\in A \\text{ and } y\\in B\\}$. Star: $A^* = \\{x_1 x_2 \\cdots x_k | k\\geq 0 \\text{ and all } x_i\\in A\\}$. note: the empty string $\\epsilon (k=0)$ is always in $A^*$. 我们已知 regular languages is closed under union，但对剩下两种 operations，很难直接构造出合适的 DFA，需要先引入新的概念。\nNondeterministic Finite Automaton (NFA) NFA NFA 与 DFA 类似，但它\n可以不获取输入自发转变状态（也可以被视作获取空字符 $\\epsilon$）。 在某个状态获取某个输入之后，转变成的新状态有很多种可能。 Definition:\nAn NFA is a 5-tuple $(Q, \\Sigma, \\delta, q, F)$:\n$Q$ is a finite set of states. $\\Sigma$, the alphabet is a finite set of symbols. $\\delta: Q\\times \\Sigma_{\\epsilon} \\to 2^Q$ is the transition function. $q\\in Q$ is the start state. $F\\subset Q$ is the set of accepting states. 这里，$\\Sigma_{\\epsilon}=\\Sigma\\cup\\{\\epsilon\\}$，$2^Q$ 是 $Q$ 的所有子集组成的集合，叫做 $Q$ 的 power set。\nDefinition:\nAn NFA $N$ accepts the input $w\\in \\Sigma^*$ if:\n$N=(Q,\\Sigma,\\delta,q,F)$. $w$ can be written as $x_1 x_2\\cdots x_n$, each $x_i\\in \\Sigma_{\\epsilon}$ for $1\\leq i\\leq n$. 注意这里用的是 \u0026ldquo;can be written\u0026rdquo; 而不是 \u0026ldquo;=\u0026quot;，因为 $\u0026ldquo;abba\u0026rdquo;$ can be written as $\u0026ldquo;a\\epsilon bb \\epsilon a\u0026rdquo;$。 $\\exists (r_0, r_1, \\cdots, r_n)$, each $r_i\\in Q$ for $0\\leq i\\leq n$ s.t. $r_0 = q$, the start state. $r_i \\in \\delta (r_{i-1}, x_i)$, for $1\\leq i\\leq n$. $r_n\\in F$. Equivalence of NFA and DFA 可以证明 NFA 与 DFA 是等价的，即对任意 NFA 都可以找到一个 DFA，使得二者的 language 相同；对任意 DFA 也可以找到一个 NFA，使得二者的 language 相同。后者是显然的，因为任意 DFA 本身就可以被视为一个 NFA。我们现在要证明前者。\nTheorem:\nFor any NFA $N$, there is a DFA $M$ such that $L(N) = L(M)$.\nProof:\n总的思想就是：at all times, M keeps track of the set of states that N could be in.\n(1) 先对 $N$ 中不存在 $\\epsilon$ 的情况证明：\n若 $N=(Q,\\Sigma,\\delta,q,F)$，令 $M=(Q^{\\prime}, \\Sigma, \\delta^{\\prime}, q^{\\prime}, F^{\\prime})$:\n$Q^{\\prime}=2^{Q}$ $\\delta^{\\prime}(R,a)=\\cup_{r\\in R}\\delta(r,a)$。其中 $R\\in 2^{Q}, a\\in \\Sigma$。 $q^{\\prime}=\\{q\\}$ $F^{\\prime}=\\{R\\in Q^{\\prime} | R\\cap F\\neq \\emptyset\\}$ (2) 对于存在 $\\epsilon$ 的情况，可以将 $\\epsilon$ 后的 state 并入前面的 state。Formally:\n令 $E(R)=\\{r\\in Q | r \\text{ is reachable from } R \\text{ using zero on more } \\epsilon \\text{-transitions}\\}$.\n$Q^{\\prime}=2^{Q}$ $\\delta^{\\prime}(R,a)=\\cup_{r\\in R}E(\\delta(r,a))$。其中 $R\\in 2^{Q}, a\\in \\Sigma$。 $q^{\\prime}=E(\\{q\\})$ $F^{\\prime}=\\{R\\in Q^{\\prime} | R\\cap F\\neq \\emptyset\\}$ 一个 NFA 需要 $\\log_2 |Q|$ bits 来存 states，与其等价的 DFA 需要 $\\log_2 |Q^{\\prime}| = |Q|$ bits 来存 states。可以举出一个 NFA 的例子，证明这样的 exponential blowup 在某些情况下是必须的。\nExample:\n$\\Sigma=\\{a,b\\}, L_k=\\{w\\in\\{a,b\\}^* | \\text{ the } k \\text{-th symbol from the end is } b\\}$. There is a $(k+1)$-state NFA recognizing $L_k$.\nThen we prove that any DFA with $\u0026lt; 2^k$ states can not recognize $L_k$.\nWe prove by contradiction.\n(1) Assume there is a DFA $M$ with $|Q| = 2^k - 1$.\n(2) Imagine running $M$ on each input $w\\in\\{a,b\\}^k$.\n(3) By the pigeonhole principle, $\\exists w \\neq w^{\\prime}\\in\\{a,b\\}^k$ s.t. after reading $w$ and $w^{\\prime}$, $M$ is in the same state.\n(4) Let $j+1$ be the first position where $w$ and $w^{\\prime}$ differ.\n(5) Run $M$ on $wa^j$ and $w^{\\prime}a^j$. $M$ is supposed to be in the same state. But $w\\notin L_k, w^{\\prime}\\in L_k$. A contradiction.\nRegular Languages are Closed under Concatenation and Star 先介绍定理：\nTheorem:\nFor any NFA $N_1$, there is an NFA $N_2$ such that $L(N_1) = L(N_2)$ and $N_2$ has exactly one accept state.\nProof:\n把 $N_1$ 中所有的 accepting states 用 $\\epsilon$ 连到一个新的 accepting states。\nTheorem:\nThe class of regular languages is closed under concatenation.\nProof:\n对于 $N_1, N_2$ 构造 $N_3$ 使得 $L(N_3)=L(N_1)\\circ L(N_2)$。只需要把 $N_1$ 的 accepting states 都用 $\\epsilon$ 连到 $N_2$ 的 starting state 就行。\nTheorem:\nThe class of regular languages is closed under star。\nProof:\n如下图构造 $L(N_2)=L(N_1)^*$:\nEquivalence of NFAs, DFAs and Regular Expressions Definition:\n一个 regular expression (REGEXP) 是用以下方式表示的一个集合： 从 $\\{a\\}, \\{\\epsilon\\}, \\emptyset$ 开始，通过 $(R_1\\cup R_2), (R_1\\circ R_2), (R_1^*)$ 构成。\n简化：省略 $\\circ$，定义计算顺序 $^*, \\circ, \\cup$。\n接下来证明 REGEXP 和 NFA 等价。\nTheorem:\n(1) The language of any REGEXP is recognized by an NFA.\n(2) The language of any NFA can be represented by a REGEXP.\n(1) 是显然的，因为 regular languages are closed under $\\cup, \\circ, ^*$。\n(2) 不显然，需要引入新概念。\nDefinition:\nA GNFA (Generalized NFA) is a 5-tuple $(Q, \\Sigma, \\delta, q_{start}, q_{accept})$:\n$Q$ is a finite set of states. $\\Sigma$, the alphabet is a finite set of symbols. $q_{start}$ is the start state. $q_{accept}$ is the accept state. $\\delta: (Q\\backslash \\{q_{accept}\\}) \\times (Q\\backslash \\{q_{start}\\}) \\to REGEXP$ is the transition function. 简单地说，GNFA 就是 NFA，只不过 transition 用 REGEXP 表达。显然 GNFA 和 NFA 等价。\n故而，证明 (2) 的思路为：任意一个 NFA 都可以用一个 GNFA 表示，任意一个 GNFA 都可以被缩减为只有两个 states，所以等价于一个 REGEXP。第一步和第三步显然。第二步逐个删去 state 即可，图解：\nNonregular Languages Example:\n\\{0^n 1^n|n\\geq 0\\} is nonregular.\nProof:\nAssume it\u0026rsquo;s regular, the DFA recognizing it consists of $p$ states. Then by the pigeonhole principle, there exists $i,j$ such that after reading $0^i, 0^j$, the DFA is in the same state. So if it accepts $0^i 1^i$, it must accept $0^j 1^i$, which is a contradiction.\n用这种思想，可以引入 pumping lemma:\nPumping Lemma:\nIf $A$ is a regular language, then there is a number $p$ (the pumping length) where, if $s$ is any string in $A$ of length at least $p$, then $s$ can be divided into $s=xyz$, satisfying:\nFor each $i\\geq 0, xy^i z \\in A$ $|y|\u0026gt;0$ $|xy|\\leq p$ Proof:\n令 $p$ 是 DFA 的状态数，再用上面那个例子中类似的思想即可。\n可以用 pumping lemma 判断 nonregular language。如果不满足 pumping lemma，那么肯定是 nonregular language。思路就是，假设有个 $p$，再从 language 中找反例。\n可以证明 $\\{ww|w\\in \\{0,1\\}^*\\}, \\{1^{n^2}|n\\geq 0\\}, \\{0^i 1^j|i\u0026gt;j\\}$ 都是 nonregular language。\n","date":"2024-04-16T17:00:00+08:00","image":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-3/cover_hu13486777222581173739.png","permalink":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-3/","title":"Theory of Computation Lecture 3"},{"content":"Leveraing Symmetry in RL-based Legged Locomotion Control 第一个科研项目。先在这放个链接，之后再补正文吧。\nArxiv链接\nDemo: （可能需要梯子） ","date":"2024-04-02T18:00:00+08:00","image":"https://suz-tsinghua.github.io/p/symmetric-quadruped/cover_hu15007363877443634568.png","permalink":"https://suz-tsinghua.github.io/p/symmetric-quadruped/","title":"Leveraging Symmetry in RL-based Legged Locomotion Control"},{"content":"Lecture 2 Mathematical Logic (2) Informal Predicate Calculus (非形式的谓词演算) 量词 对于某些命题，我们无法用第一节课中的方式将其表达为一命题形式，如“所有人都会死”，此时我们必须要引入量词来表达“所有”这样的限制语义：\nDefinition:\n全称量词 (Universal quantifier)，对所有的 $x$，$(\\forall x)$。\n存在量词 (Existensial quantifier)，存在 $x$，$(\\exists x)$。\n“所有人都会死”就可以表示成 $(\\forall x)(A(x)\\to M(x))$。 此处，$x$ 为一变元，指的是所有的东西，并不只限于人。$A(x)$ 表示 $x$ 是人，$M(x)$ 表示 $x$ 会死。\n$\\forall$ 和 $\\exists$ 之间也可以相互转换。考察句子“不是所有鸟都会飞”，可以表示为 $\\sim(\\forall x)(B(x)\\to F(x))$。句子显然等价于“有些鸟不会飞”，可以表示为 $(\\exists x)(B(x)\\land \\sim F(x))$。\n我们知道：\n$$ \\begin{align*} \\sim(\\forall x)(B(x)\\to F(x)) \u0026amp;\\iff \\sim(\\forall x)(\\sim B(x)\\lor F(x))\\\\ \u0026amp; \\iff \\sim(\\forall x)\\sim (B(x)\\land \\sim F(x)) \\end{align*} $$\n对比 $\\sim(\\forall x)\\sim (B(x)\\land \\sim F(x))$ 与 $(\\exists x)(B(x)\\land \\sim F(x))$，不难发现二者有相似之处。事实上，$(\\exists x)\\mathscr{A} \\iff \\sim(\\forall x)\\sim \\mathscr{A}$。\n一阶语言 在引入量词的基础上，我们希望跟第一讲中一样，构造一形式系统。我们称之为 一阶语言 (first order language)。按照第一讲中定义，形式系统由符号库、合式公式、公理、演绎规则组成。先考察前二者。\nThe alphabet of symbols:\nNames Symbols 变元 $x_1, x_2, \\cdots$ 某些（可能没有）个体常元 $a_1, a_2, \\cdots$ 某些（可能没有）谓词字母 $A_i^n$ 某些（可能没有）函数字母 $f_i^n$ 标点符号 ( ) , 联结词 $\\sim$ $\\to$ 量词 $\\forall$ 其中，个体常元即为一个特殊的个体，如“苏格拉底”这样一个指定的人。谓词类似一种关系 $R$，可以被视为一个函数，获取 $n$ 个输入，并返回 $T$ 或 $F$。由于 $\\exists$ 可以转化为 $\\forall$，我们在符号库中仅使用 $\\forall$。\n例如：$(\\forall x_1)(\\forall x_2) A_1^2(f_1^2(x_1,x_2), f_1^2(x_2,x_1))$，若 $A_1^2$ 表示 $=$，$f_1^2$ 表示 $+$，那整个命题就可以写成 $(\\forall x_1)(\\forall x_2) (x_1+x_2=x_2+x_1)$。\n接下来定义合式公式：\nDefinition:\n令 $\\mathscr{L}$ 是一阶语言，$\\mathscr{L}$ 中的一个 项 (term) 定义如下：\n变元和个体常元是项。 如果 $t_1, \\cdots, t_n$ 是项，那么 $f_i^n(t_1,\\cdots,t_n)$ 是项。 所有项都由上两条规则生成。 若 $t_1, \\cdots, t_k$ 是 $\\mathscr{L}$ 中的项，那么 $A_i^k(t_1, \\cdots, t_k)$ 是 $\\mathscr{L}$ 中的一个 原子公式 (atomic fomula)。\n$\\mathscr{L}$ 中的 合式公式 (well-formed formula) 定义为：\n每个原子公式是一个合式公式。 若 $\\mathscr{A}, \\mathscr{B}$ 是合式公式，那么 $(\\sim \\mathscr{A}), (\\mathscr{A}\\to\\mathscr{B})$ 和 $(\\forall x_i)\\mathscr{A}$ （其中 $x_i$ 是任何变元）也是 $\\mathscr{L}$ 中的合式公式。 所有合式公式都由上两条规则生成。 我们应注意到 $(\\forall x_1)(\\mathscr{A}\\to\\mathscr{B})$ 与 $((\\forall x_1)\\mathscr{A}\\to\\mathscr{B})$ 表达的是不同的东西，故而需要引入下面的定义。\nDefinition:\n在公式 $(\\forall x_i)\\mathscr{A}$ 中，我们称 $\\mathscr{A}$ 是量词的 辖域 (scope)。\n变元 $x_i$ 如果出现在 $(\\forall x_i)$ 的辖域中，则称它是 约束的 (bound)，反之称它是 自由的 (free)。\n例如 $(\\forall \\textcolor{red}{x_1})(A_1^2(\\textcolor{red}{x_1},x_2)\\to (\\forall \\textcolor{red}{x_2})A_1^1(\\textcolor{red}{x_2}))$ 中，标红的即为约束的变元。\n现考察变元的替换，在公式 $(\\exists x_2)(x_2=x_1(x_1+1))$ 中，我们可以将 $x_1$ 换成 $x_3, f(x_1, x_3)$ 等不包含 $x_2$ 的项，但显然不能换成 $f(x_1, x_2)$ 这种包含 $x_2$ 的项。引入以下定义：\nDefinition:\n令 $\\mathscr{A}$ 是 $\\mathscr{L}$ 中的任何公式，我们称项 $t$ 对 $\\mathscr{A}$ 中的 $x_i$ 是自由的，如果 $x_i$ 并不自由地出现在 $\\mathscr{A}$ 的一个 $(\\forall x_j)$ 的辖域中，这里 $x_j$ 是出现在 $t$ 中的任何变元。\n在上面的例子中，$t=f(x_1,x_2), x_i=x_1, x_j=x_2$，$x_1$ 自由地出现在 $\\mathscr{A}$ 中 $x_2$ 的辖域中，故 $f(x_1,x_2)$ 对 $(\\exists x_2)(x_2=x_1(x_1+1))$ 中的 $x_1$ 不是自由的。\n显然，对任何 $x_1$ 和 $\\mathscr{A}$ 来说，$x_1$ 对 $\\mathscr{A}$ 中的 $x_1$ 都是自由的。\n解释 我们现在希望考察 $\\mathscr{L}$ 中的公式什么时候能被称为是 “真” 的。事实上，只有当公式中内容的 “解释” 被给出的时候，我们才能讨论公式的真假。\n例如， $(\\forall x_1)(\\forall x_2)A_1^2(f_1^2(x_1,x_2), f_1^2(x_2,x_1))$。如果我们在自然数的范围内讨论，且认为 $A_1^2$ 代表 $=$，$f_1^2$ 代表 $+$，那么公式为真。但倘若 $f_1^2$ 代表 $-$，那么公式显然是假的。\nDefinition:\n$\\mathscr{L}$ 中的一个 解释 (interpretation) $I$ 由以下四部分组成：\n一个非空集合 $D_I$，即 $I$ 的 论域 (domain)。 一个 特异元素集 (a collection of distinguished elements) $\\bar{a}_i\\in D_I$。 一个在 $D_I$ 上的函数集 $\\bar{f}_i^n: D_I^n\\to D_I$。 一个在 $D_I$ 上的关系集 $\\bar{A}_i^n$。 四者分别是对符号表中变元，个体常元，函数字母，谓词字母的具体解释。\n例如可以取 $D_I=\\{0,1,2,\\cdots\\}, a_1=0, A_1^2$ 表示 $=$, $f_1^2$ 表示 $+$。那么在 $I$ 中，$(\\forall x_1) A_1^2(f_1^2(x_1, a_1), x_1)$ 为真。\n满足，真 Definition:\n$I$ 上的一个 赋值 (valuation) 是一从 $\\mathscr{L}$ 的项集到集合 $D_I$ 的具有下列性质的一个函数 $v$:\n$v(a_i)=\\bar{a}_i$，对 $\\mathscr{L}$ 中的每个个体常元 $a_i$。 $v(f_i^n(t_1,t_2,\\cdots, t_n)) = \\bar{f}_i^n(v(t_1), v(t_2), \\cdots, v(t_n))$，其中 $f_i^n$ 是 $\\mathscr{L}$ 中的任意函数字母，$t_1, \\cdots, t_n$ 是 $\\mathscr{L}$ 中的任意项。 接下来我们要讨论一个赋值能够使得公式为真。\nDefinition:\n两个赋值 $v, v^{\\prime}$，如果对每个 $j\\neq i$，都有 $v(x_j)=v^{\\prime}(x_j)$，则称二者是 $i$-等值的。\nDefinition:\n令 $\\mathscr{A}$ 是 $\\mathscr{L}$ 的一个公式，$I$ 是 $\\mathscr{L}$ 的一个解释，我们称 $I$ 中的一个赋值 $v$ 满足 (satisfies) $\\mathscr{A}$，如果能按如下四个条件归纳地表明 $v$ 满足 $\\mathscr{A}$:\n如果 $\\bar{A}_j^n(v(t_1), \\cdots, v(t_n))$ 在 $D_I$ 中为真，那么称 $v$ 满足原子公式 $A_j^n(t_1, \\cdots, t_n)$。 如果 $v$ 不满足 $\\mathscr{B}$，那么 $v$ 满足 $(\\sim\\mathscr{B})$。 如果 $v$ 满足 $(\\sim \\mathscr{B})$ 或 $\\mathscr{C}$，那么 $v$ 满足 $(\\mathscr{B}\\to\\mathscr{C})$。 如果每个 $i$-等值于 $v$ 的赋值 $v^{\\prime}$ 都满足 $\\mathscr{B}$，那么 $v$ 满足 $(\\forall x_i)\\mathscr{B}$。 Definition:\n一公式 $\\mathscr{A}$ 在解释 $I$ 中称为 真的 (true)，如果在 $I$ 中的每个赋值都满足 $\\mathscr{A}$。 一公式 $\\mathscr{A}$ 在解释 $I$ 中称为 假的 (false)，如果 $I$ 中不存在任何满足 $\\mathscr{A}$ 的赋值。 在 真 的基础上更近一步：\nDefinition:\n$\\mathscr{L}$ 的一个合式公式 $\\mathscr{A}$ 称为 逻辑有效的 (logically valid)，如果 $\\mathscr{A}$ 在 $\\mathscr{L}$ 中的每个解释都为真。 $\\mathscr{L}$ 的一个合式公式 $\\mathscr{A}$ 称为 矛盾的 (contradictory)，如果 $\\mathscr{A}$ 在 $\\mathscr{L}$ 中的每个解释都为假。 Proposition:\n如果在一个解释中，公式 $\\mathscr{A}$ 和 $(\\mathscr{A}\\to\\mathscr{B})$ 都为真，那么 $\\mathscr{B}$ 也为真。\nProof: 由真和赋值的定义即可证明。\nProposition:\n在一个解释中，公式 $\\mathscr{A}$ 为真，当且仅当 $(\\forall x_i) \\mathscr{A}$ 为真，其中 $x_i$ 是任意变元。\nProof: 由定义。\n例如可以取 $D_I=\\{0,1,2,\\cdots\\}, A_1^2$ 表示 $=$, $f_1^2$ 表示 $+$。那么 $$(\\forall x_1)(\\forall x_2) A_1^2(f_1^2(x_1,x_2),f_1^2(x_2,x_1))$$ $$(\\forall x_2) A_1^2(f_1^2(x_1,x_2),f_1^2(x_2,x_1))$$ $$A_1^2(f_1^2(x_1,x_2),f_1^2(x_2,x_1))$$ 都为真。\nDefinition:\n$\\mathscr{L}$ 中的一个公式 $\\mathscr{A}$ 称为 闭的 (closed)，如果没有变元在 $\\mathscr{A}$ 中自由出现。\n例如，$(\\forall x_1)(\\forall x_2) A_1^2(f_1^2(x_1,x_2),f_1^2(x_2,x_1))$ 是闭的，$(\\forall x_1) A_1^1(x_1)\\to A_1^1(x_1)$ 不是闭的。\nProposition:\n如果 $\\mathscr{A}$ 是 $\\mathscr{L}$ 中的闭公式，且 $I$ 是 $\\mathscr{L}$ 的一个解释，那么 $\\mathscr{A}$ 在 $I$ 中非真即假。\nProof:\n由于所有变量都是约束的，所以可以根据 满足 的定义的第四条得知，一个赋值满足 $\\mathscr{A}$ 当且仅当所有赋值满足 $\\mathscr{A}$。\nFormal Predicate Calculus (形式的谓词演算) 形式系统 $K_{\\mathscr{L}}$ 在 # 一阶语言 中我们说到，形式系统需由符号库、合式公式、公理、演绎规则组成，并定义了前二者。接下来我们定义后二者，并由此得到一个形式系统 $K_{\\mathscr{L}}$。\nDefinition:\n令 $\\mathscr{A},\\mathscr{B},\\mathscr{C}$ 是 $\\mathscr{L}$ 中的任意公式，以下是 $K_{\\mathscr{L}}$ 的公理：\n(K1) $(\\mathscr{A}\\to(\\mathscr{B}\\to\\mathscr{A}))$. (K2) $((\\mathscr{A}\\to(\\mathscr{B}\\to\\mathscr{C}))\\to ((\\mathscr{A}\\to\\mathscr{B})\\to(\\mathscr{A}\\to\\mathscr{C})))$. (K3) $(((\\sim \\mathscr{A})\\to(\\sim \\mathscr{B}))\\to (\\mathscr{B}\\to\\mathscr{A}))$. (K4) $((\\forall x_i) \\mathscr{A}\\to \\mathscr{A})$，如果 $x_i$ 不在 $\\mathscr{A}$ 中自由出现。 (K5) $((\\forall x_i) \\mathscr{A}(x_i)\\to \\mathscr{A}(t))$，如果 $t$ 是 $\\mathscr{L}$ 中的一个项，并且在 $\\mathscr{A}(x_i)$ 中对 $x_i$ 自由。 (K6) $(\\forall x_i)(\\mathscr{A}\\to\\mathscr{B})\\to(\\mathscr{A}\\to(\\forall x_i)\\mathscr{B})$，如果 $x_i$ 不在 $\\mathscr{A}$ 中自由出现。 演绎规则：\nMP：从 $\\mathscr{A}$ 和 $(\\mathscr{A}\\to\\mathscr{B})$ 可以演绎出 $\\mathscr{B}$。 Generalization：从 $\\mathscr{A}$ 可以演绎出 $(\\forall x_i)\\mathscr{A}$，其中 $x_i$ 是任意变元。 事实上，如果 $x_i$ 在 $\\mathscr{A}$ 中自由出现，因为 $x_i$ 在 $\\mathscr{A}$ 中对 $x_i$ 自由，可以由(K5) 知道，(K4) 仍然成立。所以，无论 $x_i$ 是否在 $\\mathscr{A}$ 中自由出现，都有 $((\\forall x_i) \\mathscr{A}\\to \\mathscr{A})$。\nDefinition:\n$K_{\\mathscr{L}}$ 中的一个 证明 (proof) 是指 $\\mathscr{L}$ 的公式的这样一个序列 $\\mathscr{A}_1, \\cdots, \\mathscr{A}_n$，使得对于每个 $i (1\\leq i\\leq n)$，$\\mathscr{A}_i$ 或者是 $K_{\\mathscr{L}}$ 的一个公理，或者是由位于 $\\mathscr{A}_i$ 之前的公式通过 MP 或 Generalization 规则得到的。最后的公式 $\\mathscr{A}_n$ 称为 $K_{\\mathscr{L}}$ 中的一条 定理 (theorem)，记作 $\\vdash_{K_{\\mathscr{L}}} \\mathscr{A}_n$。\n如果 $\\Gamma$ 是 $\\mathscr{L}$ 中的一组合式公式，在 $K_{\\mathscr{L}}$ 中从 $\\Gamma$ 的一个 演绎 (deduction) 是指公式的这样一个序列 $\\mathscr{A}_1, \\cdots, \\mathscr{A}_n$，使得对于每个 $i (1\\leq i\\leq n)$，$\\mathscr{A}_i$ 或者是 $K_{\\mathscr{L}}$ 的一个公理，或者是由位于 $\\mathscr{A}_i$ 之前的公式通过 MP 或 Generalization 规则得到，或者是 $\\Gamma$ 中的一个成员。记作 $\\Gamma \\vdash_{K_{\\mathscr{L}}} \\mathscr{A}_n$。\n以下用 $K$ 代替 $K_{\\mathscr{L}}$。\nProposition (Soundness):\n(K1)-(K6) 都是逻辑有效的。这可以用定义证明。\n由此可归纳出，$K$ 中的所有定理都是逻辑有效的。\n同样，$K$ 中也有 演绎定理，但不同于 $L$ 中的演绎定理，我们先来看一个特殊情况：\nProposition:\n我们知道对于 $K$ 中的任意公式 $\\mathscr{A}$，都有 $\\{\\mathscr{A}\\}\\vdash_{K} (\\forall x_i)\\mathscr{A}$，但是 $\\vdash_{K} (\\mathscr{A} \\to (\\forall x_i)\\mathscr{A})$ 并不是必然的。\nProof:\n例如 $\\{(x_1 = 0)\\}\\vdash_{K} (\\forall x_1) (x_1 = 0)$，但是并没有 $\\vdash_{K} ((x_1 = 0) \\to (\\forall x_1)(x_1 = 0))$。因为由 满足 定义的第四条可知，不存在赋值满足 $(\\forall x_1)(x_1 = 0)$，所以 $((x_1 = 0) \\to (\\forall x_1)(x_1 = 0))$ 不是逻辑有效的，因此不是 $K$ 中的定理。\nProposition ($K$ 的演绎定理，the deduction theorem)\n如果 $\\Gamma\\cup \\{\\mathscr{A}\\} \\vdash_{K}\\mathscr{B}$，并且演绎过程中不涉及 $\\mathscr{A}$ 中自由的变元，那么 $\\Gamma\\vdash_{K}(\\mathscr{A}\\to\\mathscr{B})$。\nProof: 类似于 $L$ 中演绎定理的证明，对 $\\mathscr{B}$ 进行分类讨论。\n由演绎定理可得推论：\nProposition:\n如果 $\\Gamma\\cup \\{\\mathscr{A}\\} \\vdash_{K}\\mathscr{B}$，并且 $\\mathscr{A}$ 是闭公式，那么 $\\Gamma\\vdash_{K}(\\mathscr{A}\\to\\mathscr{B})$。\n演绎定理的逆定理始终成立，无需加条件：\nProposition:\n如果 $\\Gamma\\vdash_{K}(\\mathscr{A}\\to\\mathscr{B})$，那么 $\\Gamma\\cup \\{\\mathscr{A}\\} \\vdash_{K}\\mathscr{B}$。\n$K$ 中同样有三段论：\nProposition (HS):\n对任意公式 $\\mathscr{A}, \\mathscr{B}, \\mathscr{C}$，如果 $\\{(\\mathscr{A}\\to\\mathscr{B}), (\\mathscr{B}\\to\\mathscr{C})\\}\\vdash_{K}(\\mathscr{A}\\to\\mathscr{C})$。\n同样有替换定理：\nProposition:\n$\\mathscr{A}$ 和 $\\mathscr{B}$ 是 $\\mathscr{L}$ 中的闭公式，如果 $\\mathscr{B}_0$ 是由 $\\mathscr{A}_0$ 通过将其中的 $\\mathscr{A}$ 替换成 $\\mathscr{B}$ 得到的，那么有： $$\\vdash_{K}(\\mathscr{A}\\leftrightarrow \\mathscr{B})\\to (\\mathscr{A}_0\\leftrightarrow \\mathscr{B}_0)$$\nProposition:\n如果 $x_i$ 不在 $\\mathscr{A}$ 中自由出现，那么\n$\\vdash_{K}(\\forall x_i)(\\mathscr{A}\\to \\mathscr{B})\\leftrightarrow (\\mathscr{A}\\to(\\forall x_i)\\mathscr{B})$ $\\vdash_{K}(\\exists x_i)(\\mathscr{A}\\to \\mathscr{B})\\leftrightarrow (\\mathscr{A}\\to(\\exists x_i)\\mathscr{B})$ 如果 $x_i$ 不在 $\\mathscr{B}$ 中自由出现，那么\n$\\vdash_{K}(\\forall x_i)(\\mathscr{A}\\to \\mathscr{B})\\leftrightarrow ((\\exists x_i)\\mathscr{A}\\to\\mathscr{B})$ $\\vdash_{K}(\\exists x_i)(\\mathscr{A}\\to \\mathscr{B})\\leftrightarrow ((\\forall x_i)\\mathscr{A}\\to\\mathscr{B})$ 前束范式 Definition:\n$\\mathscr{L}$ 中的一个公式 $\\mathscr{A}$ 称为 前束范式 (prenex form)，如果它形如: $$(Q_1 x_{i1})(Q_2 x_{i2})(Q_k x_{ik})\\mathscr{D}$$ 其中 $Q_j$ 是 $\\forall$ 或 $\\exists$，$\\mathscr{D}$ 是不带量词的公式。\nProposition:\n$\\mathscr{L}$ 中的任意合式公式 $\\mathscr{A}$，总存在前束范式 $\\mathscr{B}$ 等价于 $\\mathscr{A}$。\nProof: 由上一条 proposition 可证。\nDefinition:\n一个前束范式是一个 $\\Pi_n$ 式，如果它以 $\\forall$ 开头，并有 $n-1$ 次量词交叉。\n一个前束范式是一个 $\\Sigma_n$ 式，如果它以 $\\exists$ 开头，并有 $n-1$ 次量词交叉。\n比如 $(\\exists x_3)(\\forall x_1)(\\forall x_4)(\\forall x_5) A(x_1,x_2,x_3,x_4,x_5)$ 就是 $\\Sigma_2$ 式。\n$K_{\\mathscr{L}}$ 的完备性定理 与 $L$ 一样， $K$ 也满足 soundness, consistency, adequacy，因此 $K$ 中的定理 $\\iff$ 逻辑有效。\nProposition (Adequacy, 完备性):\n所有逻辑有效的公式都是 $K$ 中的定理。也被称为 一阶逻辑的哥德尔完备性定理。\nProof: 过于复杂不便展示。\n模型 Definition:\n如果 $S$ 是一个一阶逻辑系统，$S$ 的一个 模型 (model) 指的是一个解释，使得 $S$ 中的所有定理都为真。\nTheorem:\n如果 $I$ 是一个解释，使得 $S$ 的所有公理都为真，那么 $I$ 是 $S$ 的一个模型。\n数学系统 现在要在 $K$ 的基础上加些数学。\n一阶算术 Definition:\n一阶算术 $\\mathscr{N}$ (first order arithmetic) 是 $K$ 的一个一致扩充，额外引入了以下公理：\n(E7) $x_1=x_1$ (E8) $t_k=u\\to f_i^n(t_1,\\cdots,t_k,\\cdots,t_n)=f^n_i(t_1,\\cdots,u,\\cdots,t_n)$ (E9) $t_k=u\\to (A_i^n(t_1,\\cdots,t_k,\\cdots,t_n)\\to A^n_i(t_1,\\cdots,u,\\cdots,t_n))$ (N1) $(\\forall x_1) \\sim (x_1^{\\prime}=0)$ (N2) $(\\forall x_1)(\\forall x_2)(x_1^{\\prime}=x_2^{\\prime} \\to x_1=x_2)$ (N3) $(\\forall x_1) (x_1+0=x_1)$ (N4) $(\\forall x_1)(\\forall x_2)((x_1+x_2^{\\prime})=(x_1+x_2)^{\\prime})$ (N5) $(\\forall x_1)(x_1\\times 0=0)$ (N6) $(\\forall x_1)(\\forall x_2)((x_1\\times x_2^{\\prime})=(x_1\\times x_2)+x_1)$ (N7) $\\mathscr{A}(0) \\to ((\\forall x_1)(\\mathscr{A}(x_1)\\to\\mathscr{A}(x_1^{\\prime}))\\to (\\forall x_1)\\mathscr{A}(x_1))$，其中 $x_1$ 在 $\\mathscr{A}(x_1)$ 中自由出现。 只有 0 是 $\\mathscr{N}$ 中的符号。而 $1, 2, 3, \\cdots$ 被表示为 $0^{\\prime}, 0^{\\prime\\prime}, 0^{\\prime\\prime\\prime}, \\cdots$\nGödel incompleteness theorem (哥德尔不完全性定理) 这部分先放着吧，有兴趣有必要了再写。\n","date":"2024-04-02T17:00:00+08:00","image":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-2/cover_hu16268775583263298604.png","permalink":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-2/","title":"Theory of Computation Lecture 2"},{"content":"Lecture 1 Mathematical Logic (1) Informal Statement Calculus (非形式的命题演算) 命题、联结词和真值表 自然语言中有许多命题 (statement)，比如“拿破仑死了”。而命题之间又可以通过联结词 (connective) 组成更复杂的命题，比如“拿破仑死了并且世界正在欢腾”。在这里，我们假设所有的命题都是非真即假的。常见的联结词及其对应的含义可见下表：\nMeaning Connectives not $A$ $\\sim A$ $A$ and $B$ $A \\land B$ $A$ or $B$ $A \\lor B$ if $A$ then $B$ $A \\to B$ $A$ if and only if $B$ $A \\leftrightarrow B$ “拿破仑死了并且世界正在欢腾”就可以用$A \\land B$表示。\n各联结词的真值表如下所示：\nNegation:\n$p$ $\\sim p$ $T$ $F$ $F$ $T$ Conjunction:\n$p$ $q$ $p \\land q$ $T$ $T$ $T$ $T$ $F$ $F$ $F$ $T$ $F$ $F$ $F$ $F$ Disjunction:\n$p$ $q$ $p \\lor q$ $T$ $T$ $T$ $T$ $F$ $T$ $F$ $T$ $T$ $F$ $F$ $F$ Conditional:\n$p$ $q$ $p \\to q$ $T$ $T$ $T$ $T$ $F$ $F$ $F$ $T$ $T$ $F$ $F$ $T$ Conditional 的真值表的前两行并不难理解，后两行则可以认为是一种定义上的方便。因为采取这样的定义后，在判断 $p\\to q$ 是否恒为真时只需要判断 $p$ 为真是否始终能推出 $q$ 为真即可，而不需要考察 $p$ 为假的情况，这与先前的认知是一致的。\nBiconditional:\n$p$ $q$ $p \\leftrightarrow q$ $T$ $T$ $T$ $T$ $F$ $F$ $F$ $T$ $F$ $F$ $F$ $T$ 类似表中的字母 $p, q, r, \\cdots$ 称为命题变元 (statement variable)，它们表示任意的非特定的单个命题。而由命题变元和联结词组成的表达式称为命题形式 (statement form)，定义如下：\nDefinition:\n一个 命题形式 是一个含有命题变元和联结词的表达式，并且能用以下规则构成：\n(1) 任何命题变元是一个命题形式。\n(2) 如果 $\\mathscr{A}$ 和 $\\mathscr{B}$ 是命题形式，那么 $(\\sim \\mathscr{A}), (\\mathscr{A} \\land \\mathscr{B}), (\\mathscr{A}\\lor \\mathscr{B}), (\\mathscr{A}\\to\\mathscr{B}), (\\mathscr{A} \\leftrightarrow \\mathscr{B})$ 是命题形式。\n每个命题形式都有其真值表。\nDefinition:\n(1) 一命题形式称为 重言式 (tautology) ，如果对于其中出现的命题变元的各种可能的真值指派，它总取真值为 T 。\n(2) 一命题形式称为 矛盾式 (contradiction) ，如果对于其中出现的命题变元的各种可能的真值指派，它总取真值为 F 。\n$(p\\lor \\sim p)$ 是一个重言式，而 $(q\\land \\sim q)$ 是一个矛盾式。\nDefinition:\n设 $\\mathscr{A}$ 和 $\\mathscr{B}$ 是命题形式，我们说 $\\mathscr{A}$ 逻辑蕴含 (logically implies) $\\mathscr{B}$，如果 $(\\mathscr{A}\\to\\mathscr{B})$ 是一重言式，我们说 $\\mathscr{A}$ 逻辑等值 (logically equivalent) $\\mathscr{B}$，如果 $(\\mathscr{A}\\leftrightarrow\\mathscr{B})$ 是一重言式。\n$(p\\land q)$ 逻辑蕴含 $p$，$(\\sim(p\\land q))$ 逻辑等值 $((\\sim p)\\lor(\\sim q))$。\n运算和代入规则 Proposition:\n如果 $\\mathscr{A}$ 和 $(\\mathscr{A}\\to\\mathscr{B})$ 都是重言式，那么 $\\mathscr{B}$ 也是重言式。\nProof: omitted.\nProposition (Rules for Substitution):\n令 $\\mathscr{A}$ 是一个命题形式，其中有命题变元 $p_1, p_2, \\cdots, p_n$，并且令 $\\mathscr{A}_1, \\mathscr{A}_2, \\cdots, \\mathscr{A}_n$ 是任意命题形式。如果 $\\mathscr{A}$ 是一个重言式，那么由 $\\mathscr{A}$ 通过用 $\\mathscr{A}_i$ 到处去替换每个 $p_i$ 而得到的 $\\mathscr{B}$ 也是一重言式。\nProof: omitted.\n比如，$(p\\land q)$ 逻辑蕴含 $p$，所以对任意 $\\mathscr{A}, \\mathscr{B}$，都有 $(\\mathscr{A}\\land \\mathscr{B})$ 逻辑蕴含 $\\mathscr{A}$。\nProposition (De Morgan\u0026rsquo;s Law):\n令 $\\mathscr{A}_1, \\mathscr{A}_2, \\cdots, \\mathscr{A}_n$ 是任意的命题形式，那么：\n(1) $(\\mathop{\\lor}\\limits_{i=1}^n (\\sim\\mathscr{A}_i))$ 逻辑等值于 $(\\sim(\\mathop{\\land}\\limits_{i=1}^n \\mathscr{A}_i))$\n(2) $(\\mathop{\\land}\\limits_{i=1}^n (\\sim\\mathscr{A}_i))$ 逻辑等值于 $(\\sim(\\mathop{\\lor}\\limits_{i=1}^n \\mathscr{A}_i))$\nProof:\n用数学归纳法，先用 Rules for Substitution 证明 $n=2$ 的情形，再推广至任意正整数 $n$。\n范式 Definition:\n定义只含有联结词 $\\sim, \\land, \\lor$ 的命题形式为 限制的命题形式 (restricted statement form) 。\nProposition:\n每个非矛盾的命题形式逻辑等值于一个限制的命题形式 $\\mathop{\\lor}\\limits_{i=1}^m(\\mathop{\\land}_{j=1}^n Q_{ij})$，其中每个 $Q_{ij}$ 或是一个命题变元，或是一个命题变元的否定。这个形式称为 析取范式 (disjunctive normal form) 。\n每个非重言的命题形式逻辑等值于一个限制的命题形式 $\\mathop{\\land}\\limits_{i=1}^m(\\mathop{\\lor}_{j=1}^n Q_{ij})$，其中每个 $Q_{ij}$ 或是一个命题变元，或是一个命题变元的否定。这个形式称为 合取范式 (conjunctive normal form) 。\nProof:\n仅对析取范式进行证明，合取范式同理。仅需证明任意真值表中只有一行为 $T$ 的命题形式可以用 $\\mathop{\\land}_{j=1}^n Q_{j}$ 表示即可。多行为 $T$ 的命题形式可由一行为 $T$ 的命题形式通过 $\\lor$ 联结得到。\n对于每个命题变元 $q_j$，若其在真值表中的那一行取 $T$，则 $Q_{j}=q_j$，否则取 $Q_{j}=\\sim q_j$。\n这样就可以对任意非矛盾的命题形式构造出一个与其逻辑等值的析取范式。\n综合可得：\nProposition:\n每个真值函数都可以用一个限制的命题形式表示。\n联结词的完全集 Definition:\n一个 联结词的完全集 (adequate set of connectives) 是这样一个集合，使得每个真值函数都能由仅仅含有该集中的联结词的命题形式所表示。\n显然，$\\{\\sim, \\lor, \\land\\}$ 是一个完全集。\nProposition:\n$\\{\\sim, \\land\\}, \\{\\sim, \\lor\\}, \\{\\sim, \\to\\}$ 都是完全集。\nProof:\n可以用 $\\{\\sim, \\lor, \\land\\}$ 是完全集来证明。\n然而，以上介绍的联结词均不能单独构成一个完全集，不过可以通过引入新的联结词来构成只含一个联结词的完全集。\nNOR（即 not+or）\n$p$ $q$ $p \\downarrow q$ $T$ $T$ $F$ $T$ $F$ $F$ $F$ $T$ $F$ $F$ $F$ $T$ NAND（即 not+and）\n$p$ $q$ $p | q$ $T$ $T$ $F$ $T$ $F$ $T$ $F$ $T$ $T$ $F$ $F$ $T$ Proposition:\n$\\{\\downarrow\\}, \\{ | \\}$ 都是联结词的完全集。\nProof:\n可以用已知的完全集来证明。\n但是仅使用一个联结词可能会导致表达式非常复杂，比如仅用 $\\downarrow$ 构造出 $(p\\to q)$：\n$$\\{(p\\downarrow p)\\downarrow [(q\\downarrow q)\\downarrow (q\\downarrow q)]\\}\\downarrow \\{(p\\downarrow p)\\downarrow [(q\\downarrow q)\\downarrow (q\\downarrow q)]\\}$$\n论证和有效性 Definition:\n论证形式 (argument form) 定义为类似：\n$$\\mathscr{A}_1, \\mathscr{A}_2, \\cdots, \\mathscr{A}_n; \\therefore \\mathscr{A}$$\n的形式。\n我们称一个论证形式是 无效 (nonvalid) 的，如果存在一种对命题变元的真值指派，使得每个 $\\mathscr{A}_i$ 均取值 $T$，但是 $\\mathscr{A}$ 取值 $F$。否则称其是 有效 (valid) 的。\nProposition:\n论证形式 $$\\mathscr{A}_1, \\mathscr{A}_2, \\cdots, \\mathscr{A}_n; \\therefore \\mathscr{A}$$\n是有效的，当且仅当命题形式\n$$((\\mathscr{A}_1\\land \\mathscr{A}_2 \\land \\cdots \\land \\mathscr{A}_n) \\to \\mathscr{A})$$\n是一个重言式。\nProof: omitted.\nFormal Statement Calculus (形式的命题演算) 命题演算形式系统 $L$ 我们主要关注两个特殊的形式系统（命题演算形式系统、谓词演算形式系统），我们先给出形式系统的一般性定义：\nDefinition:\n一个 形式系统 (formal system) 由以下几部分构成：\n(1) 一个符号库 (an alphabet of symbols)。\n(2) 这些符号组成的有限字符串（称为合式公式，well-formed fomula）的一个集合。\n(3) 合式公式的一个集合，称为公理 (axiom)。\n(4) 有限个演绎规则 (rules of deduction) 组成的集合。\n接下来给出命题演算形式系统的定义：\nDefinition:\n一个 命题演算形式系统 L (formal system L of statement calculus) 由以下几部分构成：\n(1) 一个无限的符号库：\n$$\\sim, \\to, (, ), p_1, p_2, p_3, \\cdots$$\n(2) 一个合式公式的集合，由以下规则确定：\n对于每个 $i\\geq 1$, $p_i$ 是合式公式。 如果 $\\mathscr{A}$ 和 $\\mathscr{B}$ 是合式公式，那么 $(\\sim \\mathscr{A})$ 和 $(\\mathscr{A}\\to\\mathscr{B})$ 也是合式公式。 所有合式公式都由以上两条规则产生。 (3) 一个公理的集合，对于任何合式公式 $\\mathscr{A}, \\mathscr{B}, \\mathscr{C}$，以下公式是 $L$ 的公理：\n(L1) $(\\mathscr{A}\\to(\\mathscr{B}\\to\\mathscr{A}))$. (L2) $((\\mathscr{A}\\to(\\mathscr{B}\\to\\mathscr{C}))\\to ((\\mathscr{A}\\to\\mathscr{B})\\to(\\mathscr{A}\\to\\mathscr{C})))$. (L3) $(((\\sim \\mathscr{A})\\to(\\sim \\mathscr{B}))\\to (\\mathscr{B}\\to\\mathscr{A}))$. (4) $L$ 中仅有一条演绎规则，称为 modus ponens (MP)，即对于 $L$ 中的任何公式 $\\mathscr{A}$ 和 $(\\mathscr{A}\\to\\mathscr{B})$，$\\mathscr{B}$ 也是 $L$ 中的一个公式。\n目前而言，在考察 $L$ 中的公式及其演绎时，不应考虑其是否“正确”，而应将其完全视为一文字游戏，只能通过已有公式与演绎规则推出新的合式公式。而至于新推出的合式公式的“正确性”（即新公式为一重言式），则由“公理为重言式”（可直接验证），以及“演绎规则保持公式的重言性”（先前已证明）来保证。\nDefinition:\n$L$ 中的一个 证明 (proof) 是指公式的这样一个序列 $\\mathscr{A}_1, \\cdots, \\mathscr{A}_n$，使得对于每个 $i (1\\leq i\\leq n)$，或者 $\\mathscr{A}_i$ 是 $L$ 的一个公理，或者 $\\mathscr{A}_i$ 可由序列中位于 $\\mathscr{A}_i$ 前面的两个公式 $\\mathscr{A}_j, \\mathscr{A}_k (j,k\u0026lt;i)$ 通过 MP 得到。这样的证明称为在 $L$ 中 $\\mathscr{A}_n$ 的一个证明，$\\mathscr{A}_n$ 称为 $L$ 中的一个 定理 (theorem)。\n$\\mathscr{A}_n$ 是 $L$ 中的一个定理可记作 $\\vdash_L \\mathscr{A}_n$。\nExercise:\nProve: $\\vdash_L (\\mathscr{A}\\to\\mathscr{A})$.\nSolution:\n$$\\begin{align*} (1)\u0026amp;\\quad (\\mathscr{A}\\to((\\mathscr{A}\\to\\mathscr{A})\\to\\mathscr{A})) \u0026amp;\u0026amp;(L1)\\\\ (2)\u0026amp;\\quad ((\\mathscr{A}\\to(\\mathscr{A}\\to\\mathscr{A}))\\to(\\mathscr{A}\\to\\mathscr{A})) \u0026amp;\u0026amp;(1)+(L2)+MP\\\\ (3)\u0026amp;\\quad (\\mathscr{A}\\to(\\mathscr{A}\\to\\mathscr{A})) \u0026amp;\u0026amp;(L1)\\\\ (4)\u0026amp;\\quad (\\mathscr{A}\\to\\mathscr{A}) \u0026amp;\u0026amp;(2)+(3)+MP \\end{align*}$$\nDefinition:\n令 $\\Gamma$ 是 $L$ 中的公式集（可以是也可以不是 $L$ 中的公理或定理）。$L$ 中的公式序列 $\\mathscr{A}_1, \\cdots, \\mathscr{A}_n$ 是从 $\\Gamma$ 的一个 演绎 (deduction)，如果每个 $i (1\\leq i\\leq n)$，下列之一成立：\n$\\mathscr{A}_i$ 是 $L$ 的一个公理。 $\\mathscr{A}_i$ 是 $\\Gamma$ 中的一个成员。 $\\mathscr{A}_i$ 可由序列中在 $\\mathscr{A}_i$ 前的两个公式通过 MP 得到。 记作 $\\Gamma\\vdash_L \\mathscr{A}_n$。\nExercise:\nProve: $\\{\\mathscr{A},(\\mathscr{B}\\to(\\mathscr{A}\\to\\mathscr{C}))\\}\\vdash_L (\\mathscr{B}\\to\\mathscr{C})$.\nSolution:\n$$\\begin{align*} (1)\u0026amp;\\quad \\mathscr{A} \u0026amp;\u0026amp;假设\\\\ (2)\u0026amp;\\quad (\\mathscr{A}\\to(\\mathscr{B}\\to\\mathscr{A})) \u0026amp;\u0026amp;(L1)\\\\ (3)\u0026amp;\\quad (\\mathscr{B}\\to\\mathscr{A}) \u0026amp;\u0026amp;(1)+(2)+MP\\\\ (4)\u0026amp;\\quad (\\mathscr{B}\\to(\\mathscr{A}\\to\\mathscr{C})) \u0026amp;\u0026amp;假设\\\\ (5)\u0026amp;\\quad ((\\mathscr{B}\\to\\mathscr{A})\\to(\\mathscr{B}\\to\\mathscr{C})) \u0026amp;\u0026amp;(4)+(L2)+MP\\\\ (6)\u0026amp;\\quad (\\mathscr{B}\\to\\mathscr{C}) \u0026amp;\u0026amp;(3)+(5)+MP \\end{align*}$$\n注意，这样子推出来的公式并不一定是 $L$ 中的一个定理。\nProposition (The deduction theorem):\n$\\Gamma\\cup\\{\\mathscr{A}\\}\\vdash_L\\mathscr{B}$ 当且仅当 $\\Gamma\\vdash_L(\\mathscr{A}\\to\\mathscr{B})$，其中 $\\mathscr{A}$ 和 $\\mathscr{B}$ 是 $L$ 中的公式，$\\Gamma$ 是 $L$ 的公式集（可能是空集）。\nProof:\n仅证明从左至右的部分，从右至左可以直接运用 MP。运用数学归纳法证明从左至右的部分，假设从 $\\Gamma\\cup\\{\\mathscr{A}\\}$ 到 $\\mathscr{B}$ 的演绎是一个有 $n$ 个公式的序列。\n(1) $n=1$ 时，三种情况：\n$\\mathscr{B}$ 是 $L$ 中的一条公理，则有： $$\\begin{align*} (1)\u0026amp;\\quad \\mathscr{B} \u0026amp;\u0026amp;公理\\\\ (2)\u0026amp;\\quad (\\mathscr{B}\\to (\\mathscr{A}\\to\\mathscr{B})) \u0026amp;\u0026amp;(L1)\\\\ (3)\u0026amp;\\quad (\\mathscr{A}\\to\\mathscr{B}) \u0026amp;\u0026amp; (1)+(2)+MP \\end{align*}$$ $\\mathscr{B}\\in \\Gamma$，则同上一种情况。 $\\mathscr{B}=\\mathscr{A}$，先前已证 $\\vdash_L (\\mathscr{A}\\to\\mathscr{A})$。 (2) 假设从 $\\Gamma\\cup\\{\\mathscr{A}\\}$ 到 $\\mathscr{B}$ 的演绎长度 $\u0026lt; n$ 时，proposition 成立。可以假设 $\\mathscr{B}$ 不是 $L$ 中的公理，不在 $\\Gamma$ 中，不为 $\\mathscr{A}$，那么在 $\\Gamma\\cup\\{\\mathscr{A}\\}$ 到 $\\mathscr{B}$ 的演绎序列中， $\\mathscr{B}$ 只可能由先前的两个公式 $\\mathscr{C}$ 和 $(\\mathscr{C}\\to\\mathscr{B})$ 通过 MP 得到。故而我们有 $\\Gamma\\cup \\{\\mathscr{A}\\} \\vdash_L \\mathscr{C}$ 和 $\\Gamma\\cup \\{\\mathscr{A}\\} \\vdash_L (\\mathscr{C}\\to\\mathscr{B})$。由归纳假设，$\\Gamma\\vdash_L(\\mathscr{A}\\to\\mathscr{C}), \\Gamma\\vdash_L(\\mathscr{A}\\to(\\mathscr{C}\\to\\mathscr{B}))$。由二者，通过 (L2) 和 MP 可得 $\\Gamma\\vdash_L(\\mathscr{A}\\to\\mathscr{B})$。\n这个 proposition 被称为 演绎定理。\nProposition (Hypothetical Syllogism (HS)):\n对任何 $L$ 中的公式 $\\mathscr{A}, \\mathscr{B}, \\mathscr{C}$：\n$$\\{(\\mathscr{A}\\to\\mathscr{B}), (\\mathscr{B}\\to\\mathscr{C})\\} \\vdash_L (\\mathscr{A}\\to\\mathscr{C})$$\nProof:\n$$\\begin{align*} (1)\u0026amp;\\quad (\\mathscr{A}\\to\\mathscr{B}) \u0026amp;\u0026amp;假设\\\\ (2)\u0026amp;\\quad (\\mathscr{B}\\to \\mathscr{C}) \u0026amp;\u0026amp;假设\\\\ (3)\u0026amp;\\quad \\mathscr{A} \u0026amp;\u0026amp;假设\\\\ (4)\u0026amp;\\quad \\mathscr{B} \u0026amp;\u0026amp;(1)+(3)+MP \\\\ (5)\u0026amp;\\quad \\mathscr{C} \u0026amp;\u0026amp;(2)+(4)+MP \\\\ \\end{align*}$$ 所以有 $\\{(\\mathscr{A}\\to\\mathscr{B}), (\\mathscr{B}\\to\\mathscr{C}), \\mathscr{A}\\} \\vdash_L \\mathscr{C}$，根据演绎定理，可得 $\\{(\\mathscr{A}\\to\\mathscr{B}), (\\mathscr{B}\\to\\mathscr{C})\\} \\vdash_L (\\mathscr{A}\\to\\mathscr{C})$。\n此 proposition 被称为 假言三段论。\nExercise:\nProve:\n(1) $\\vdash_L(\\sim\\mathscr{B}\\to(\\mathscr{B}\\to\\mathscr{A}))$.\n(2) $\\vdash_L((\\sim\\mathscr{A}\\to\\mathscr{A})\\to\\mathscr{A})$.\nProof:\n(1) $$\\begin{align*} (1)\u0026amp;\\quad (\\sim \\mathscr{B}\\to(\\sim\\mathscr{A}\\to\\sim\\mathscr{B})) \u0026amp;\u0026amp;(L1)\\\\ (2)\u0026amp;\\quad ((\\sim\\mathscr{A}\\to\\sim\\mathscr{B})\\to(\\mathscr{B}\\to\\mathscr{A})) \u0026amp;\u0026amp;(L2)\\\\ (3)\u0026amp;\\quad (\\sim \\mathscr{B}\\to(\\mathscr{B}\\to\\mathscr{A})) \u0026amp;\u0026amp;(1)+(2)+HS\\\\ \\end{align*}$$ (2) 由演绎定理，只需证明 $\\{(\\sim \\mathscr{A}\\to\\mathscr{A})\\}\\vdash_L \\mathscr{A}$： $$\\begin{align*} (1)\u0026amp;\\quad (\\sim\\mathscr{A}\\to\\mathscr{A}) \u0026amp;\u0026amp;假设\\\\ (2)\u0026amp;\\quad (\\sim\\mathscr{A}\\to(\\sim\\sim(\\sim\\mathscr{A}\\to\\mathscr{A})\\to\\sim\\mathscr{A})) \u0026amp;\u0026amp;(L1)\\\\ (3)\u0026amp;\\quad ((\\sim\\sim(\\sim\\mathscr{A}\\to\\mathscr{A})\\to\\sim\\mathscr{A}) \\to (\\mathscr{A}\\to\\sim(\\sim\\mathscr{A}\\to\\mathscr{A}))) \u0026amp;\u0026amp;(L3)\\\\ (4)\u0026amp;\\quad (\\sim\\mathscr{A}\\to (\\mathscr{A}\\to\\sim(\\sim\\mathscr{A}\\to\\mathscr{A}))) \u0026amp;\u0026amp;(2)+(3)+HS\\\\ (5)\u0026amp;\\quad ((\\sim\\mathscr{A}\\to \\mathscr{A})\\to (\\sim\\mathscr{A}\\to\\sim(\\sim\\mathscr{A}\\to\\mathscr{A}))) \u0026amp;\u0026amp;(4)+(L2)+MP\\\\ (6)\u0026amp;\\quad ((\\sim\\mathscr{A}\\to\\sim(\\sim\\mathscr{A}\\to\\mathscr{A})) \\to ((\\sim\\mathscr{A}\\to\\mathscr{A})\\to\\mathscr{A})) \u0026amp;\u0026amp;(L2)\\\\ (7)\u0026amp;\\quad ((\\sim\\mathscr{A}\\to\\mathscr{A})\\to((\\sim\\mathscr{A}\\to\\mathscr{A})\\to\\mathscr{A})) \u0026amp;\u0026amp;(5)+(6)+HS\\\\ (8)\u0026amp;\\quad \\mathscr{A} \u0026amp;\u0026amp;(1)+(7)+2\\times MP \\end{align*}$$\n$L$ 的完备性定理 Definition:\n$L$ 的一个 赋值 (valuation) 是一个函数 $v$，它的定义域是 $L$ 的公式，值域是集合 $\\{T, F\\}$，并且使得对 $L$ 的任意公式 $\\mathscr{A}, \\mathscr{B}$：\n(1) $v(\\mathscr{A})\\neq v(\\sim\\mathscr{A})$.\n(2) $v(\\mathscr{A}\\to\\mathscr{B})=F$ 当且仅当 $v(\\mathscr{A})=T$ 和 $v(\\mathscr{A})=F$。\nDefinition:\n$L$ 中的一个公式 $\\mathscr{A}$ 是一个 重言式，如果对每个赋值 $v$，都有 $v(\\mathscr{A})=T$。\nProposition (Soundness，可靠性):\n$L$ 中的每个定理都是重言式。\nProof: 数学归纳法。\nProposition (Consistency, 一致性):\n$L$ 中的不存在公式 $\\mathscr{A}$，使得 $\\mathscr{A}$ 和 $(\\sim\\mathscr{A})$ 都是 $L$ 中的定理。\nProof: Soundness 可以推出 consistency。\nProposition (Adequacy, 完备性):\n如果 $\\mathscr{A}$ 是 $L$ 中的一个公式，且为重言式，那么 $\\mathscr{A}$ 是 $L$ 中的一个定理。\n为了证明 adequacy，我们需要引入新的概念。\nDefinition:\n$L$ 的一个 扩充 (extension) 是一个形式系统，它通过修改或者扩大公理组使得原来所有的定理仍是定理（也可能引入新的定理）而得到。\n注意，此处说的是 修改 或 扩大，而并不仅仅是 扩大。 一个与 $L$ 没有共同公理的形式系统也可能是 $L$ 的一个扩充。\nProposition:\n$L$ 的一个扩充 $L^*$ 是一致的，当且仅当存在一个公式，它不是 $L^*$ 中的定理。\nProof:\n(1) 如果 $L^*$ 一致，那么对任意公式 $\\mathscr{A}$，总有 $\\mathscr{A}$ 或 $(\\sim\\mathscr{A})$ 不是 $L^*$ 中的定理。\n(2) 如果 $L^*$ 不一致，我们证明任意公式 $\\mathscr{A}$ 都是 $L^*$ 中的定理。因为存在 $\\mathscr{B}$，使得 $\\mathscr{B}$ 和 $(\\sim\\mathscr{B})$ 都是 $L^*$ 中的定理。先前证过 $\\vdash_L (\\sim\\mathscr{B}\\to(\\mathscr{B}\\to\\mathscr{A}))$，所以 $\\vdash_{L^*} (\\sim\\mathscr{B}\\to(\\mathscr{B}\\to\\mathscr{A}))$。再应用两次 MP 即可得到 $\\vdash_{L^*} \\mathscr{A}$。\nProposition:\n令 $L^*$ 是 $L$ 的一个一致的扩充，并且 $\\mathscr{A}$ 是一个公式，它不是 $L^*$ 的一条定理，那么将 $L^*$ 补充公理 $(\\sim\\mathscr{A})$ 得到的系统 $L^{**}$ 也是一致的。\nProof:\n如果 $L^{**}$ 不一致，那么 $\\vdash_{L^{**}} \\mathscr{A}$，即 $\\{\\sim\\mathscr{A}\\}\\vdash_{L^*} \\mathscr{A}$。\n由 deduction theorem，$\\vdash_{L^*} (\\sim\\mathscr{A}\\to\\mathscr{A})$。\n先前有 $\\vdash_L ((\\sim\\mathscr{A}\\to\\mathscr{A})\\to\\mathscr{A})$，故 $\\vdash_{L^*} \\mathscr{A}$。矛盾\n由此可知，我们可以依次考察所有公式 $\\mathscr{A}$，将 $\\mathscr{A}$ 或 $(\\sim\\mathscr{A})$ 加入公理组中，最终得到一个一致的形式系统，且有对于所有公式 $\\mathscr{A}$，都有 $\\mathscr{A}$ 或 $(\\sim\\mathscr{A})$ 是它的定理。\nDefinition:\n$L$ 的一个扩充是 完全 (complete) 的，如果对每个公式 $\\mathscr{A}$，都有 $\\mathscr{A}$ 或 $(\\sim\\mathscr{A})$ 是该扩充的一条定理。\nProposition:\n存在一个 $L$ 的一致完全扩充。\nProposition:\n如果 $L^*$ 是 $L$ 的一个一致完全扩充，那么存在一种赋值使得 $L^*$ 中的每个定理都取值 $T$。\nProof:\n定义 $v(\\mathscr{A})=T$，如果 $\\mathscr{A}$ 是 $L^*$ 中的一条定理，反之定义 $v(\\mathscr{A})=F$。用 valuation 的定义可以证明这样的函数是一个 valuation。\nProof of the adequacy theorem of L:\n假设 $\\mathscr{A}$ 是重言式，但不是 $L$ 的定理，那么可以扩充 $L$ 为 $L\\cup\\{\\sim\\mathscr{A}\\}$ 再到一个一致完全扩充 $L^*$。$\\vdash_{L^*}(\\sim\\mathscr{A})$，故必定存在赋值 $v$ 使得 $v(\\sim\\mathscr{A})=T$。矛盾。\n","date":"2024-03-26T17:00:00Z","image":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-1/cover_hu6051549307129623916.png","permalink":"https://suz-tsinghua.github.io/p/theory-of-computation-lecture-1/","title":"Theory of Computation Lecture 1"},{"content":"Lecture 1.7 Group convolutions are all you need! 这一节要证明的是，对于一个线性映射，如果希望它是 equivariant 的，那它必须是一个 group convolution。\nClassical fully connected layer 先看传统的全连接层：\n$$\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\end{pmatrix} = \\varphi\\left(\\begin{pmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14} \u0026amp; w_{15} \u0026amp; \\cdots \\\\ w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24} \u0026amp; w_{25} \u0026amp; \\cdots \\\\ w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \u0026amp; w_{35} \u0026amp; \\cdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ \\vdots \\end{pmatrix} + \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\end{pmatrix}\\right)$$\n其中 $\\varphi$ 为某个非线性函数。如果希望全连接层是 equivariant 的，则需要类似有如下的形式：\n$$\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\end{pmatrix} = \\varphi\\left(\\begin{pmatrix} w_{1} \u0026amp; w_{2} \u0026amp; w_{3} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \\\\ 0 \u0026amp; w_{1} \u0026amp; w_{2} \u0026amp; w_{3} \u0026amp; 0 \u0026amp; \\cdots \\\\ 0 \u0026amp; 0 \u0026amp; w_{1} \u0026amp; w_{2} \u0026amp; w_{3} \u0026amp; \\cdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ \\vdots \\end{pmatrix} + \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\end{pmatrix}\\right)$$\n这其实就是一维的卷积。\nLinear maps in continuous space 对于从 $\\mathbb{R}^n$ 到 $\\mathbb{R}^m$ 的线性映射，我们知道其必须是 matrix-vector multiplication：\n$$y_j = \\sum_i K_{i,j} x_i$$\n对于一个 feature map $f\\in \\mathbb{L}_2(X)$，我们也可以将其视为一个 vector，维数就是 $f$ 的定义域 $X$ 的大小 $|X|$。考虑将一个 feature map $f^{in}\\in \\mathbb{L}_2 (X)$ 线性映射到另一个 feature map $f^{out}\\in \\mathbb{L}_2 (Y)$，当定义域是连续的时候，仿照 matrix-vector multiplication，可知线性映射有如下形式：\n$$ f^{out}(y) = (Kf)(y) = \\int_X k(y,x) f(x) \\mathrm{ d}x $$\n对于连续空间中的线性映射，有如下定理：\nTheorem (G-convs are all you need):\n$\\mathscr{K}: \\mathbb{L}_2(X)\\rightarrow \\mathbb{L}_2(Y)$ 是一个线性映射，且 $X$ 和 $Y$ 都是群 $G$ 的 homogeneous space。\n随意取 $y_0\\in Y$，令 $H=Stab_G(y_0)$，那么 $Y\\equiv G/H$。对于 $\\forall y\\in Y$，令 $g_y\\in G$ 为某个满足 $y=g_y y_0$ 的元素。\n$\\mathscr{K}$ is equivariant iff:\n$\\mathscr{K}$ 是一个 group convolution: $[\\mathscr{K}f] (y)=\\int_{X} \\frac{1}{|\\det g_y|}k(g_y^{-1}x)f(x)\\mathrm{ d}x$\nkernel 满足对称约束: $\\forall h\\in H, \\frac{1}{|\\det h|}k(h^{-1}x) = k(x)$\n定理中的 $|\\det g_y|$ 即为 $g_y$ 的雅可比行列式的绝对值，理论上来说其会随 $x$ 而变，但在大部分实际情况中，$|\\det g_y| = 1$。\n以二维图像的 convolution 为例子来解释一下这个定理。此时 $X$ 与 $Y$ 均为 $\\mathbb{R}^2$， $G$ 即为二维 translation group。若希望 linear map 是 equivariant 的，必须有 $[\\mathscr{K}f] (y)=\\int_{X} k(x - y)f(x)\\mathrm{ d}x$ 这样的卷积形式（平移变换的雅可比行列式大小为1）。\nProof:\n只证明一边，即 equivariant 的 $\\mathscr{K}$ 必须满足定理中的两个条件。\n已知 $\\mathscr{K}$ 具有形式\n$$[\\mathscr{K}f] (y) = \\int_X \\tilde{k} (y,x)f(x)\\mathrm{ d} x \\tag{1}$$\n希望 $\\mathscr{K}$ is equivariant，即\n$$\\forall g\\in G, \\forall f\\in\\mathbb{L}_2(X),\\quad (\\mathscr{K}\\circ \\mathscr{L}_g^{G\\rightarrow \\mathbb{L}_2(X)})(f)=(\\mathscr{L}_g^{G\\rightarrow \\mathbb{L}_2(Y)}\\circ \\mathscr{K})(f)$$\n代入 $\\mathscr{K}$ 和 $\\mathscr{L}$ 的表达式\n$$\\int_{X}\\tilde{k}(y,x) f(g^{-1}x)\\mathrm{ d}x=\\int_{X}\\tilde{k}(g^{-1}y,x) f(x)\\mathrm{ d}x$$\n在 RHS 中用 $g^{-1}x$ 替换 $x$\n$$\\int_{X}\\tilde{k}(y,x) f(g^{-1}x)\\mathrm{ d}x=\\int_{X}\\tilde{k}(g^{-1}y,g^{-1}x) f(g^{-1}x)\\mathrm{ d}(g^{-1}x)$$\n代入雅可比行列式\n$$\\int_{X}\\tilde{k}(y,x) f(g^{-1}x)\\mathrm{ d}x=\\int_{X}\\tilde{k}(g^{-1}y,g^{-1}x) f(g^{-1}x)\\frac{1}{|\\det g|}\\mathrm{ d}x$$\n这对任意 $f$ 都要成立，故\n$$\\tilde{k}(y,x) = \\frac{1}{|\\det g|} \\tilde{k}(g^{-1}y,g^{-1}x)$$\n又已知 $G$ acts transitively on $Y$，$\\exists g_y,$ s.t. $y=g_y y_0$\n$$\\tilde{k}(y,x) = \\tilde{k}(g_y y_0,x) = \\frac{1}{|\\det g_y|} \\tilde{k}(y_0,g_y^{-1}x)$$\n定义 $k(g_y^{-1}x) := \\tilde{k}(y_0,g_y^{-1}x)$，则有\n$$\\tilde{k}(y,x) = \\frac{1}{|\\det g_y|} k(g_y^{-1}x)$$\n代回到 (1) 式中，就得到了\n$$[\\mathscr{K}f] (y)=\\int_{X} \\frac{1}{|\\det g_y|}k(g_y^{-1}x)f(x)\\mathrm{ d}x \\tag{2}$$\n$\\forall h\\in H, k(h^{-1}x) = \\tilde{k}(y_0, h^{-1}x) = |\\det h| \\tilde{k}(h y_0, x) = |\\det h| k(x)$，即\n$$\\forall h\\in H, \\quad \\frac{1}{|\\det h|}k(h^{-1}x) = k(x) \\tag{3}$$\n例子 我们以几个例子来更好地理解这个定理的内容及应用。\nClassical convolution 当 $G$ 为 translation group 的时候，传统的二维图像卷积 kernel 满足对称约束。因为此时 $H=\\{e\\}$，显然有 $k(ex)=k(x)$。\n当 $G$ 为 roto-translation group 的时候，传统的二维图像卷积就不满足对称约束了。对于任意 $y_0$ ，roto-translation group 中有很多元素可以保持 $y_0$不变。因此构造出来的 $H$ 中除了 $e$ 还有别的元素，这些元素不能满足 $k(hx)=k(x)$，所以 $G$ 为 roto-translation group 的时候，传统卷积操作不再具有等变性。\nLifting convolution \u0026amp; Group convolution Lifting convolution 和 group convolution 的 $Y$ 均为 $SE(2)$，对于 roto-translation group，$H=\\{e\\}$。所以对称约束始终满足，不对 $k$ 有任何约束。二者均具有等变性。\n结论 因此，当我们希望构造一个 $G$-equivariant convolution 时，最好的办法就是直接让 $Y=G$，这样的话 $H=\\{e\\}$，对称约束始终满足，kernel $k$ 不受任何约束，most expressive。\nGroup convolutions are all you need!\n","date":"2024-01-23T12:00:00+08:00","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.7/","title":"Group Equivariant Deep Learning Lecture 1.7"},{"content":"Lecture 1.6 Group Theory | Homogeneous/quotient spaces 这一节为后面的内容做准备，再来介绍一下 group theory。\nGroup action group action 是一个 operator $\\odot: G\\times X\\rightarrow X$，其满足以下性质：\n$$\\forall g, g^{\\prime}\\in G, \\forall x\\in X, \\quad g\\odot (g^{\\prime}\\odot x)=(gg^{\\prime})\\odot x$$\nTransitive action 称一个 group action $\\odot$ 具有传递性 (transitivity)，当其满足：\n$$\\forall x_0, x\\in X, \\exists g\\in G, \\quad x=g\\odot x_0$$\n比如 $(\\mathbb{R}^2,+)$ 作用在 $\\mathbb{R}^2$ 上，$SE(2)$ 作用在 $\\mathbb{R}^2$ 上都是 transitive 的。但 $SO(2)$ 作用在 $\\mathbb{R}^2$ 上不是。\nHomogeneous space 如果一个群 $G$ 能够传递地作用在空间 $X$ 上，则称 $X$ 是 $G$ 的一个 homogeneous space。\n这个性质很重要，因为卷积可以被视为对 kernel 作用 $G$ 中的每个元素后与输入 $f$ 进行点积，只有 $f$ 的定义域是 $G$ 的一个 homogeneous space 的时候，才能保证 $f$ 中的所有信息能被卷积提取出来。\n比如对于普通的二维卷积操作，正是因为 $\\mathbb{R}^2$ 是 $(\\mathbb{R}^2,+)$ 的一个 homogeneous space，kernel 才能遍历 $f$ 的每个点。对于 lifting correlation 也是一样，$\\mathbb{R}^2$ 是 $SE(2)$ 的一个 homogeneous space。\n可以定义三维旋转群 $SO(3)$ 作用在三维向量（起点为原点） $\\mathbb{R}^3$ 上的 group action，roll 对应向量的伸缩，pitch 和 yaw 对应向量方向的旋转。若将作用后的三维向量投影到三维球面 $S^2$ 上，可以发现 $S^2$ 是 $SO(3)$ 的一个 homogeneous space。\nQuotient space 对于群 $G$ 以及其正规子群 $H$，定义 quotient space/group 为：\n$$G/H=\\left\\{\\{gh|h\\in H\\}| g\\in G\\right\\}$$\n$G/H$ 中的元素是集合。\n其实在 $SO(3)$ 作用在 $S^2$ 的过程中， roll 并没有起作用（无论向量如何伸缩，都会被投影回 $S^2$），所以有 $S^2\\cong SO(3)/SO(2)$。\nStabilizer 对于集合 $X$ 中的元素 $x_0$ 以及作用在 $X$ 上的群 $G$，定义 $x_0$ 的 stabilizer 为：\n$$Stab_G(x_0)=\\{g|gx_0=x_0\\}$$\n即那些能让 $x_0$ 保持稳定的元素 $g$。\nHomogeneous space $\\equiv$ Quotient space Slides 中的表述为：Any quotient space is a homogeneous space. Any homogeneous space is a quotient space.\n显然，quotient space 是一个 homogeneous space，因为 $G$ 可以传递地作用在 $G/H$ 上。\n但第二句话的表述并不严谨，quotient space 一定是一个 group，但 homogeneous space 不一定是个 group。因此对于一个给定的 homogeneous space，无法构造一个 quotient space，使得二者同构。但可以构造一个 quotient space，使得二者之间存在 bijection。\nLecture notes 中对第二句话的表述为：\nProof:\n对于任意一个 $x_0\\in X$，都可以构造一个映射 $f: G/H \\rightarrow X, gH \\mapsto gx_0$。\n这个映射是良定义的，因为如果 $\\exists g_1, g_2,$ s.t. $g_1 H=g_2 H$，那么 $\\exists h\\in H,$ s.t. $g_1=g_2 h$。所以 $g_1 x_0 = g_2 h x_0 = g_2 x_0$。\n因为 $X$ 是一个 homogeneous space，所以 $f$ 是满射。\n假设 $g_1 x_0=g_2 x_0$，那么 $g_2^{-1} g_1 x_0 = x_0, g_2^{-1} g_1\\in H$，所以 $g_1 H = g_2 H$。$f$ 是单射。\n$f$ is a bijection.\n","date":"2024-01-22T18:00:00+08:00","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.6/","title":"Group Equivariant Deep Learning Lecture 1.6"},{"content":"Lecture 1.5 A brief history of G-CNNs 本节简单讲了下 G-CNN 的发展历程，从 discrete group 一步步到更一般的 group。感兴趣可以自己看：\nslides链接\n","date":"2024-01-22T17:00:00+08:00","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.5/","title":"Group Equivariant Deep Learning Lecture 1.5"},{"content":"Lecture 1.4 SE(2) Equivariant NN Example | histopathology 本节以有丝分裂细胞识别为例（即给一张细胞的图片，判断其是否正在进行有丝分裂），构造一个 rotation invariant CNN。对于一张图片的不同旋转版本，网络需要返回同样的输出。\nInvariance 我们在 Lecture 1.2 节已经介绍过等变性 (equivariance)，即对于一个 operator $\\Phi: X\\rightarrow Y$，其具有以下的性质：\n$$\\Phi\\circ \\rho^X(g)=\\rho^Y(g)\\circ \\Phi$$\n而不变性 (invariance) 则是：\n$$\\Phi\\circ \\rho^X(g)=\\Phi$$\n即不论输入在经过 $\\Phi$ 之前是否经过 $\\rho^X(g)$，其最终输出都是一样的。\n网络架构 网络架构如上图所示，对于一张图片，将其经过一层 lifting convolution 和几层 group convolution 后，每个 output channel 都只含有一个 $ 1\\times |G| $ 的向量。通过这样的操作抹去了 $x, y$，只留下了 $\\theta$ 轴。由于中间的每一层都是 equivariant 的，因此对于不同旋转角度的输入，这一层的输出仅仅是在 $\\theta$ 轴上有不同的平移。为了消除 $\\theta$ 轴向平移的影响，在后面再加一层 projection layer （或者说是 pooling layer），在 $\\theta$ 方向取 mean 或 max 等等。\n此时我们就得到了一个 $1 \\times \\# \\text{ output channels}$ 的向量，此时的输出结果是 roto-invariant 的，可以直接在后面加一个 linear 层进行 classification。\n也可以在每层之后加一层 pointwise non-linear layer，这显然不会改变 equivariance。\nNote: 这样构造出来的 CNN 的网络大小是正比于 $|G|$ 的。比如如果希望构造一个完全 rotational invariant 的网络，$|G|$ 的大小即旋转的度数的个数，是无穷的，这样的网络无法达到效果。这里实现的只是取了几个旋转角度，比如 $0, \\frac{\\pi}{2}, \\pi, \\frac{3\\pi}{2}$。后续讲的网络可以解决这个问题。 From rotation to scale equivariant CNNs 上面我们展示了 roto-translation equivariant CNN 的作用，除此以外，scale equivariant CNN 也有其用处， 比如可以识别不同大小的人脸、不同音量音高的音频。\n效果 G-CNN 可以保证输出的等变性。 G-CNN 可以获得比单纯用 data-augmentation 更好的效果。比如要进行人脸识别，训练集里某一张图片有两张人脸，data-augmentation 可以识别将两张人脸作为一个整体旋转得到的图片，而 G-CNN 可以识别两张人脸分别旋转得到的图片。 G-CNN 增加了 sample efficiency。 ","date":"2024-01-22T12:00:00+08:00","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.4/","title":"Group Equivariant Deep Learning Lecture 1.4"},{"content":"Lecture 1.3 Regular group convolutions | Template matching viewpoint Cross-correlations 定义 kernel $k\\in \\mathbb{L}_2(\\mathbb{R}^2)$ 和二维图像 $f\\in \\mathbb{L}_2(\\mathbb{R}^2)$ 之间的 cross-correlations 为：\n$$(k\\star_{\\mathbb{R}^2} f)(\\mathbf{x})=\\int_{\\mathbb{R}^2} k(\\mathbf{x}^{\\prime}-\\mathbf{x})f(\\mathbf{x}^{\\prime})\\mathrm{ d} \\mathbf{x}^{\\prime}=(\\mathscr{L}_{\\mathbf{x}} k, f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}$$\nRHS 的标记是定义出来的。\nNote: 这其实就是 CNN 中的 convolution，但并不是严格意义上的 convolution，严格意义上的 convolution 为： $$\\int_{\\mathbb{R}^2} k(\\mathbf{x}-\\mathbf{x}^{\\prime})f(\\mathbf{x}^{\\prime})\\mathrm{ d} \\mathbf{x}^{\\prime}$$\nEquivariance Convolutions/cross-correlations 具有平移等变性，即对于 $\\forall \\mathbf{x}, \\mathbf{y}\\in \\mathbb{R}^2$，有\n$$\\mathscr{L}_{\\mathbf{y}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2(\\mathbb{R}^2)}[(k\\star_{\\mathbb{R}^2}f)(\\mathbf{x})]=(k\\star_{\\mathbb{R}^2}\\mathscr{L}_{\\mathbf{y}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2(\\mathbb{R}^2)}f)(\\mathbf{x})$$\nProof: $$LHS=(k\\star_{\\mathbb{R}^2} f)(\\mathbf{x}-\\mathbf{y})=(\\mathscr{L}_{\\mathbf{x}-\\mathbf{y}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} k, f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}=(\\mathscr{L}_{\\mathbf{x}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} k, \\mathscr{L}_{\\mathbf{y}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}=RHS$$\n一般情况下，卷积操作对于旋转操作并不具有等变性。\n$$\\mathscr{L}_{\\theta}^{SO(2)\\rightarrow \\mathbb{L}_2(\\mathbb{R}^2)}[(k\\star_{\\mathbb{R}^2}f)(\\mathbf{x})]\\neq (k\\star_{\\mathbb{R}^2}\\mathscr{L}_{\\theta}^{SO(2)\\rightarrow \\mathbb{L}_2(\\mathbb{R}^2)}f)(\\mathbf{x})$$\nRegular group CNN 接下来，我们从构造 roto-translation equivariant 的卷积操作入手，一步步搭建 regular group CNN。\nLifting correlations 首先，cross-correlation 可以写为：\n$$(k\\star_{\\mathbb{R}^2} f)(\\mathbf{x})=(\\mathscr{L}_{\\mathbf{x}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} k, f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}$$\n类似地，对于 $\\forall k, f\\in \\mathbb{L}_2 (\\mathbb{R}^2)$，我们定义 lifting correlations 为：\n$$(k\\tilde{\\star} f)(\\mathbf{x}, \\theta)=(\\mathscr{L}_{(\\mathbf{x},\\theta)}^{SE(2)\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} k, f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}=(\\mathscr{L}_{\\mathbf{x}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} \\mathscr{L}_{\\theta}^{SO(2)\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)} k, f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}$$\n这就相当于将 kernel $k$ 的每一种旋转以及每一种平移都与 $f$ 做一次点积操作，最后得到一个 3D feature map。\nLifing correlations 具有 roto-translation 等变性，即（省略上标）：\n$$\\mathscr{L}_{\\mathbf{y}} \\mathscr{L}_{\\varphi} (k\\tilde{\\star} f)(\\mathbf{x}, \\theta) = (k\\tilde{\\star} \\mathscr{L}_{\\mathbf{y}} \\mathscr{L}_{\\varphi} f)(\\mathbf{x}, \\theta)$$\nProof: $$LHS=(k\\tilde{\\star} f)(\\mathbf{y},\\varphi)^{-1} (\\mathbf{x},\\theta)=(\\mathscr{L}_{\\mathbf{y}} \\mathscr{L}_{\\varphi} \\mathscr{L}_{\\mathbf{x}} \\mathscr{L}_{\\theta} k, f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}=(\\mathscr{L}_{\\mathbf{x}} \\mathscr{L}_{\\theta} k, \\mathscr{L}_{\\mathbf{y}} \\mathscr{L}_{\\varphi} f)_{\\mathbb{L}_2 (\\mathbb{R}^2)}=RHS$$\nGroup correlations Lifting correlations 可以学到 low-level features，比如要匹配人脸，某个 kernel 可能学到了眼睛，某个 kernel 可能学到了鼻子。如果此时直接对 $\\theta$ 轴进行投影，各部分不同的朝向可能会导致相同的投影，如下图所示。\n所以需要后续的 layers 用其他 kernels 来匹配全局信息。对于 $\\forall k, y \\in \\mathbb{L}_2 (SE(2))$，定义 group correlations 为：\n$$(k\\star f)(\\mathbf{x}, \\theta)=(\\mathscr{L}_{(\\mathbf{x},\\theta)}^{SE(2)\\rightarrow \\mathbb{L}_2 (SE(2))} k, f)_{\\mathbb{L}_2 (SE(2))}=(\\mathscr{L}_{\\mathbf{x}}^{\\mathbb{R}^2\\rightarrow \\mathbb{L}_2 (SE(2))} \\mathscr{L}_{\\theta}^{SO(2)\\rightarrow \\mathbb{L}_2 (SE(2))} k, f)_{\\mathbb{L}_2 (SE(2))}$$\nGroup correlations 将两个 3D feature maps 映射为一个 3D feature map。\n类似可以证明，group correlations 也是 roto-translation equivalent 的。\n网络架构 将上面说的 lifting correlations 和 group correlations 连起来，就得到了一个 regular group CNN 最简单的样子。可以在后面再加上 projection layer 等层，将 $\\theta$ 轴去掉，得到一个二维图像。我们构造了一个对 roto-translation 具有等变性的 CNN，当然也可以以类似的方法构造对 scale-translation 具有等变性的 CNN，如下图所示。\n当然，我们目前只得到了一个二维图像，下一节会以细胞识别为例子，构造一个完整的 regular group CNN。\n","date":"2024-01-20T23:00:00Z","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.3/","title":"Group Equivariant Deep Learning Lecture 1.3"},{"content":"Lecture 1.2 Group Theory | The basics 先介绍一下群论里的一些概念。\nGroup 一个群 (group) $(G, \\cdot)$ 是一个元素集合 $G$ 加上定义在 $G$ 中元素上的二元运算 $\\cdot$。二元运算 $\\cdot$ 满足以下四条性质：\nClosure: 对于 $\\forall g, h \\in G$, 它们的积 $g\\cdot h\\in G$。 Associativity: 对于 $\\forall g,h,i \\in G, \\cdot$ 满足结合律，$g\\cdot (h\\cdot i)=(g\\cdot h)\\cdot i$。 Indentity element: 存在一个 identity element $e\\in G$，满足对于 $\\forall g\\in G, e\\cdot g=g\\cdot e=g$。 Inverse element: 对于 $\\forall g\\in G$，其都有 inverse element $g^{-1}\\in G$ s.t. $g^{-1}\\cdot g=g \\cdot g^{-1}=e$。 Translation group $(\\mathbb{R}^2, +)$ Translation group 中的元素即为 $\\mathbb{R}^2$ 中的点，对于 $g=(\\mathbf{x}), g^{\\prime}=(\\mathbf{x}^{\\prime})$，其中 $\\mathbf{x},\\mathbf{x}^{\\prime}\\in \\mathbb{R}^2$，我们有：\n$$g\\cdot g^{\\prime}=(\\mathbf{x}+\\mathbf{x}^{\\prime})$$ $$g^{-1}=(-\\mathbf{x})$$\nTranslation group 中的一个元素可以被视为一个平移变换 （Lecture 1.6 中会细讲）。\nRoto-translation group $SE(2)$ (2D Special Euclidean motion group) $SE(2)=\\mathbb{R}^2\\rtimes SO(2)$，其中的每个元素 $g=(\\mathbf{x}, \\mathbf{R}_{\\theta} )$ 由一个二维向量 $\\mathbf{x}\\in \\mathbb{R}^2$ 以及一个二维旋转矩阵 $\\mathbf{R}_{\\theta} \\in SO(2)$ 组成。对于 $g=(\\mathbf{x}, \\mathbf{R}_{\\theta}) , g^{\\prime}=(\\mathbf{x}^{\\prime}, \\mathbf{R}_{\\theta^{\\prime}} )$，我们有：\n$$g\\cdot g^{\\prime}=(\\mathbf{x}, \\mathbf{R}_{\\theta}) \\cdot (\\mathbf{x}^{\\prime}, \\mathbf{R}_{\\theta^{\\prime}})=(\\mathbf{R}_{\\theta}\\mathbf{x}^{\\prime}+\\mathbf{x}, \\mathbf{R}_{\\theta+\\theta^{\\prime}} )$$\n$$g^{-1}=(-\\mathbf{R}_{\\theta}^{-1} \\mathbf{x}, \\mathbf{R}_{\\theta}^{-1} )$$\n$SE(2)$ 中的一个元素可以被视为一个旋转变换加上一个平移变换。\n矩阵表示： $SE(2)$ 中的元素也可以用矩阵表示： $$g=(\\mathbf{x}, \\mathbf{R}_{\\theta})\\quad \\leftrightarrow\\quad \\mathbf{G}=\\begin{pmatrix} \\mathbf{R}_{\\theta} \u0026amp; \\mathbf{x}\\\\ \\mathbf{0}^{\\top} \u0026amp; 1 \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta \u0026amp; -\\sin\\theta \u0026amp; x\\\\ \\sin\\theta \u0026amp; \\cos\\theta \u0026amp; y\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}$$\n$$\\begin{pmatrix} \\mathbf{R}_{\\theta} \u0026amp; \\mathbf{x}\\\\ \\mathbf{0}^{\\top} \u0026amp; 1 \\end{pmatrix}\\begin{pmatrix} \\mathbf{R}_{\\theta^{\\prime}} \u0026amp; \\mathbf{x}^{\\prime}\\\\ \\mathbf{0}^{\\top} \u0026amp; 1 \\end{pmatrix}=\\begin{pmatrix} \\mathbf{R}_{\\theta+\\theta^{\\prime}} \u0026amp; \\mathbf{R}_{\\theta} \\mathbf{x}^{\\prime}+\\mathbf{x}\\\\ \\mathbf{0}^{\\top} \u0026amp; 1 \\end{pmatrix}$$\nScale-translation group $\\mathbb{R}^2\\rtimes\\mathbb{R}^+$ $\\mathbb{R}^2\\rtimes\\mathbb{R}^+$ 中的每个元素 $g=(\\mathbf{x},s)$ 由一个二维向量 $\\mathbf{x}\\in\\mathbb{R}^2$ 以及一个正标量 $s\\in \\mathbb{R}^+$ 组成。对于 $g=(\\mathbf{x},s), g^{\\prime}=(\\mathbf{x}^{\\prime}, s^{\\prime})$，我们有：\n$$g\\cdot g^{\\prime}=(s\\mathbf{x}^{\\prime}+\\mathbf{x}, ss^{\\prime})$$\n$$g^{-1}=\\left(-\\frac{1}{s}\\mathbf{x}, \\frac{1}{s}\\right)$$\n$\\mathbb{R}^2\\rtimes\\mathbb{R}^+$ 中的一个元素可以被视为一个缩放变换加上一个平移变换。\nAffine groups $G=\\mathbb{R}^{d}\\rtimes H$ Affine groups 是某个 group $H$ 和 $\\mathbb{R}^d$ 的半直积。对于 $g=(\\mathbf{x},h), g^{\\prime}=(\\mathbf{x}^{\\prime}, h^{\\prime})$，我们有：\n$$g\\cdot g^{\\prime}=(h\\cdot\\mathbf{x}^{\\prime}+\\mathbf{x}, h\\cdot h^{\\prime})$$ $$g^{-1}=\\left(-h^{-1}\\cdot\\mathbf{x}, h^{-1}\\right)$$\n上面说的 $SE(2)=\\mathbb{R}^2\\rtimes SO(2)$ 和 $\\mathbb{R}^2\\rtimes\\mathbb{R}^+$ 都属于 affine groups。Affine groups 中的一个元素可以被视为一个变换 $h$ 加上一个平移变换。\nRepresentations 一个 representation $\\rho: G\\rightarrow GL(V)$ 是从 $G$ 到 $GL(V)$ 的一个群同态 (group homomorphism)。\n$GL_n$ 就是由所有 $n\\times n$ 的可逆矩阵组成的群，群运算即为矩阵乘法。 这就是说 $\\rho(g)$ 是由 $g\\in G$ 决定的线性变换，它作用在一个向量 $\\mathbf{v}\\in V$ 上，具有性质：\n$$\\rho(g^{\\prime})\\circ\\rho(g)[\\mathbf{v}]=\\rho(g^{\\prime}\\cdot g)[\\mathbf{v}]$$\nLeft-regular representations 一个 left-regular representation $\\mathscr{L}_g$ 作用在一个函数 $f$ 上，定义为：\n$$\\mathscr{L}_g [f](x):=f(g^{-1}\\cdot x)$$\n这么说有点抽象，举个例子。$f\\in \\mathbb{L}_2(\\mathbb{R}^2)$ 是一个二维图像，它给 $\\mathbb{R}^2$ 上的每个点赋值。$g\\in G=SE(2)$ 可以被看做是一个平移变换加上一个旋转变换。对于二维平面上的一个点 $\\mathbf{y}\\in\\mathbb{R}^2$，可以算出：\n$$\\mathscr{L}_g [f](\\mathbf{y}):=f(g^{-1}\\cdot \\mathbf{y})=f(\\mathbf{R}_{-\\theta}(\\mathbf{y}-\\mathbf{x}))$$\n即现在 $\\mathbf{y}$ 处的函数值为先将 $\\mathbf{y}$ 平移 $-\\mathbf{x}$，再旋转 $-\\theta$ 处的 $f$ 值。也就是说，现在的图像是由原先的图像先旋转 $\\theta$ 再平移 $\\mathbf{x}$ 得到的。在这种情况下 $\\mathscr{L}_g $ 也可以写做 $\\mathscr{L}_g^{SE(2)\\rightarrow \\mathbb{L}_2 (\\mathbb{R}^2)}$。这就是说，在给定 $f$ 的情况下，$\\mathscr{L}$ 会将 $g\\in SE(2)$ 转成一个二维图像 $f^{\\prime}\\in \\mathbb{L}_2 (\\mathbb{R}^2)$。\nLeft-regular representations 满足性质：\n$$\\mathscr{L}_{g^{\\prime}}\\circ\\mathscr{L}_g=\\mathscr{L}_{g^{\\prime}\\cdot g}$$\nEquivariance 如果一个 operator $\\Phi: X\\rightarrow Y$ （如一个神经网络）满足以下条件，则说其具有等变性 (equivariance)：\n$$\\Phi\\circ \\rho^X(g)=\\rho^Y(g)\\circ \\Phi$$\n即，先作用 $\\Phi$ 与后作用 $\\Phi$ 得到的效果是一样的。比如 $g$ 表示一个平移变换，$\\Phi$ 表示用 CNN 从图片中提取特征。\n","date":"2024-01-20T22:00:00Z","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.2/","title":"Group Equivariant Deep Learning Lecture 1.2"},{"content":"最近在做一个将对称性与 RL 结合的工作，其中将 PPO 里的 actor-critic 网络换成了具有对称性的 EMLP (equivariant MLP)。EMLP 的效果就是，对于具有对称性的输入，其保证输出也具有对称性。EMLP 利用 escnn 库实现。我在 escnn 库的 README.md 里找到了 Amsterdam 大学的一个暑期课程 An Introduction to Group Equivariant Deep Learning，感觉是个很有趣的领域，于是打算学习一下并记点笔记。\nLecture 1 Regular group convolutional neural networks Lecture 1.1 Introduction 其实 DL 中的许多问题都要求对于以某种方式对称后的输入，网络的输出也具有某种对称性（或不变性）。比如对于肿瘤细胞的识别：给定一张细胞的图片，要求判断其是否为恶性肿瘤细胞。我们希望图像旋转后，判断的结果保持不变。\n一个最直接的办法就是 data augmentation，对于训练集中的一张图片，将其经过若干种旋转后的图片都加到训练集中。尽管这样可以在一定程度上解决问题，但是这种方法仍没有完全保证输出关于对称输入的不变性 (invariance)，而且其将有限的网络 capacity 用于学习对称性上，在相同的参数量下可能会造成 capacity 的下降。因此我们希望直接在网络层面保证对称性。\n我们已经知道 CNN 具有平移对称性，即对于平移一定距离的输入，CNN 的输出也具有相同方向、相同距离的平移。但如下图所示， CNN 并不具备旋转对称性。其中 input 即为输入的图像，feature map 是 CNN 的原始输出，stabilized view 是将 feature map 转回到原来的角度。对于旋转后的图像，CNN 的输出并不稳定，即 stabilized view 并不保持不变。\n我们希望构造一个 CNN 网络结构 (Group equivariant CNN)，使得 CNN 具有旋转对称性，如下图所示。\n","date":"2024-01-20T20:00:00Z","permalink":"https://suz-tsinghua.github.io/p/gedl-notes-1.1/","title":"Group Equivariant Deep Learning Lecture 1.1"},{"content":"2023秋 NLP 课程项目 —— 歌曲问答及推荐系统 介绍 受到训练数据的限制，现在的 LLM（如ChatGPT）并没有能力很好地执行对歌曲信息的问答以及歌曲推荐任务，尤其是对于较新的歌曲和中文歌曲。比如“鸡你太美是什么歌曲？”“半岛铁盒是谁的歌？”“给我推荐一首吉他演奏的轻音乐。”\n我们希望通过利用搜索引擎、构建本地知识库等方式来解决这个问题。\n项目目前开源在 https://github.com/hs-black/Music-Recommander （不要在意拼写错误的细节）。\n技术实现 工具 我们在网上找到了这些工具：\nBing API: Bing API 支持用 python 脚本调用 Bing 的搜索功能，可以利用这一 API 搜索歌曲的信息。 NeteaseCloudMusicApi: 这是一个开源的库，可以调用网易云音乐的很多功能，如获取歌曲的百科信息（曲风、推荐标签、语言、BPM 等）。 Langchain-Chatchat: 这个库利用 ChatGLM 等大语言模型与 Langchain 实现了很多功能，如本地知识库问答、搜索引擎问答等。我们主要利用其对本地知识库的检索功能，此部分利用 Embedding Model 进行本地文件的向量化与搜索匹配。 OpenAI API: 利用 OpenAI 的 API 接口在 Python 中调用 ChatGPT 等大语言模型。 Pipeline 整个项目的 pipeline 如上图所示。对于用户的输入，先用 LLM 检查是否需要调用网易云搜索、Bing 搜索、本地知识库搜索工具。因为其实用户的有些输入与歌曲问答或推荐并没有关系，可以直接用 LLM 进行回答。\n若 LLM 认为需要调用工具，它会从用户的输入中提取出调用工具所需要的参数，如歌名、歌手、曲风、语言等信息，并调用网易云搜索、Bing 搜索、本地知识库搜索工具。LLM 会再将三个工具返回的信息进行整合提取，根据用户的输入返回其需要的信息。\n我们也实现了 mp3 的搜索功能，对于 LLM 在最后一步中返回的歌曲，利用网易云接口找到其 mp3 音频，并可直接在项目前端播放。\n前端 项目前端采用 streamlit 实现，用户可以在前端直接与我们的项目进行交流，直接听到项目返回的歌曲。\n本地知识库 我们利用网易云的接口获取了大约500k首歌曲的信息，包括歌名、歌手、曲风、推荐标签、语种、BPM、乐评，整合进几个 csv 文件中。\n接着利用 Langchain-Chatchat 的实现，用 bge-large-zh Embedding Model 对 csv 文件进行向量化，存储在本地向量库里。对于用户的一个 query，将 query 同样向量化以后在本地向量库中寻找相似信息进行整合返回。具体实现可以参考 Langchain-Chatchat 的文档。\nEvaluation 本项目其实可以被视为两部分：歌曲推荐系统以及歌曲问答系统，因此 evaluation 也从这两部分分别下手。\n歌曲推荐系统的 evaluation 我们用 ChatGPT 生成了60个歌曲推荐问题，每个问题指定了歌曲的歌名、歌手、曲风等信息中的一个或几个。为了展示我们项目在中文歌曲上的优越性，我们还多设置了20个与中文歌曲有关的歌曲推荐问题。对于每个模型对每个问题的回答，为了保证评估的科学性与严谨性，我们将其顺序打乱并隐去了对应模型的名字，然后对其进行0到9分的评分。最后统计出每个模型回答的平均得分与其回答为最佳回答的次数。\n各模型的含义：\nOur_Recommender_full: 这是我们的系统的完整版。 GPT: 调用 gpt-3.5-turbo-1106 的 API 对用户的生成直接返回。 Our_recommender_no_kb: kb 即 knowledge base，本地知识库。此模型为我们的系统但是去掉本地知识库，仅调用网易云搜索与 Bing 搜索。测试这个模型是为了探究本地知识库对系统生成效果的影响。 ChatGLM_with_kb: 这部分利用了 ChatGLM 大模型以及本地知识库。其实是 Langchain-Chatchat 的知识库问答的实现，可以作为我们系统的离线版本，其不需要进行联网。 DeepAI Song Recommender: 一个歌曲推荐系统，链接为 https://deepai.org/chat/songs。 从表中可见，在 GPT 生成的问题中，除去 ChatGLM_with_kb 以外的所有模型的表现相差并不多，我们的模型仅有略微的领先。这可能是因为这些模型大多基于 GPT ，在回答 GPT 生成的问题方面可能具有天然的优势。而且这部分的许多问题都偏向西方的音乐，我们的模型由于采用了网易云音乐以及 Bing 中文搜索，在西方音乐方面并没有很大的优势。\n而对于中文歌曲，我们的系统就有明显的优势。\n歌曲问答系统的 evaluation 我们利用网易云音乐的歌曲榜单构建了一个包含 100 首歌曲的 test set，对于每首歌曲，给定其歌名，向模型询问其歌手信息，并让模型从几个待选标签中选择一个最可能的标签，计算其正确率。\n我们的模型在两个任务上都有明显的优势。\n在歌手信息方面，GPT表现得非常差，这是因为 test set 包含的歌曲多为较新的中文歌曲，其并没有出现在 GPT 的训练数据中。ChatGLM_with_kb 也表现得较差，这是因为 kb 中包含有一首歌曲的很多翻唱版本，LLM 无法判断哪个版本较为重要，因此很难返回正确的结果。但是比较 Our_Recommender_full 和 Our_recommender_no_kb 可以发现将 kb 整合进 Our_recommender_no_kb 中可以提升系统的表现，这是因为网易云搜索与 Bing 搜索已经返回了一些歌曲，kb 返回的翻唱版本并不能对最后的结果有很大的影响，反而，kb 还能补充一些网易云搜索和 Bing 搜索不好搜索到的歌曲信息。\n而在 tags 方面，GPT表现得并不像歌手信息方面那么差，这是因为正确的 tags 本身就占待选 tags 的一部分，就算是 random guess 也能有不差的分数，而且 GPT 还可以通过歌名来对 tags 有一个估计。\n写在最后 我们本来想尝试一下用我们的数据对 GPT 进行 finetune，但无奈经费不足，便放弃了。\n感谢我的队友 zzz 和 yht。\n","date":"2024-01-18T00:00:00Z","image":"https://suz-tsinghua.github.io/p/2023%E7%A7%8B-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/demo_hu4022502737283411239.png","permalink":"https://suz-tsinghua.github.io/p/2023%E7%A7%8B-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/","title":"2023秋 自然语言处理 课程项目"},{"content":"建站了，打算以后在这里写点课程笔记和项目介绍。\n放张02做封面吧，她真好看。\n","date":"2024-01-07T00:00:00Z","image":"https://suz-tsinghua.github.io/p/zeroth-post/02_hu15400669692437418024.jpg","permalink":"https://suz-tsinghua.github.io/p/zeroth-post/","title":"第0个post"}]