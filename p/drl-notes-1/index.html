<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="MDP, Value Iteration and Policy Iteration"><title>Deep Reinforcement Learning Lecture 1</title>
<link rel=canonical href=https://suz-tsinghua.github.io/p/drl-notes-1/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Deep Reinforcement Learning Lecture 1"><meta property='og:description' content="MDP, Value Iteration and Policy Iteration"><meta property='og:url' content='https://suz-tsinghua.github.io/p/drl-notes-1/'><meta property='og:site_name' content='suz'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='DRL'><meta property='article:published_time' content='2024-07-15T17:00:00+00:00'><meta property='article:modified_time' content='2024-07-15T17:00:00+00:00'><meta name=twitter:title content="Deep Reinforcement Learning Lecture 1"><meta name=twitter:description content="MDP, Value Iteration and Policy Iteration"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu1189040654346373991.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😇</span></figure><div class=site-meta><h1 class=site-name><a href=/>suz</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/SUZ-tsinghua target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=R5Y1xlUAAAAJ&amp;hl=zh-CN&amp;oi=sra' target=_blank title="Google scholar" rel=me><svg class="icon icon-tabler icon-tabler-brand-google" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M20.945 11a9 9 0 11-3.284-5.997l-2.655 2.392A5.5 5.5.0 1017.125 14H13v-3h7.945z"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#mdp-basics>MDP Basics</a><ol><li><a href=#definition-of-mdp>Definition of MDP</a></li><li><a href=#an-example>An Example</a></li><li><a href=#policy>Policy</a></li><li><a href=#utility>Utility</a></li><li><a href=#optimal-quantities>Optimal Quantities</a></li></ol></li><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#policy-iteration>Policy Iteration</a><ol><li><a href=#policy-evaluation>Policy Evaluation</a></li><li><a href=#policy-improvement>Policy Improvement</a></li><li><a href=#policy-iteration-1>Policy Iteration</a></li></ol></li><li><a href=#mdp-to-reinforcement-learning>MDP to Reinforcement Learning</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/>课程笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/drl-notes-1/>Deep Reinforcement Learning Lecture 1</a></h2><h3 class=article-subtitle>MDP, Value Iteration and Policy Iteration</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 15, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 5 分钟</time></div></footer></div></header><section class=article-content><p>2024春季学期选了xhz老师的限选课<strong>深度强化学习</strong>，恰巧也在研究这方面的内容，于是打算写个笔记。</p><h1 id=mdp-value-iteration-and-policy-iteration>MDP, Value Iteration and Policy Iteration</h1><h2 id=mdp-basics>MDP Basics</h2><h3 id=definition-of-mdp>Definition of MDP</h3><p>Markov Decision Process (MDP) 是一个决策过程数学模型，直观地来说：一个智能体 (agent) 处在一个环境中，环境处于不同的状态 (state)；每一步，agent 可以得知环境的部分或全部 state 信息，这部分信息称为 agent 的观测 (observation)；通过 observation，agent 每一步会作出决策，给出一个动作 (action)；这个动作会影响环境，环境有概率转移到另一个 state；同时，环境根据潜在的奖励函数 (rewards) 来给 agent 提供奖励。Formally:</p><blockquote><p><strong>Definition:</strong></p><p>An MDP is a 4-tuple $(S, A, T, R)$:</p><ul><li>$S$ is a set of states called the <em>state space</em>.</li><li>$A$ is a set of actions called the <em>action space</em>.</li><li>$T(s, a, s^{\prime})=Pr(s_{t+1}=s^{\prime}|s_t=s, a_t=a)$ is the probability that action $a$ at $s$ leads to $s^{\prime}$, called the <em>transition fuction</em> (also <em>model</em> or <em>dynamics</em>).</li><li>$R(s, a, s^{\prime})$ is the immediate <em>reward</em> received after transitioning from state $s$ to state $s^{\prime}$, due to action $a$.</li></ul></blockquote><p>当然这只是最简单的定义，还可以根据情况的不同引入额外的东西。比如系统初始状态的分布函数 (<em>initial state distribution</em>)，一个一旦到达就直接停止的结束状态 (<em>terminal state</em>)。再比如某些情况下，环境可能是 partially observable 的，agent 无法观测到环境的整个 state (比如打扑克的时候，你无法看到对方手上的牌，这是 partially observable 的，但是下围棋的时候，你可以看到棋盘上所有的东西，这是 fully observable 的)，此时需要在定义中引入 a set of observations $O$，此时的 MDP 称为 Partially Observable MDP (POMDP)。</p><p>定义中 Markov 的意思是：给定当前状态之后，未来与过去就无关了，即 $Pr(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, &mldr;, s_0)=Pr(s_{t+1}|s_t, a_t)$。可以认为过去的信息都被浓缩到当前的 state 中了。</p><h3 id=an-example>An Example</h3><p>用一个简单的例子来加深理解:</p><p><img src=/p/drl-notes-1/grid_world.png width=486 height=439 srcset="/p/drl-notes-1/grid_world_hu4615231692853378775.png 480w, /p/drl-notes-1/grid_world_hu13349423474860228102.png 1024w" loading=lazy alt="An example of MDP" class=gallery-image data-flex-grow=110 data-flex-basis=265px></p><p>比如一个 agent 位于这样一个 grid world 中</p><ul><li>每一时刻的 state 就是 agent 的位置。</li><li>action 是上下左右。</li><li>$T$ 我们定义为，有 80% 的可能，agent 的 transition 与其 action 一致；有 10% 的可能，无论什么 action，agent 都往左；有 10% 的可能，无论什么 action，agent 都往右。</li><li>只有在 agent 吃到钻石的时候才会有 reward。</li></ul><h3 id=policy>Policy</h3><p>以上我们主要关注环境，接下来我们看 agent，我们将 agent 从 state/observation 得到 action 的决策称为 policy:</p><blockquote><p><strong>Definition:</strong></p><p>Policy $\pi$ 是一个条件概率密度函数 $\pi(a|s)$，表示 agent 在 state $s$ 时采取 action $a$ 的概率。</p></blockquote><h3 id=utility>Utility</h3><p>RL 的目标是学出一个好的 policy，那么这个 “好的” 该如何进行评价。直观来看，我们可以将 agent 放在某个 initial state，让其根据自己的 policy 进行运动固定的步数，或是等到最后结束。那么问题就到了，agent 跑出的这个序列 $(s_0, a_0, r_0, s_1, a_1, r_1, &mldr;, s_t, a_t, r_t)$ (称为 trajectory) 该如何评价。我们引入 Reward Hypothesis, 即MDP中所有的目标都应被 reward 定义，而不牵扯到其他的量。那么可以将 trajectory 中的 rewards 单独抽出来 $(r_0, r_1, r_2, &mldr;, r_t)$。我们希望有一个函数 (Utility) 能够将这个 rewards 序列映射成一个标量，这样有助于比较不同 trajectories 的优劣。</p><p>一种方法是直接加起来，即 additive utilities: $U([r_0, r_1, r_2, &mldr;]) = r_0+r_1+r_2 + &mldr;$</p><p>考虑到现实生活中，当下的 reward 往往比之后的 reward 更具有价值 (当下给你一块钱往往优于两天后给你一块钱)，一个更常用的 utility 是 discounted utilities: $U([r_0, r_1, r_2, &mldr;]) = r_0+\gamma r_1+\gamma^2 r_2 + &mldr;$。其中 $\gamma$ 称为 discounted factor。</p><h3 id=optimal-quantities>Optimal Quantities</h3><p>直观上定义 optimal quantities:</p><ul><li>Optimal policy: $\pi^*(s)$ = optimal action from state $s$.</li><li>Optimal value/utility of a state $s$: $V^*(s)$ = expected utility starting from $s$ and acting optimally.</li><li>Optimal Q value: $Q^*(s,a)$ = expected utility taking action $a$ from state $s$ and acting optimally.</li></ul><p>Formally 可以递归地定义这些量：</p><p>$$\pi^{*}(s)=\argmax_a Q^{*}(s,a)$$
$$V^{*}(s)=\max_a Q^{*}(s,a)$$
$$Q^{*}(s,a)=\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]$$</p><p>从以上两个式子我们可以消去 $Q^{*}$ 得到 $V^{*}$ 满足的等式：</p><p>$$V^{*}(s)=\max_a \sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]$$</p><p>称为 Bellman Equation。</p><h2 id=value-iteration>Value Iteration</h2><p>RL 的最终目标是得到 $\pi^{*}$，我们可以利用 $V^{*}$ 来得到 $\pi^{*}$。一种可行的用来得到 $V^{*}$ 的方法称为 Value Iteration，其利用了 Bellman Equation。</p><p>假设 MDP 在 $k$ 步后结束，定义 $s$ 的 optimal value 为 $V^{*}_k(s)$，那么有：</p><p>$$V_0^{*}(s)=0$$
$$V_{k+1}^{*}(s)=\max_a \sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V_k^{*}(s^{\prime})]$$</p><p>迭代计算直至收敛，即可得到 $V_{\infty}^{*}=V^{*}$。</p><p>VI 有两个问题：</p><ul><li>VI 每一步的时间复杂度为 $O(S^2A)$，因此仅仅适用于 discrete case，并且要求 $S$ 和 $A$ 均比较小，无法适用于连续空间。</li><li>Policy 往往会比 Value 收敛得更早，如果能够提前发现 policy 已经收敛会更好。</li></ul><h2 id=policy-iteration>Policy Iteration</h2><h3 id=policy-evaluation>Policy Evaluation</h3><p>上面介绍的 $V^{*}$ 是 optimal policy 的 value function，我们也可以给定一个 policy $\pi$，计算其对应的 value function $V^{\pi}$，这个计算过程称为 Policy Evaluation。</p><p>$$V^{\pi}(s) = \text{expected total discounted rewards starting in } s \text{ and following } \pi$$</p><p>计算过程类似于 Value Iteration，也是从相应的 Bellman Equation 入手进行迭代计算：</p><p>$$V^{\pi}(s)=\sum_{s^{\prime}}T(s,\pi(s),s^{\prime})[R(s,\pi(s),s^{\prime})+\gamma V^{\pi}(s^{\prime})]$$</p><p>Policy Evaluation 一步花费时间 $O(S^2)$。</p><h3 id=policy-improvement>Policy Improvement</h3><p>假设我们知道一个 MDP 的 value function, 如何得到在这个 value function 下的 optimal policy。显然可以在某个状态 $s$ 遍历所有的 action $a$，看哪个 $a$ 的收益最大，即：</p><p>$$\pi(s)=\argmax_a \sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V(s^{\prime})]$$</p><p><img src=/p/drl-notes-1/policy_improvement.png width=388 height=292 srcset="/p/drl-notes-1/policy_improvement_hu14245069186618032335.png 480w, /p/drl-notes-1/policy_improvement_hu5568883509343015072.png 1024w" loading=lazy alt="Value function of a grid world" class=gallery-image data-flex-grow=132 data-flex-basis=318px></p><p>这个过程就是 policy improvement。</p><h3 id=policy-iteration-1>Policy Iteration</h3><p>结合 Policy Evaluation 和 Policy Improvement，我们可以用另一种方法（不同于 Value Iteration）来得到 $\pi^*$。</p><p>循环以下两步直至 policy 收敛：</p><ul><li>Step 1: Policy Evaluation。对当前的 policy 进行 policy evaluation。</li><li>Step 2: Policy Improvement。对 Step 1 中得到的 value function 进行 policy improvement，得到新的 policy。</li></ul><p>这就是 Policy Iteration。PI 也可以得到 optimal $\pi^*$，并且在一些情况下比 VI 收敛得更快。</p><h2 id=mdp-to-reinforcement-learning>MDP to Reinforcement Learning</h2><p>不管是 VI 还是 PI，都要求我们知道 MDP 的 $T(s,a,s^{\prime})$ 和 $R(s,a,s^{\prime})$。但是在现实中的大部分情况，我们并不能准确地知道 $T(s,a,s^{\prime})$ 和 $R(s,a,s^{\prime})$，尤其是 $T$，因此需要引入 RL。</p><p><img src=/p/drl-notes-1/RL.png width=774 height=328 srcset="/p/drl-notes-1/RL_hu2981708100594716212.png 480w, /p/drl-notes-1/RL_hu12781544691208971070.png 1024w" loading=lazy alt="Reinforcement Learning" class=gallery-image data-flex-grow=235 data-flex-basis=566px></p><p>RL 的主要思想是：</p><ul><li>环境会为 agent 的 action 提供 reward 进行反馈。</li><li>Agent 的所有 utility 都被 reward 定义。</li><li>Agent 的目标是 maximize expected rewards。</li><li>学习只能基于 agent 获取到的 observations, actions, rewards 等信息（不知道真实的 $T(s,a,s^{\prime})$ 和 $R(s,a,s^{\prime})$）。</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/drl/>DRL</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/drl-notes-6/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 6</h2></div></a></article><article><a href=/p/drl-notes-5/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 5</h2></div></a></article><article><a href=/p/drl-notes-4/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 4</h2></div></a></article><article><a href=/p/drl-notes-3/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 3</h2></div></a></article><article><a href=/p/drl-notes-2/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 2</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=SUZ-tsinghua/SUZ-tsinghua.github.io issue-term=title crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2024 suz</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>