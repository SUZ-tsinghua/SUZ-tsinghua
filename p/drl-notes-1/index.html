<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="MDP, Value Iteration and Policy Iteration"><title>Deep Reinforcement Learning Lecture 1</title>
<link rel=canonical href=https://suz-tsinghua.github.io/p/drl-notes-1/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Deep Reinforcement Learning Lecture 1"><meta property='og:description' content="MDP, Value Iteration and Policy Iteration"><meta property='og:url' content='https://suz-tsinghua.github.io/p/drl-notes-1/'><meta property='og:site_name' content='suz'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='DRL'><meta property='article:published_time' content='2024-07-15T17:00:00+00:00'><meta property='article:modified_time' content='2024-07-15T17:00:00+00:00'><meta name=twitter:title content="Deep Reinforcement Learning Lecture 1"><meta name=twitter:description content="MDP, Value Iteration and Policy Iteration"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu1189040654346373991.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ˜‡</span></figure><div class=site-meta><h1 class=site-name><a href=/>suz</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/SUZ-tsinghua target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=R5Y1xlUAAAAJ&amp;hl=zh-CN&amp;oi=sra' target=_blank title="Google scholar" rel=me><svg class="icon icon-tabler icon-tabler-brand-google" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M20.945 11a9 9 0 11-3.284-5.997l-2.655 2.392A5.5 5.5.0 1017.125 14H13v-3h7.945z"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#mdp-basics>MDP Basics</a><ol><li><a href=#definition-of-mdp>Definition of MDP</a></li><li><a href=#an-example>An Example</a></li><li><a href=#policy>Policy</a></li><li><a href=#utility>Utility</a></li><li><a href=#optimal-quantities>Optimal Quantities</a></li></ol></li><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#policy-iteration>Policy Iteration</a><ol><li><a href=#policy-evaluation>Policy Evaluation</a></li><li><a href=#policy-improvement>Policy Improvement</a></li><li><a href=#policy-iteration-1>Policy Iteration</a></li></ol></li><li><a href=#mdp-to-reinforcement-learning>MDP to Reinforcement Learning</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/>è¯¾ç¨‹ç¬”è®°</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/drl-notes-1/>Deep Reinforcement Learning Lecture 1</a></h2><h3 class=article-subtitle>MDP, Value Iteration and Policy Iteration</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 15, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 5 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><p>2024æ˜¥å­£å­¦æœŸé€‰äº†xhzè€å¸ˆçš„é™é€‰è¯¾<strong>æ·±åº¦å¼ºåŒ–å­¦ä¹ </strong>ï¼Œæ°å·§ä¹Ÿåœ¨ç ”ç©¶è¿™æ–¹é¢çš„å†…å®¹ï¼Œäºæ˜¯æ‰“ç®—å†™ä¸ªç¬”è®°ã€‚</p><h1 id=mdp-value-iteration-and-policy-iteration>MDP, Value Iteration and Policy Iteration</h1><h2 id=mdp-basics>MDP Basics</h2><h3 id=definition-of-mdp>Definition of MDP</h3><p>Markov Decision Process (MDP) æ˜¯ä¸€ä¸ªå†³ç­–è¿‡ç¨‹æ•°å­¦æ¨¡å‹ï¼Œç›´è§‚åœ°æ¥è¯´ï¼šä¸€ä¸ªæ™ºèƒ½ä½“ (agent) å¤„åœ¨ä¸€ä¸ªç¯å¢ƒä¸­ï¼Œç¯å¢ƒå¤„äºä¸åŒçš„çŠ¶æ€ (state)ï¼›æ¯ä¸€æ­¥ï¼Œagent å¯ä»¥å¾—çŸ¥ç¯å¢ƒçš„éƒ¨åˆ†æˆ–å…¨éƒ¨ state ä¿¡æ¯ï¼Œè¿™éƒ¨åˆ†ä¿¡æ¯ç§°ä¸º agent çš„è§‚æµ‹ (observation)ï¼›é€šè¿‡ observationï¼Œagent æ¯ä¸€æ­¥ä¼šä½œå‡ºå†³ç­–ï¼Œç»™å‡ºä¸€ä¸ªåŠ¨ä½œ (action)ï¼›è¿™ä¸ªåŠ¨ä½œä¼šå½±å“ç¯å¢ƒï¼Œç¯å¢ƒæœ‰æ¦‚ç‡è½¬ç§»åˆ°å¦ä¸€ä¸ª stateï¼›åŒæ—¶ï¼Œç¯å¢ƒæ ¹æ®æ½œåœ¨çš„å¥–åŠ±å‡½æ•° (rewards) æ¥ç»™ agent æä¾›å¥–åŠ±ã€‚Formally:</p><blockquote><p><strong>Definition:</strong></p><p>An MDP is a 4-tuple $(S, A, T, R)$:</p><ul><li>$S$ is a set of states called the <em>state space</em>.</li><li>$A$ is a set of actions called the <em>action space</em>.</li><li>$T(s, a, s^{\prime})=Pr(s_{t+1}=s^{\prime}|s_t=s, a_t=a)$ is the probability that action $a$ at $s$ leads to $s^{\prime}$, called the <em>transition fuction</em> (also <em>model</em> or <em>dynamics</em>).</li><li>$R(s, a, s^{\prime})$ is the immediate <em>reward</em> received after transitioning from state $s$ to state $s^{\prime}$, due to action $a$.</li></ul></blockquote><p>å½“ç„¶è¿™åªæ˜¯æœ€ç®€å•çš„å®šä¹‰ï¼Œè¿˜å¯ä»¥æ ¹æ®æƒ…å†µçš„ä¸åŒå¼•å…¥é¢å¤–çš„ä¸œè¥¿ã€‚æ¯”å¦‚ç³»ç»Ÿåˆå§‹çŠ¶æ€çš„åˆ†å¸ƒå‡½æ•° (<em>initial state distribution</em>)ï¼Œä¸€ä¸ªä¸€æ—¦åˆ°è¾¾å°±ç›´æ¥åœæ­¢çš„ç»“æŸçŠ¶æ€ (<em>terminal state</em>)ã€‚å†æ¯”å¦‚æŸäº›æƒ…å†µä¸‹ï¼Œç¯å¢ƒå¯èƒ½æ˜¯ partially observable çš„ï¼Œagent æ— æ³•è§‚æµ‹åˆ°ç¯å¢ƒçš„æ•´ä¸ª state (æ¯”å¦‚æ‰“æ‰‘å…‹çš„æ—¶å€™ï¼Œä½ æ— æ³•çœ‹åˆ°å¯¹æ–¹æ‰‹ä¸Šçš„ç‰Œï¼Œè¿™æ˜¯ partially observable çš„ï¼Œä½†æ˜¯ä¸‹å›´æ£‹çš„æ—¶å€™ï¼Œä½ å¯ä»¥çœ‹åˆ°æ£‹ç›˜ä¸Šæ‰€æœ‰çš„ä¸œè¥¿ï¼Œè¿™æ˜¯ fully observable çš„)ï¼Œæ­¤æ—¶éœ€è¦åœ¨å®šä¹‰ä¸­å¼•å…¥ a set of observations $O$ï¼Œæ­¤æ—¶çš„ MDP ç§°ä¸º Partially Observable MDP (POMDP)ã€‚</p><p>å®šä¹‰ä¸­ Markov çš„æ„æ€æ˜¯ï¼šç»™å®šå½“å‰çŠ¶æ€ä¹‹åï¼Œæœªæ¥ä¸è¿‡å»å°±æ— å…³äº†ï¼Œå³ $Pr(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, &mldr;, s_0)=Pr(s_{t+1}|s_t, a_t)$ã€‚å¯ä»¥è®¤ä¸ºè¿‡å»çš„ä¿¡æ¯éƒ½è¢«æµ“ç¼©åˆ°å½“å‰çš„ state ä¸­äº†ã€‚</p><h3 id=an-example>An Example</h3><p>ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥åŠ æ·±ç†è§£:</p><p><img src=/p/drl-notes-1/grid_world.png width=486 height=439 srcset="/p/drl-notes-1/grid_world_hu4615231692853378775.png 480w, /p/drl-notes-1/grid_world_hu13349423474860228102.png 1024w" loading=lazy alt="An example of MDP" class=gallery-image data-flex-grow=110 data-flex-basis=265px></p><p>æ¯”å¦‚ä¸€ä¸ª agent ä½äºè¿™æ ·ä¸€ä¸ª grid world ä¸­</p><ul><li>æ¯ä¸€æ—¶åˆ»çš„ state å°±æ˜¯ agent çš„ä½ç½®ã€‚</li><li>action æ˜¯ä¸Šä¸‹å·¦å³ã€‚</li><li>$T$ æˆ‘ä»¬å®šä¹‰ä¸ºï¼Œæœ‰ 80% çš„å¯èƒ½ï¼Œagent çš„ transition ä¸å…¶ action ä¸€è‡´ï¼›æœ‰ 10% çš„å¯èƒ½ï¼Œæ— è®ºä»€ä¹ˆ actionï¼Œagent éƒ½å¾€å·¦ï¼›æœ‰ 10% çš„å¯èƒ½ï¼Œæ— è®ºä»€ä¹ˆ actionï¼Œagent éƒ½å¾€å³ã€‚</li><li>åªæœ‰åœ¨ agent åƒåˆ°é’»çŸ³çš„æ—¶å€™æ‰ä¼šæœ‰ rewardã€‚</li></ul><h3 id=policy>Policy</h3><p>ä»¥ä¸Šæˆ‘ä»¬ä¸»è¦å…³æ³¨ç¯å¢ƒï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹ agentï¼Œæˆ‘ä»¬å°† agent ä» state/observation å¾—åˆ° action çš„å†³ç­–ç§°ä¸º policy:</p><blockquote><p><strong>Definition:</strong></p><p>Policy $\pi$ æ˜¯ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•° $\pi(a|s)$ï¼Œè¡¨ç¤º agent åœ¨ state $s$ æ—¶é‡‡å– action $a$ çš„æ¦‚ç‡ã€‚</p></blockquote><h3 id=utility>Utility</h3><p>RL çš„ç›®æ ‡æ˜¯å­¦å‡ºä¸€ä¸ªå¥½çš„ policyï¼Œé‚£ä¹ˆè¿™ä¸ª â€œå¥½çš„â€ è¯¥å¦‚ä½•è¿›è¡Œè¯„ä»·ã€‚ç›´è§‚æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°† agent æ”¾åœ¨æŸä¸ª initial stateï¼Œè®©å…¶æ ¹æ®è‡ªå·±çš„ policy è¿›è¡Œè¿åŠ¨å›ºå®šçš„æ­¥æ•°ï¼Œæˆ–æ˜¯ç­‰åˆ°æœ€åç»“æŸã€‚é‚£ä¹ˆé—®é¢˜å°±åˆ°äº†ï¼Œagent è·‘å‡ºçš„è¿™ä¸ªåºåˆ— $(s_0, a_0, r_0, s_1, a_1, r_1, &mldr;, s_t, a_t, r_t)$ (ç§°ä¸º trajectory) è¯¥å¦‚ä½•è¯„ä»·ã€‚æˆ‘ä»¬å¼•å…¥ Reward Hypothesis, å³MDPä¸­æ‰€æœ‰çš„ç›®æ ‡éƒ½åº”è¢« reward å®šä¹‰ï¼Œè€Œä¸ç‰µæ‰¯åˆ°å…¶ä»–çš„é‡ã€‚é‚£ä¹ˆå¯ä»¥å°† trajectory ä¸­çš„ rewards å•ç‹¬æŠ½å‡ºæ¥ $(r_0, r_1, r_2, &mldr;, r_t)$ã€‚æˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ä¸ªå‡½æ•° (Utility) èƒ½å¤Ÿå°†è¿™ä¸ª rewards åºåˆ—æ˜ å°„æˆä¸€ä¸ªæ ‡é‡ï¼Œè¿™æ ·æœ‰åŠ©äºæ¯”è¾ƒä¸åŒ trajectories çš„ä¼˜åŠ£ã€‚</p><p>ä¸€ç§æ–¹æ³•æ˜¯ç›´æ¥åŠ èµ·æ¥ï¼Œå³ additive utilities: $U([r_0, r_1, r_2, &mldr;]) = r_0+r_1+r_2 + &mldr;$</p><p>è€ƒè™‘åˆ°ç°å®ç”Ÿæ´»ä¸­ï¼Œå½“ä¸‹çš„ reward å¾€å¾€æ¯”ä¹‹åçš„ reward æ›´å…·æœ‰ä»·å€¼ (å½“ä¸‹ç»™ä½ ä¸€å—é’±å¾€å¾€ä¼˜äºä¸¤å¤©åç»™ä½ ä¸€å—é’±)ï¼Œä¸€ä¸ªæ›´å¸¸ç”¨çš„ utility æ˜¯ discounted utilities: $U([r_0, r_1, r_2, &mldr;]) = r_0+\gamma r_1+\gamma^2 r_2 + &mldr;$ã€‚å…¶ä¸­ $\gamma$ ç§°ä¸º discounted factorã€‚</p><h3 id=optimal-quantities>Optimal Quantities</h3><p>ç›´è§‚ä¸Šå®šä¹‰ optimal quantities:</p><ul><li>Optimal policy: $\pi^*(s)$ = optimal action from state $s$.</li><li>Optimal value/utility of a state $s$: $V^*(s)$ = expected utility starting from $s$ and acting optimally.</li><li>Optimal Q value: $Q^*(s,a)$ = expected utility taking action $a$ from state $s$ and acting optimally.</li></ul><p>Formally å¯ä»¥é€’å½’åœ°å®šä¹‰è¿™äº›é‡ï¼š</p><p>$$\pi^{*}(s)=\argmax_a Q^{*}(s,a)$$
$$V^{*}(s)=\max_a Q^{*}(s,a)$$
$$Q^{*}(s,a)=\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]$$</p><p>ä»ä»¥ä¸Šä¸¤ä¸ªå¼å­æˆ‘ä»¬å¯ä»¥æ¶ˆå» $Q^{*}$ å¾—åˆ° $V^{*}$ æ»¡è¶³çš„ç­‰å¼ï¼š</p><p>$$V^{*}(s)=\max_a \sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]$$</p><p>ç§°ä¸º Bellman Equationã€‚</p><h2 id=value-iteration>Value Iteration</h2><p>RL çš„æœ€ç»ˆç›®æ ‡æ˜¯å¾—åˆ° $\pi^{*}$ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ $V^{*}$ æ¥å¾—åˆ° $\pi^{*}$ã€‚ä¸€ç§å¯è¡Œçš„ç”¨æ¥å¾—åˆ° $V^{*}$ çš„æ–¹æ³•ç§°ä¸º Value Iterationï¼Œå…¶åˆ©ç”¨äº† Bellman Equationã€‚</p><p>å‡è®¾ MDP åœ¨ $k$ æ­¥åç»“æŸï¼Œå®šä¹‰ $s$ çš„ optimal value ä¸º $V^{*}_k(s)$ï¼Œé‚£ä¹ˆæœ‰ï¼š</p><p>$$V_0^{*}(s)=0$$
$$V_{k+1}^{*}(s)=\max_a \sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V_k^{*}(s^{\prime})]$$</p><p>è¿­ä»£è®¡ç®—ç›´è‡³æ”¶æ•›ï¼Œå³å¯å¾—åˆ° $V_{\infty}^{*}=V^{*}$ã€‚</p><p>VI æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š</p><ul><li>VI æ¯ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(S^2A)$ï¼Œå› æ­¤ä»…ä»…é€‚ç”¨äº discrete caseï¼Œå¹¶ä¸”è¦æ±‚ $S$ å’Œ $A$ å‡æ¯”è¾ƒå°ï¼Œæ— æ³•é€‚ç”¨äºè¿ç»­ç©ºé—´ã€‚</li><li>Policy å¾€å¾€ä¼šæ¯” Value æ”¶æ•›å¾—æ›´æ—©ï¼Œå¦‚æœèƒ½å¤Ÿæå‰å‘ç° policy å·²ç»æ”¶æ•›ä¼šæ›´å¥½ã€‚</li></ul><h2 id=policy-iteration>Policy Iteration</h2><h3 id=policy-evaluation>Policy Evaluation</h3><p>ä¸Šé¢ä»‹ç»çš„ $V^{*}$ æ˜¯ optimal policy çš„ value functionï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»™å®šä¸€ä¸ª policy $\pi$ï¼Œè®¡ç®—å…¶å¯¹åº”çš„ value function $V^{\pi}$ï¼Œè¿™ä¸ªè®¡ç®—è¿‡ç¨‹ç§°ä¸º Policy Evaluationã€‚</p><p>$$V^{\pi}(s) = \text{expected total discounted rewards starting in } s \text{ and following } \pi$$</p><p>è®¡ç®—è¿‡ç¨‹ç±»ä¼¼äº Value Iterationï¼Œä¹Ÿæ˜¯ä»ç›¸åº”çš„ Bellman Equation å…¥æ‰‹è¿›è¡Œè¿­ä»£è®¡ç®—ï¼š</p><p>$$V^{\pi}(s)=\sum_{s^{\prime}}T(s,\pi(s),s^{\prime})[R(s,\pi(s),s^{\prime})+\gamma V^{\pi}(s^{\prime})]$$</p><p>Policy Evaluation ä¸€æ­¥èŠ±è´¹æ—¶é—´ $O(S^2)$ã€‚</p><h3 id=policy-improvement>Policy Improvement</h3><p>å‡è®¾æˆ‘ä»¬çŸ¥é“ä¸€ä¸ª MDP çš„ value function, å¦‚ä½•å¾—åˆ°åœ¨è¿™ä¸ª value function ä¸‹çš„ optimal policyã€‚æ˜¾ç„¶å¯ä»¥åœ¨æŸä¸ªçŠ¶æ€ $s$ éå†æ‰€æœ‰çš„ action $a$ï¼Œçœ‹å“ªä¸ª $a$ çš„æ”¶ç›Šæœ€å¤§ï¼Œå³ï¼š</p><p>$$\pi(s)=\argmax_a \sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V(s^{\prime})]$$</p><p><img src=/p/drl-notes-1/policy_improvement.png width=388 height=292 srcset="/p/drl-notes-1/policy_improvement_hu14245069186618032335.png 480w, /p/drl-notes-1/policy_improvement_hu5568883509343015072.png 1024w" loading=lazy alt="Value function of a grid world" class=gallery-image data-flex-grow=132 data-flex-basis=318px></p><p>è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ policy improvementã€‚</p><h3 id=policy-iteration-1>Policy Iteration</h3><p>ç»“åˆ Policy Evaluation å’Œ Policy Improvementï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å¦ä¸€ç§æ–¹æ³•ï¼ˆä¸åŒäº Value Iterationï¼‰æ¥å¾—åˆ° $\pi^*$ã€‚</p><p>å¾ªç¯ä»¥ä¸‹ä¸¤æ­¥ç›´è‡³ policy æ”¶æ•›ï¼š</p><ul><li>Step 1: Policy Evaluationã€‚å¯¹å½“å‰çš„ policy è¿›è¡Œ policy evaluationã€‚</li><li>Step 2: Policy Improvementã€‚å¯¹ Step 1 ä¸­å¾—åˆ°çš„ value function è¿›è¡Œ policy improvementï¼Œå¾—åˆ°æ–°çš„ policyã€‚</li></ul><p>è¿™å°±æ˜¯ Policy Iterationã€‚PI ä¹Ÿå¯ä»¥å¾—åˆ° optimal $\pi^*$ï¼Œå¹¶ä¸”åœ¨ä¸€äº›æƒ…å†µä¸‹æ¯” VI æ”¶æ•›å¾—æ›´å¿«ã€‚</p><h2 id=mdp-to-reinforcement-learning>MDP to Reinforcement Learning</h2><p>ä¸ç®¡æ˜¯ VI è¿˜æ˜¯ PIï¼Œéƒ½è¦æ±‚æˆ‘ä»¬çŸ¥é“ MDP çš„ $T(s,a,s^{\prime})$ å’Œ $R(s,a,s^{\prime})$ã€‚ä½†æ˜¯åœ¨ç°å®ä¸­çš„å¤§éƒ¨åˆ†æƒ…å†µï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½å‡†ç¡®åœ°çŸ¥é“ $T(s,a,s^{\prime})$ å’Œ $R(s,a,s^{\prime})$ï¼Œå°¤å…¶æ˜¯ $T$ï¼Œå› æ­¤éœ€è¦å¼•å…¥ RLã€‚</p><p><img src=/p/drl-notes-1/RL.png width=774 height=328 srcset="/p/drl-notes-1/RL_hu2981708100594716212.png 480w, /p/drl-notes-1/RL_hu12781544691208971070.png 1024w" loading=lazy alt="Reinforcement Learning" class=gallery-image data-flex-grow=235 data-flex-basis=566px></p><p>RL çš„ä¸»è¦æ€æƒ³æ˜¯ï¼š</p><ul><li>ç¯å¢ƒä¼šä¸º agent çš„ action æä¾› reward è¿›è¡Œåé¦ˆã€‚</li><li>Agent çš„æ‰€æœ‰ utility éƒ½è¢« reward å®šä¹‰ã€‚</li><li>Agent çš„ç›®æ ‡æ˜¯ maximize expected rewardsã€‚</li><li>å­¦ä¹ åªèƒ½åŸºäº agent è·å–åˆ°çš„ observations, actions, rewards ç­‰ä¿¡æ¯ï¼ˆä¸çŸ¥é“çœŸå®çš„ $T(s,a,s^{\prime})$ å’Œ $R(s,a,s^{\prime})$ï¼‰ã€‚</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/drl/>DRL</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/drl-notes-6/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 6</h2></div></a></article><article><a href=/p/drl-notes-5/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 5</h2></div></a></article><article><a href=/p/drl-notes-4/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 4</h2></div></a></article><article><a href=/p/drl-notes-3/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 3</h2></div></a></article><article><a href=/p/drl-notes-2/><div class=article-details><h2 class=article-title>Deep Reinforcement Learning Lecture 2</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=SUZ-tsinghua/SUZ-tsinghua.github.io issue-term=title crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2024 suz</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>