<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="本 blog 主要以 TidyBot 这篇工作为例子，简要介绍一下 Language-Conditioned Mobile Manipulation 这个研究领域。本文采取一个从下而上的方式，先从具体的 TidyBot 出发，再去介绍更广的领域。 TidyBot 简单来说，T"><title>Language-Conditioned Mobile Manipulation: 以 TidyBot 为例</title>
<link rel=canonical href=https://suz-tsinghua.github.io/p/tidybot/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Language-Conditioned Mobile Manipulation: 以 TidyBot 为例"><meta property='og:description' content="本 blog 主要以 TidyBot 这篇工作为例子，简要介绍一下 Language-Conditioned Mobile Manipulation 这个研究领域。本文采取一个从下而上的方式，先从具体的 TidyBot 出发，再去介绍更广的领域。 TidyBot 简单来说，T"><meta property='og:url' content='https://suz-tsinghua.github.io/p/tidybot/'><meta property='og:site_name' content='suz'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='DL'><meta property='article:tag' content='NLP'><meta property='article:published_time' content='2024-05-19T14:00:00+08:00'><meta property='article:modified_time' content='2024-05-19T14:00:00+00:00'><meta property='og:image' content='https://suz-tsinghua.github.io/p/tidybot/cover.png'><meta name=twitter:title content="Language-Conditioned Mobile Manipulation: 以 TidyBot 为例"><meta name=twitter:description content="本 blog 主要以 TidyBot 这篇工作为例子，简要介绍一下 Language-Conditioned Mobile Manipulation 这个研究领域。本文采取一个从下而上的方式，先从具体的 TidyBot 出发，再去介绍更广的领域。 TidyBot 简单来说，T"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://suz-tsinghua.github.io/p/tidybot/cover.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu1189040654346373991.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😇</span></figure><div class=site-meta><h1 class=site-name><a href=/>suz</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/SUZ-tsinghua target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=R5Y1xlUAAAAJ&amp;hl=zh-CN&amp;oi=sra' target=_blank title="Google scholar" rel=me><svg class="icon icon-tabler icon-tabler-brand-google" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M20.945 11a9 9 0 11-3.284-5.997l-2.655 2.392A5.5 5.5.0 1017.125 14H13v-3h7.945z"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#tidybot>TidyBot</a><ol><li><a href=#tldr>TL;DR</a></li><li><a href=#method>Method</a></li><li><a href=#experiments>Experiments</a><ol><li><a href=#benchmark-dataset>Benchmark Dataset</a></li><li><a href=#baseline-comparisons>Baseline Comparisons</a></li><li><a href=#ablation-studies>Ablation Studies</a></li><li><a href=#human-evaluation>Human Evaluation</a></li><li><a href=#real-world-experiments>Real-world Experiments</a></li></ol></li></ol></li><li><a href=#language-conditioned-mobile-manipulation>Language-Conditioned Mobile Manipulation</a><ol><li><a href=#architecture-framework>Architecture Framework</a></li><li><a href=#approaches-categorization>Approaches Categorization</a><ol><li><a href=#language-conditioned-reinforcement-learning>Language-conditioned Reinforcement Learning</a></li><li><a href=#language-conditioned-imitation-learning>Language-conditioned Imitation Learning</a></li><li><a href=#empowered-by-llms--vlms>Empowered by LLMs & VLMs</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/tidybot/><img src=/p/tidybot/cover_hu17936045975075887955.png srcset="/p/tidybot/cover_hu17936045975075887955.png 800w, /p/tidybot/cover_hu15819638151306782997.png 1600w" width=800 height=601 loading=lazy alt="Featured image of post Language-Conditioned Mobile Manipulation: 以 TidyBot 为例"></a></div><div class=article-details><header class=article-category><a href=/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>学习笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/tidybot/>Language-Conditioned Mobile Manipulation: 以 TidyBot 为例</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 19, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 9 分钟</time></div></footer></div></header><section class=article-content><p>本 blog 主要以 TidyBot 这篇工作为例子，简要介绍一下 Language-Conditioned Mobile Manipulation 这个研究领域。本文采取一个从下而上的方式，先从具体的 TidyBot 出发，再去介绍更广的领域。</p><h2 id=tidybot>TidyBot</h2><p>简单来说，TidyBot 是一个能 <em>个性化、自动化</em> 地帮助人们整理家中杂乱物品的机器人。</p><ul><li>论文地址：<a class=link href=https://arxiv.org/abs/2305.05658 target=_blank rel=noopener>https://arxiv.org/abs/2305.05658</a></li><li>项目官网：<a class=link href=https://tidybot.cs.princeton.edu/ target=_blank rel=noopener>https://tidybot.cs.princeton.edu/</a></li><li>开源代码：<a class=link href=https://github.com/jimmyyhwu/tidybot target=_blank rel=noopener>https://github.com/jimmyyhwu/tidybot</a></li></ul><div class=video-wrapper><video controls src=/tidybot_demo.mp4 autoplay><p>Your browser doesn't support HTML5 video. Here is a
<a href=/tidybot_demo.mp4>link to the video</a> instead.</p></video></div><h3 id=tldr>TL;DR</h3><p>这篇工作的主要目的就是设计一个能够 <em>个性化、自动化</em> 地帮助人们整理家中杂物的机器人。它需要能够识别地上的物体，判断该物体需要被 <em>如何</em> 收纳到 <em>何处</em>（如扔到垃圾桶里、放到抽屉里、放到沙发上等）。注意两点要求：</p><ul><li>个性化：由于不同的人可能有不同的收纳习惯，可能有些人喜欢把衣服放在架子上，有些人则可能喜欢放在抽屉里。
<img src=/p/tidybot/1.png width=1822 height=834 srcset="/p/tidybot/1_hu11827031459779453228.png 480w, /p/tidybot/1_hu5399589212461683874.png 1024w" loading=lazy class=gallery-image data-flex-grow=218 data-flex-basis=524px>
这意味着机器人不能给出一个广泛的策略（对于所有人来说，都把衣服放到架子上），它必须学习到其主人的喜好，从而指定专门的策略。</li><li>自动化：一旦设定完成，机器人收纳杂物的过程必须是全自动化的，不能让它的主人在旁边告诉它某物应该收纳到某处。</li></ul><p>运用 LLM 的总结推理能力可以很好地解决这两个问题。这篇文章的 methods 非常直接，分为两步：</p><ul><li>在机器人第一次开始工作之前，先让主人提供几个例子，比如“黄衬衫要被放在抽屉里、深紫色衬衫要被放在柜子里、白色袜子要被放在抽屉里、黑色衬衫要被放在柜子里”。将这些例子告诉 LLM，让其总结出规则，LLM 就会总结出：“浅色的东西需要放在抽屉里，深色的东西需要放在柜子里。”</li><li>机器人工作过程中，先识别地上的某个物体，将第一步中得到的规则和这个物体是什么告诉 LLM，LLM 就可以告诉机器人这个物体需要被放在什么地方。</li></ul><p>对于每个物体该 <em>如何</em> 被放置也是同理，先给 LLM 提供一些例子，如 &ldquo;pick and place yellow shirt, pick and place dark purple shirt, pick and toss white socks&rdquo;。LLM 可以总结出 &ldquo;pick and place shirts, pick and toss socks&rdquo;，再将 LLM 的 summarization 用于新物体即可。</p><p>再加上一些物体识别，以及让机器人执行对应的收纳动作，这个个性化、自动化的收纳系统就可以被运用于真实世界中。</p><h3 id=method>Method</h3><p><a class=link href=#tldr># TL;DR</a> 中已经简略介绍了本工作的 methods，接下来 formally 展示下这样一个收纳系统的 pipeline：</p><div align=center><img src=pipeline.png width=50%></div><ul><li>$E_{receptacle}$ 和 $E_{primitive}$ 都是用户的个性化输入，分别代表了每个物品 $o_i$ 需要被收纳到何处 $r_i$，以及需要被如何收纳 $p_i$。</li><li>接着运用 LLM 将 $E_{receptacle}$ 和 $E_{primitive}$ 总结成 $S_{receptacle}$ 和 $S_{primitive}$。</li><li>此时需要将 $S_{receptacle}$ 中 LLM 总结出的物体类别 （如浅色衣服、深色衣服）提取出来，以便于视觉系统进行分类。此处 pipeline 中只写了 $S_{receptacle}$，而没写 $S_{primitive}$，或许是默认了二者提取出来的物体类别是一致的，但严谨来说，同时考虑 $S_{receptacle}$ 和 $S_{primitive}$ 应该更合理。将物体类别 $C$ 提取出来的好处在于，后面进行物体分类的时候就可以只考虑较少的类别，不容易分类错误，而且不同的用户的 $C$ 也可以不同，更加 flexible。</li><li>做好了前置工作，就可以将系统部署到真实的机器人上了，系统会进入以下收纳循环，每一循环收纳一个物品，直到没有物品可以收纳：</li><li><ul><li>利用外置摄像头得到地板的俯视图，通过 ViLD 识别出距离机器人最近的物体。</li></ul></li><li><ul><li>机器人移动到此物体旁，通过其自身的摄像头得到物体的近距离照片，将近距离照片与 $C$ 告诉 CLIP，让其对物体进行分类，得到类别 $c$。</li></ul></li><li><ul><li>让 LLM 根据 $c, S_{receptacle}, S_{primitive}$ 总结出物体该 <em>如何</em> 被放置到 <em>何处</em>。</li></ul></li><li><ul><li>机器人执行相应的收纳动作。</li></ul></li></ul><p><img src=/p/tidybot/real_system.png width=1525 height=377 srcset="/p/tidybot/real_system_hu18148537030116396249.png 480w, /p/tidybot/real_system_hu14811919430971523437.png 1024w" loading=lazy class=gallery-image data-flex-grow=404 data-flex-basis=970px></p><p>涉及到 LLM 的部分，具体 prompt 可以参阅原论文 Appendix A。</p><h3 id=experiments>Experiments</h3><h4 id=benchmark-dataset>Benchmark Dataset</h4><p>为了评估所提出方法的可靠性，作者专门做了一个 benchmark dataset，其中共包含 96 个个性化场景，每个场景里都有一些容器和一些物品，其中有些物品被标注了应该被放到什么容器里，而另一些物品并没被标注。注意，每个场景可能代表了不同的收纳喜好，所以对于同一个物品，不同场景的收纳容器可能大不相同。任务的目的就是根据被标注的物品来预测未被标注的物体应该被放到哪里。</p><p>在这个数据集上，作者做了一些实验：<a class=link href=#baseline-comparisons># Baseline Comparisons</a>, <a class=link href=#ablation-studies># Ablation Studies</a>, <a class=link href=#human-evaluation># Human Evaluation</a>。</p><h4 id=baseline-comparisons>Baseline Comparisons</h4><p>这部分，作者将自己的方法与一些 baseline 作比较，比如只给 LLM 提供标注物体，直接让其预测为被标注物体该被放到哪里，而不经过 summarization 过程；再比如利用 pre-trained text embedding，对于未标注的物体，直接找到与其 embedding 距离最近的标注物体，认为二者应该被收纳到同一个地方。结论就是，作者的方法胜过其他 baseline。</p><p><img src=/p/tidybot/baseline.png width=1041 height=534 srcset="/p/tidybot/baseline_hu2260679436813115871.png 480w, /p/tidybot/baseline_hu7433888565880427837.png 1024w" loading=lazy class=gallery-image data-flex-grow=194 data-flex-basis=467px></p><h4 id=ablation-studies>Ablation Studies</h4><p>作者一共做了三个方面的 ablation studies：</p><ul><li><ol><li>不利用 user specific preference，直接让 LLM 依据 commonsense 来推断物品应该被放到哪里。</li></ol></li><li><ol start=2><li>让人类来进行 summarization，不用 LLM 做。</li></ol></li><li><ol start=3><li>比较采用不同 LLM 的准确率。</li></ol></li></ul><p><img src=/p/tidybot/ablation_studies_1.png width=658 height=291 srcset="/p/tidybot/ablation_studies_1_hu9791429553669372484.png 480w, /p/tidybot/ablation_studies_1_hu16712101575763368406.png 1024w" loading=lazy class=gallery-image data-flex-grow=226 data-flex-basis=542px> <img src=/p/tidybot/ablation_studies_2.png width=799 height=384 srcset="/p/tidybot/ablation_studies_2_hu4240727006035632664.png 480w, /p/tidybot/ablation_studies_2_hu11499249390621271082.png 1024w" loading=lazy class=gallery-image data-flex-grow=208 data-flex-basis=499px></p><p>结论是，让 LLM 进行 summarization 会比直接用 commonsense 有非常大的提升，但相较于直接让人类进行 summarization 仍有不足。这也说明通过提升 LLM 的总结能力还能进一步提升此系统的能力。</p><p>另一方面，在不同的 LLM 中，text-davinci-003 有较好的效果。</p><h4 id=human-evaluation>Human Evaluation</h4><p>作者还招募了一些志愿者，向他们提供 user preference、baseline 给出的收纳建议、自己方法给出的收纳建议，让他们比较自己的方法与 baseline 的结果，哪个更符合 user preference。题目形式如下图所示：</p><p><img src=/p/tidybot/question.png width=744 height=547 srcset="/p/tidybot/question_hu15245168944330260646.png 480w, /p/tidybot/question_hu16038960857171736720.png 1024w" loading=lazy class=gallery-image data-flex-grow=136 data-flex-basis=326px></p><p>结果显示，作者的方法有 46.9% 的情况被认为更好，而 baseline 只有 19.1% 的情况被认为更好。</p><p><img src=/p/tidybot/human_evaluation.png width=1604 height=275 srcset="/p/tidybot/human_evaluation_hu12242310108710729194.png 480w, /p/tidybot/human_evaluation_hu7342609013758737524.png 1024w" loading=lazy class=gallery-image data-flex-grow=583 data-flex-basis=1399px></p><h4 id=real-world-experiments>Real-world Experiments</h4><p>正如 <a class=link href=#method># Method</a> 中说的，作者还搭建了一个真实的机器人平台，让文章中提出的个性化收纳方法能够落地。作者构造了 8 个真实场景，每个场景包含一些散落在地上的物品以及几个收纳容器，然后让系统根据 <a class=link href=#method># Method</a> 中的 pipeline 运行。结果显示，系统在 85.0% 情况下都能正确完成收纳任务。</p><p><img src=/p/tidybot/real.png width=1671 height=592 srcset="/p/tidybot/real_hu16826072460319586654.png 480w, /p/tidybot/real_hu17600697350256475454.png 1024w" loading=lazy class=gallery-image data-flex-grow=282 data-flex-basis=677px></p><h2 id=language-conditioned-mobile-manipulation>Language-Conditioned Mobile Manipulation</h2><p>TidyBot 属于一个更广的研究领域 Language-Conditioned Mobile Manipulation。这个领域将 CV、NLP、Robotics 结合了起来，要求机器人能够根据人类的自然语言指令去做出相应的行为。</p><div align=center><img src=cross_field.png width=50%></div><p>这篇文章为 Language-Conditioned Mobile Manipulation 领域做了个详细的调查：</p><ul><li>论文地址：<a class=link href=https://arxiv.org/pdf/2312.10807 target=_blank rel=noopener>https://arxiv.org/pdf/2312.10807</a></li><li>开源代码：<a class=link href=https://github.com/hk-zh/language-conditioned-robot-manipulation-models target=_blank rel=noopener>https://github.com/hk-zh/language-conditioned-robot-manipulation-models</a></li></ul><h3 id=architecture-framework>Architecture Framework</h3><p><img src=/p/tidybot/framework.png width=1425 height=934 srcset="/p/tidybot/framework_hu8973135417564341417.png 480w, /p/tidybot/framework_hu14937730675035186088.png 1024w" loading=lazy class=gallery-image data-flex-grow=152 data-flex-basis=366px></p><p>此领域工作的总体框架如上图所示。主要包括三个模块</p><ul><li>语言模块。其主要作用是理解用户的语言输入，并转化为机器人动作指导。如 TidyBot 中，用户告诉系统收纳 preference，语言模块就会进行处理，进行 preference 的总结等。</li><li>感知模块。其主要作用是感知周围环境，例如 TidyBot 中机器人利用自身的相机去识别物体进行分类。</li><li>控制模块。其主要作用是让机器人执行需要执行的指令。对应到 TidyBot 中，就是机器人去执行 “移动到某处”、“拿起地上的物品”、“把物品放置到某处” 等。在 TidyBot 中，这样的动作是 hard-coded 的，当然也可以采用 reinforcement learning (RL), imitation learing (IL) 等方法得到。</li></ul><h3 id=approaches-categorization>Approaches Categorization</h3><p>Language-Conditioned Mobile Manipulation 的工作主要可以被粗略分为以下几类：</p><ul><li>Language-conditioned Reinforcement Learning</li><li>Language-conditioned Imitation Learning</li><li>Empowered by LLMs & VLMs</li></ul><p>当然，有些工作可能可以被同时划分到多种类别中。其中，前两种方法较为传统，没有采用大语言模型等现成工具。第三种方法利用现成的 LLMs 与 VLMs，简化了系统，提高了能力。</p><p><img src=/p/tidybot/categories.png width=1955 height=1212 srcset="/p/tidybot/categories_hu14519433019373800883.png 480w, /p/tidybot/categories_hu12373372102478987912.png 1024w" loading=lazy class=gallery-image data-flex-grow=161 data-flex-basis=387px></p><h4 id=language-conditioned-reinforcement-learning>Language-conditioned Reinforcement Learning</h4><p>此类工作利用强化学习，通过人为设计等方法，建立一个从自然语言到 reward 的一个映射，当 agent 达到自然语言描述的目标时，它就能得到对应的 reward。Agent 在这个过程中可以学习到一个从自然语言到具体动作的映射。具体工作有：</p><ul><li>Lancon-learn: Learning with language to enable generalization in multi-task manipulation <a class=link href=https://ieeexplore.ieee.org/document/9667188 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/hartikainen/metaworld/tree/reward-tweaks-rebase target=_blank rel=noopener>[code]</a></li><li>Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards <a class=link href=https://proceedings.mlr.press/v155/goyal21a.html target=_blank rel=noopener>[paper]</a><a class=link href=https://github.com/prasoongoyal/PixL2R target=_blank rel=noopener>[code]</a></li><li>Learning from symmetry: Meta-reinforcement learning with symmetrical behaviors and language instructions <a class=link href=https://arxiv.org/abs/2209.10656 target=_blank rel=noopener>[paper]</a><a class=link href=https://tumi6robot.wixsite.com/symmetry/ target=_blank rel=noopener>[website]</a></li><li>Meta-reinforcement learning via language instructions <a class=link href=https://arxiv.org/abs/2209.04924 target=_blank rel=noopener>[paper]</a><a class=link href=https://github.com/yaoxt3/MILLION target=_blank rel=noopener>[code]</a><a class=link href=https://tumi6robot.wixsite.com/million target=_blank rel=noopener>[website]</a></li><li>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation <a class=link href=https://proceedings.mlr.press/v164/nair22a/nair22a.pdf target=_blank rel=noopener>[paper]</a></li><li>Concept2robot: Learning manipulation concepts from instructions and human demonstrations <a class=link href=https://www.roboticsproceedings.org/rss16/p082.pdf target=_blank rel=noopener>[paper]</a></li></ul><h4 id=language-conditioned-imitation-learning>Language-conditioned Imitation Learning</h4><p>此类工作利用模仿学习的范式，其不像强化学习那样要求提供 reward，但是需要提供一些正确的行为例子 (expert demonstrations)，agent 会根据这些正确的行为进行学习。具体可以再被细分为 behavior cloning (BC) 和 inverse reinforcement learning (IRL)。</p><p>BC 就是直接依样画葫芦，expert demonstrations 里怎么做，agent 就怎么做。具体工作有：</p><ul><li>Language conditioned imitation learning over unstructured data <a class=link href=https://arxiv.org/abs/2005.07648 target=_blank rel=noopener>[paper]</a> <a class=link href>[code]</a> <a class=link href=https://language-play.github.io/ target=_blank rel=noopener>[website]</a></li><li>Bc-z: Zero-shot task generalization with robotic imitation learning <a class=link href=https://arxiv.org/abs/2202.02005 target=_blank rel=noopener>[paper]</a></li><li>What matters in language-conditioned robotic imitation learning over unstructured data <a class=link href=https://arxiv.org/abs/2204.06252 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/lukashermann/hulc target=_blank rel=noopener>[code]</a><a class=link href=http://hulc.cs.uni-freiburg.de/ target=_blank rel=noopener>[website]</a></li><li>Grounding language with visual affordances over unstructured data <a class=link href=https://arxiv.org/abs/2210.01911 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/mees/hulc2 target=_blank rel=noopener>[code]</a><a class=link href=http://hulc2.cs.uni-freiburg.de/ target=_blank rel=noopener>[website]</a></li><li>Language-conditioned imitation learning with base skill priors under unstructured data <a class=link href=https://arxiv.org/abs/2305.19075 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/hk-zh/spil target=_blank rel=noopener>[code]</a> <a class=link href=https://hk-zh.github.io/spil/ target=_blank rel=noopener>[website]</a></li><li>Pay attention!- robustifying a deep visuomotor policy through task-focused visual attention <a class=link href=https://arxiv.org/abs/1809.10093 target=_blank rel=noopener>[paper]</a></li><li>Language-conditioned imitation learning for robot manipulation tasks <a class=link href=https://arxiv.org/abs/2010.12083 target=_blank rel=noopener>[paper]</a></li></ul><p>IRL 则需要经历两个步骤，第一步先从 expert demonstrations 和语言命令中学习一个从自然语言命令到 reward 的映射，再通过 RL 的方式学习行为策略（这样看来，此部分与 <a class=link href=#language-conditioned-reinforcement-learning># Language-conditioned Reinforcement Learning</a> 也有交集）。具体工作：</p><ul><li>Grounding english commands to reward function <a class=link href=https://www.roboticsproceedings.org/rss11/p18.pdf target=_blank rel=noopener>[paper]</a></li><li>From language to goals: Inverse reinforcement learning for vision-based instruction following <a class=link href=https://arxiv.org/abs/1902.07742 target=_blank rel=noopener>[paper]</a></li></ul><h4 id=empowered-by-llms--vlms>Empowered by LLMs & VLMs</h4><p>前两类方法均需要对文本信息进行学习，而有了 LLM 这样强有力的工具，就可以对系统进行简化。具体而言，可以利用好大语言模型的 planning 和 reasoning 能力。</p><p>大语言模型的 planning 能力指的是其将复杂任务转化为一系列简单的、机器人能够执行的任务的能力。譬如要求机器人炒菜，直接学习一个炒菜的策略是非常难的，但可以先让 LLM 将炒菜的动作拆分成 洗菜、放油、放菜 等一系列简单的、机器人能够学会的动作，此时再让机器人去执行这些动作就能完成炒菜的任务了。具体工作有：</p><ul><li>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning <a class=link href=https://arxiv.org/abs/2307.06135 target=_blank rel=noopener>[paper]</a></li><li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <a class=link href=https://arxiv.org/abs/2201.07207 target=_blank rel=noopener>[paper]</a></li><li>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents <a class=link href=https://arxiv.org/abs/2302.01560 target=_blank rel=noopener>[paper]</a></li><li>Progprompt: Generating situated robot task plans using large language models <a class=link href=https://arxiv.org/abs/2209.11302 target=_blank rel=noopener>[paper]</a></li><li>Robots that ask for help: Uncertainty alignment for large language model planners <a class=link href=https://arxiv.org/abs/2307.01928 target=_blank rel=noopener>[paper]</a></li><li>Task and motion planning with large language models for object rearrangement <a class=link href=https://arxiv.org/abs/2303.06247 target=_blank rel=noopener>[paper]</a></li><li>Do as i can, not as i say: Grounding language in robotic affordances <a class=link href=https://arxiv.org/abs/2204.01691 target=_blank rel=noopener>[paper]</a></li><li>The 2014 international planning competition: Progress and trends <a class=link href=https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2571 target=_blank rel=noopener>[paper]</a></li><li>Robot task planning via deep reinforcement learning: a tabletop object sorting application <a class=link href=https://ieeexplore.ieee.org/document/8914278 target=_blank rel=noopener>[paper]</a></li><li>Robot task planning and situation handling in open worlds <a class=link href=https://arxiv.org/abs/2210.01287 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/yding25/GPT-Planner target=_blank rel=noopener>[code]</a> <a class=link href=https://cowplanning.github.io/ target=_blank rel=noopener>[website]</a></li><li>Embodied Task Planning with Large Language Models <a class=link href=https://arxiv.org/abs/2307.01848 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/Gary3410/TaPA target=_blank rel=noopener>[code]</a> <a class=link href=https://gary3410.github.io/TaPA/ target=_blank rel=noopener>[website]</a></li><li>Text2motion: From natural language instructions to feasible plans <a class=link href=https://arxiv.org/abs/2303.12153 target=_blank rel=noopener>[paper]</a> <a class=link href=https://sites.google.com/stanford.edu/text2motion target=_blank rel=noopener>[website]</a></li><li>Large language models as commonsense knowledge for large-scale task planning <a class=link href=https://arxiv.org/abs/2305.14078 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/1989Ryan/llm-mcts target=_blank rel=noopener>[code]</a> <a class=link href=https://llm-mcts.github.io/ target=_blank rel=noopener>[website]</a></li><li>Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation <a class=link href=https://arxiv.org/abs/2305.18898 target=_blank rel=noopener>[paper]</a></li><li>Learning to reason over scene graphs: a case study of finetuning gpt-2 into a robot language model for grounded task planning <a class=link href=https://www.frontiersin.org/articles/10.3389/frobt.2023.1221739/full target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/dnandha/RobLM target=_blank rel=noopener>[code]</a></li><li>Scaling up and distilling down: Language-guided robot skill acquisition <a class=link href=https://arxiv.org/abs/2307.14535 target=_blank rel=noopener>[paper]</a><a class=link href=https://github.com/real-stanford/scalingup target=_blank rel=noopener>[code]</a> <a class=link href=https://www.cs.columbia.edu/~huy/scalingup/ target=_blank rel=noopener>[website]</a></li><li>Stap: Sequencing task-agnostic policies <a class=link href=https://ieeexplore.ieee.org/document/10160220 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/agiachris/STAP target=_blank rel=noopener>[code]</a><a class=link href=https://sites.google.com/stanford.edu/stap/home target=_blank rel=noopener>[website]</a></li><li>Inner monologue: Embodied reasoning through planning with language models <a class=link href=https://arxiv.org/abs/2207.05608 target=_blank rel=noopener>[paper]</a> <a class=link href=https://innermonologue.github.io/ target=_blank rel=noopener>[website]</a></li></ul><p>大语言模型的 reasoning 能力就像 TidyBot 中展示的那样，利用 LLM 去推理某个物品应该被放置到何处，再让机器人去执行特定的策略。具体工作有：</p><ul><li>Rearrangement:A challenge for embodied ai <a class=link href=https://arxiv.org/abs/2011.01975 target=_blank rel=noopener>[paper]</a></li><li>The threedworld transport challenge: A visually guided task and motion planning benchmark for physically realistic embodied ai <a class=link href=https://ieeexplore.ieee.org/document/9812329 target=_blank rel=noopener>[paper]</a></li><li>Tidy up my room: Multi-agent cooperation for service tasks in smart environments <a class=link href=https://dl.acm.org/doi/abs/10.3233/AIS-190524 target=_blank rel=noopener>[paper]</a></li><li>A quantifiable stratification strategy for tidy-up in service robotics <a class=link href=https://ieeexplore.ieee.org/document/9542842 target=_blank rel=noopener>[paper]</a></li><li>Tidybot: Personalized robot assistance with large language models <a class=link href=https://arxiv.org/abs/2305.05658 target=_blank rel=noopener>[paper]</a></li><li>Housekeep: Tidying virtual households using commonsense reasoning <a class=link href=https://arxiv.org/abs/2205.10712 target=_blank rel=noopener>[paper]</a></li><li>Building cooperative embodied agents modularly with large language models <a class=link href=https://arxiv.org/abs/2307.02485 target=_blank rel=noopener>[paper]</a></li><li>Socratic models: Composing zero-shot multimodal reasoning with language <a class=link href=https://arxiv.org/abs/2204.00598 target=_blank rel=noopener>[paper]</a></li><li>Voyager: An open-ended embodied agent with large language models <a class=link href=https://arxiv.org/abs/2305.16291 target=_blank rel=noopener>[paper]</a></li><li>Translating natural language to planning goals with large-language models <a class=link href=https://arxiv.org/abs/2302.05128 target=_blank rel=noopener>[paper]</a></li></ul><p>在自然语言的基础上，可以再加上视觉工具，比如 TidyBot 中识别物体的部分。利用了 VLMs 的具体工作有：</p><ul><li>Cliport: What and where pathways for robotic manipulation <a class=link href=https://arxiv.org/abs/2109.12098 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/cliport/cliport target=_blank rel=noopener>[code]</a> <a class=link href=https://cliport.github.io/ target=_blank rel=noopener>[website]</a></li><li>Transporter networks: Rearranging the visual world for robotic manipulation <a class=link href=https://proceedings.mlr.press/v155/zeng21a/zeng21a.pdf target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/google-research/ravens target=_blank rel=noopener>[code]</a> <a class=link href=https://transporternets.github.io/ target=_blank rel=noopener>[website]</a></li><li>Simple but effective: Clip embeddings for embodied ai <a class=link href=https://openaccess.thecvf.com/content/CVPR2022/papers/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.pdf target=_blank rel=noopener>[paper]</a></li><li>Instruct2act: Mapping multi-modality instructions to robotic actions with large language model <a class=link href=https://arxiv.org/abs/2305.11176 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/OpenGVLab/Instruct2Act target=_blank rel=noopener>[code]</a></li><li>Latte: Language trajectory transformer <a class=link href=https://arxiv.org/abs/2208.02918 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr target=_blank rel=noopener>[code]</a></li><li>Embodied Task Planning with Large Language Models <a class=link href=https://arxiv.org/abs/2307.01848 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/Gary3410/TaPA target=_blank rel=noopener>[code]</a> <a class=link href=https://gary3410.github.io/TaPA/ target=_blank rel=noopener>[website]</a></li><li>Palm-e: An embodied multimodal language model <a class=link href=https://arxiv.org/abs/2303.03378 target=_blank rel=noopener>[paper]</a> <a class=link href=https://palm-e.github.io/ target=_blank rel=noopener>[website]</a></li><li>Socratic models: Composing zero-shot multimodal reasoning with language <a class=link href=https://arxiv.org/abs/2204.00598 target=_blank rel=noopener>[paper]</a></li><li>Pretrained language models as visual planners for human assistance <a class=link href=https://openaccess.thecvf.com/content/ICCV2023/papers/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.pdf target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/facebookresearch/vlamp target=_blank rel=noopener>[code]</a></li><li>Open-world object manipulation using pre-trained vision-language models <a class=link href=https://arxiv.org/abs/2303.00905 target=_blank rel=noopener>[paper]</a> <a class=link href=https://robot-moo.github.io/ target=_blank rel=noopener>[website]</a></li><li>Robotic skill acquisition via instruction augmentation with vision-language models <a class=link href=https://arxiv.org/abs/2211.11736 target=_blank rel=noopener>[paper]</a> <a class=link href=https://instructionaugmentation.github.io/ target=_blank rel=noopener>[website]</a></li><li>Language reward modulation for pretraining reinforcement learning <a class=link href=https://arxiv.org/abs/2308.12270 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/ademiadeniji/lamp target=_blank rel=noopener>[code]</a></li><li>Vision-language models as success detectors <a class=link href=https://proceedings.mlr.press/v232/du23b.html target=_blank rel=noopener>[paper]</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/dl/>DL</a>
<a href=/tags/nlp/>NLP</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 May 19, 2024 14:00 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/gedl-notes-1.7/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.7</h2></div></a></article><article><a href=/p/gedl-notes-1.6/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.6</h2></div></a></article><article><a href=/p/gedl-notes-1.5/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.5</h2></div></a></article><article><a href=/p/gedl-notes-1.4/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.4</h2></div></a></article><article><a href=/p/gedl-notes-1.3/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.3</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=SUZ-tsinghua/SUZ-tsinghua.github.io issue-term=title crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2024 suz</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>