<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="æœ¬ blog ä¸»è¦ä»¥ TidyBot è¿™ç¯‡å·¥ä½œä¸ºä¾‹å­ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ Language-Conditioned Mobile Manipulation è¿™ä¸ªç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡é‡‡å–ä¸€ä¸ªä»ä¸‹è€Œä¸Šçš„æ–¹å¼ï¼Œå…ˆä»å…·ä½“çš„ TidyBot å‡ºå‘ï¼Œå†å»ä»‹ç»æ›´å¹¿çš„é¢†åŸŸã€‚ TidyBot ç®€å•æ¥è¯´ï¼ŒT"><title>Language-Conditioned Mobile Manipulation: ä»¥ TidyBot ä¸ºä¾‹</title>
<link rel=canonical href=https://suz-tsinghua.github.io/p/tidybot/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Language-Conditioned Mobile Manipulation: ä»¥ TidyBot ä¸ºä¾‹"><meta property='og:description' content="æœ¬ blog ä¸»è¦ä»¥ TidyBot è¿™ç¯‡å·¥ä½œä¸ºä¾‹å­ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ Language-Conditioned Mobile Manipulation è¿™ä¸ªç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡é‡‡å–ä¸€ä¸ªä»ä¸‹è€Œä¸Šçš„æ–¹å¼ï¼Œå…ˆä»å…·ä½“çš„ TidyBot å‡ºå‘ï¼Œå†å»ä»‹ç»æ›´å¹¿çš„é¢†åŸŸã€‚ TidyBot ç®€å•æ¥è¯´ï¼ŒT"><meta property='og:url' content='https://suz-tsinghua.github.io/p/tidybot/'><meta property='og:site_name' content='suz'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='DL'><meta property='article:tag' content='NLP'><meta property='article:published_time' content='2024-05-19T14:00:00+08:00'><meta property='article:modified_time' content='2024-05-19T14:00:00+00:00'><meta property='og:image' content='https://suz-tsinghua.github.io/p/tidybot/cover.png'><meta name=twitter:title content="Language-Conditioned Mobile Manipulation: ä»¥ TidyBot ä¸ºä¾‹"><meta name=twitter:description content="æœ¬ blog ä¸»è¦ä»¥ TidyBot è¿™ç¯‡å·¥ä½œä¸ºä¾‹å­ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ Language-Conditioned Mobile Manipulation è¿™ä¸ªç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡é‡‡å–ä¸€ä¸ªä»ä¸‹è€Œä¸Šçš„æ–¹å¼ï¼Œå…ˆä»å…·ä½“çš„ TidyBot å‡ºå‘ï¼Œå†å»ä»‹ç»æ›´å¹¿çš„é¢†åŸŸã€‚ TidyBot ç®€å•æ¥è¯´ï¼ŒT"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://suz-tsinghua.github.io/p/tidybot/cover.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu1189040654346373991.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ˜‡</span></figure><div class=site-meta><h1 class=site-name><a href=/>suz</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/SUZ-tsinghua target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=R5Y1xlUAAAAJ&amp;hl=zh-CN&amp;oi=sra' target=_blank title="Google scholar" rel=me><svg class="icon icon-tabler icon-tabler-brand-google" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M20.945 11a9 9 0 11-3.284-5.997l-2.655 2.392A5.5 5.5.0 1017.125 14H13v-3h7.945z"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#tidybot>TidyBot</a><ol><li><a href=#tldr>TL;DR</a></li><li><a href=#method>Method</a></li><li><a href=#experiments>Experiments</a><ol><li><a href=#benchmark-dataset>Benchmark Dataset</a></li><li><a href=#baseline-comparisons>Baseline Comparisons</a></li><li><a href=#ablation-studies>Ablation Studies</a></li><li><a href=#human-evaluation>Human Evaluation</a></li><li><a href=#real-world-experiments>Real-world Experiments</a></li></ol></li></ol></li><li><a href=#language-conditioned-mobile-manipulation>Language-Conditioned Mobile Manipulation</a><ol><li><a href=#architecture-framework>Architecture Framework</a></li><li><a href=#approaches-categorization>Approaches Categorization</a><ol><li><a href=#language-conditioned-reinforcement-learning>Language-conditioned Reinforcement Learning</a></li><li><a href=#language-conditioned-imitation-learning>Language-conditioned Imitation Learning</a></li><li><a href=#empowered-by-llms--vlms>Empowered by LLMs & VLMs</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/tidybot/><img src=/p/tidybot/cover_hu17936045975075887955.png srcset="/p/tidybot/cover_hu17936045975075887955.png 800w, /p/tidybot/cover_hu15819638151306782997.png 1600w" width=800 height=601 loading=lazy alt="Featured image of post Language-Conditioned Mobile Manipulation: ä»¥ TidyBot ä¸ºä¾‹"></a></div><div class=article-details><header class=article-category><a href=/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>å­¦ä¹ ç¬”è®°</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/tidybot/>Language-Conditioned Mobile Manipulation: ä»¥ TidyBot ä¸ºä¾‹</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 19, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 9 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><p>æœ¬ blog ä¸»è¦ä»¥ TidyBot è¿™ç¯‡å·¥ä½œä¸ºä¾‹å­ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ Language-Conditioned Mobile Manipulation è¿™ä¸ªç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡é‡‡å–ä¸€ä¸ªä»ä¸‹è€Œä¸Šçš„æ–¹å¼ï¼Œå…ˆä»å…·ä½“çš„ TidyBot å‡ºå‘ï¼Œå†å»ä»‹ç»æ›´å¹¿çš„é¢†åŸŸã€‚</p><h2 id=tidybot>TidyBot</h2><p>ç®€å•æ¥è¯´ï¼ŒTidyBot æ˜¯ä¸€ä¸ªèƒ½ <em>ä¸ªæ€§åŒ–ã€è‡ªåŠ¨åŒ–</em> åœ°å¸®åŠ©äººä»¬æ•´ç†å®¶ä¸­æ‚ä¹±ç‰©å“çš„æœºå™¨äººã€‚</p><ul><li>è®ºæ–‡åœ°å€ï¼š<a class=link href=https://arxiv.org/abs/2305.05658 target=_blank rel=noopener>https://arxiv.org/abs/2305.05658</a></li><li>é¡¹ç›®å®˜ç½‘ï¼š<a class=link href=https://tidybot.cs.princeton.edu/ target=_blank rel=noopener>https://tidybot.cs.princeton.edu/</a></li><li>å¼€æºä»£ç ï¼š<a class=link href=https://github.com/jimmyyhwu/tidybot target=_blank rel=noopener>https://github.com/jimmyyhwu/tidybot</a></li></ul><div class=video-wrapper><video controls src=/tidybot_demo.mp4 autoplay><p>Your browser doesn't support HTML5 video. Here is a
<a href=/tidybot_demo.mp4>link to the video</a> instead.</p></video></div><h3 id=tldr>TL;DR</h3><p>è¿™ç¯‡å·¥ä½œçš„ä¸»è¦ç›®çš„å°±æ˜¯è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿ <em>ä¸ªæ€§åŒ–ã€è‡ªåŠ¨åŒ–</em> åœ°å¸®åŠ©äººä»¬æ•´ç†å®¶ä¸­æ‚ç‰©çš„æœºå™¨äººã€‚å®ƒéœ€è¦èƒ½å¤Ÿè¯†åˆ«åœ°ä¸Šçš„ç‰©ä½“ï¼Œåˆ¤æ–­è¯¥ç‰©ä½“éœ€è¦è¢« <em>å¦‚ä½•</em> æ”¶çº³åˆ° <em>ä½•å¤„</em>ï¼ˆå¦‚æ‰”åˆ°åƒåœ¾æ¡¶é‡Œã€æ”¾åˆ°æŠ½å±‰é‡Œã€æ”¾åˆ°æ²™å‘ä¸Šç­‰ï¼‰ã€‚æ³¨æ„ä¸¤ç‚¹è¦æ±‚ï¼š</p><ul><li>ä¸ªæ€§åŒ–ï¼šç”±äºä¸åŒçš„äººå¯èƒ½æœ‰ä¸åŒçš„æ”¶çº³ä¹ æƒ¯ï¼Œå¯èƒ½æœ‰äº›äººå–œæ¬¢æŠŠè¡£æœæ”¾åœ¨æ¶å­ä¸Šï¼Œæœ‰äº›äººåˆ™å¯èƒ½å–œæ¬¢æ”¾åœ¨æŠ½å±‰é‡Œã€‚
<img src=/p/tidybot/1.png width=1822 height=834 srcset="/p/tidybot/1_hu11827031459779453228.png 480w, /p/tidybot/1_hu5399589212461683874.png 1024w" loading=lazy class=gallery-image data-flex-grow=218 data-flex-basis=524px>
è¿™æ„å‘³ç€æœºå™¨äººä¸èƒ½ç»™å‡ºä¸€ä¸ªå¹¿æ³›çš„ç­–ç•¥ï¼ˆå¯¹äºæ‰€æœ‰äººæ¥è¯´ï¼Œéƒ½æŠŠè¡£æœæ”¾åˆ°æ¶å­ä¸Šï¼‰ï¼Œå®ƒå¿…é¡»å­¦ä¹ åˆ°å…¶ä¸»äººçš„å–œå¥½ï¼Œä»è€ŒæŒ‡å®šä¸“é—¨çš„ç­–ç•¥ã€‚</li><li>è‡ªåŠ¨åŒ–ï¼šä¸€æ—¦è®¾å®šå®Œæˆï¼Œæœºå™¨äººæ”¶çº³æ‚ç‰©çš„è¿‡ç¨‹å¿…é¡»æ˜¯å…¨è‡ªåŠ¨åŒ–çš„ï¼Œä¸èƒ½è®©å®ƒçš„ä¸»äººåœ¨æ—è¾¹å‘Šè¯‰å®ƒæŸç‰©åº”è¯¥æ”¶çº³åˆ°æŸå¤„ã€‚</li></ul><p>è¿ç”¨ LLM çš„æ€»ç»“æ¨ç†èƒ½åŠ›å¯ä»¥å¾ˆå¥½åœ°è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚è¿™ç¯‡æ–‡ç« çš„ methods éå¸¸ç›´æ¥ï¼Œåˆ†ä¸ºä¸¤æ­¥ï¼š</p><ul><li>åœ¨æœºå™¨äººç¬¬ä¸€æ¬¡å¼€å§‹å·¥ä½œä¹‹å‰ï¼Œå…ˆè®©ä¸»äººæä¾›å‡ ä¸ªä¾‹å­ï¼Œæ¯”å¦‚â€œé»„è¡¬è¡«è¦è¢«æ”¾åœ¨æŠ½å±‰é‡Œã€æ·±ç´«è‰²è¡¬è¡«è¦è¢«æ”¾åœ¨æŸœå­é‡Œã€ç™½è‰²è¢œå­è¦è¢«æ”¾åœ¨æŠ½å±‰é‡Œã€é»‘è‰²è¡¬è¡«è¦è¢«æ”¾åœ¨æŸœå­é‡Œâ€ã€‚å°†è¿™äº›ä¾‹å­å‘Šè¯‰ LLMï¼Œè®©å…¶æ€»ç»“å‡ºè§„åˆ™ï¼ŒLLM å°±ä¼šæ€»ç»“å‡ºï¼šâ€œæµ…è‰²çš„ä¸œè¥¿éœ€è¦æ”¾åœ¨æŠ½å±‰é‡Œï¼Œæ·±è‰²çš„ä¸œè¥¿éœ€è¦æ”¾åœ¨æŸœå­é‡Œã€‚â€</li><li>æœºå™¨äººå·¥ä½œè¿‡ç¨‹ä¸­ï¼Œå…ˆè¯†åˆ«åœ°ä¸Šçš„æŸä¸ªç‰©ä½“ï¼Œå°†ç¬¬ä¸€æ­¥ä¸­å¾—åˆ°çš„è§„åˆ™å’Œè¿™ä¸ªç‰©ä½“æ˜¯ä»€ä¹ˆå‘Šè¯‰ LLMï¼ŒLLM å°±å¯ä»¥å‘Šè¯‰æœºå™¨äººè¿™ä¸ªç‰©ä½“éœ€è¦è¢«æ”¾åœ¨ä»€ä¹ˆåœ°æ–¹ã€‚</li></ul><p>å¯¹äºæ¯ä¸ªç‰©ä½“è¯¥ <em>å¦‚ä½•</em> è¢«æ”¾ç½®ä¹Ÿæ˜¯åŒç†ï¼Œå…ˆç»™ LLM æä¾›ä¸€äº›ä¾‹å­ï¼Œå¦‚ &ldquo;pick and place yellow shirt, pick and place dark purple shirt, pick and toss white socks&rdquo;ã€‚LLM å¯ä»¥æ€»ç»“å‡º &ldquo;pick and place shirts, pick and toss socks&rdquo;ï¼Œå†å°† LLM çš„ summarization ç”¨äºæ–°ç‰©ä½“å³å¯ã€‚</p><p>å†åŠ ä¸Šä¸€äº›ç‰©ä½“è¯†åˆ«ï¼Œä»¥åŠè®©æœºå™¨äººæ‰§è¡Œå¯¹åº”çš„æ”¶çº³åŠ¨ä½œï¼Œè¿™ä¸ªä¸ªæ€§åŒ–ã€è‡ªåŠ¨åŒ–çš„æ”¶çº³ç³»ç»Ÿå°±å¯ä»¥è¢«è¿ç”¨äºçœŸå®ä¸–ç•Œä¸­ã€‚</p><h3 id=method>Method</h3><p><a class=link href=#tldr># TL;DR</a> ä¸­å·²ç»ç®€ç•¥ä»‹ç»äº†æœ¬å·¥ä½œçš„ methodsï¼Œæ¥ä¸‹æ¥ formally å±•ç¤ºä¸‹è¿™æ ·ä¸€ä¸ªæ”¶çº³ç³»ç»Ÿçš„ pipelineï¼š</p><div align=center><img src=pipeline.png width=50%></div><ul><li>$E_{receptacle}$ å’Œ $E_{primitive}$ éƒ½æ˜¯ç”¨æˆ·çš„ä¸ªæ€§åŒ–è¾“å…¥ï¼Œåˆ†åˆ«ä»£è¡¨äº†æ¯ä¸ªç‰©å“ $o_i$ éœ€è¦è¢«æ”¶çº³åˆ°ä½•å¤„ $r_i$ï¼Œä»¥åŠéœ€è¦è¢«å¦‚ä½•æ”¶çº³ $p_i$ã€‚</li><li>æ¥ç€è¿ç”¨ LLM å°† $E_{receptacle}$ å’Œ $E_{primitive}$ æ€»ç»“æˆ $S_{receptacle}$ å’Œ $S_{primitive}$ã€‚</li><li>æ­¤æ—¶éœ€è¦å°† $S_{receptacle}$ ä¸­ LLM æ€»ç»“å‡ºçš„ç‰©ä½“ç±»åˆ« ï¼ˆå¦‚æµ…è‰²è¡£æœã€æ·±è‰²è¡£æœï¼‰æå–å‡ºæ¥ï¼Œä»¥ä¾¿äºè§†è§‰ç³»ç»Ÿè¿›è¡Œåˆ†ç±»ã€‚æ­¤å¤„ pipeline ä¸­åªå†™äº† $S_{receptacle}$ï¼Œè€Œæ²¡å†™ $S_{primitive}$ï¼Œæˆ–è®¸æ˜¯é»˜è®¤äº†äºŒè€…æå–å‡ºæ¥çš„ç‰©ä½“ç±»åˆ«æ˜¯ä¸€è‡´çš„ï¼Œä½†ä¸¥è°¨æ¥è¯´ï¼ŒåŒæ—¶è€ƒè™‘ $S_{receptacle}$ å’Œ $S_{primitive}$ åº”è¯¥æ›´åˆç†ã€‚å°†ç‰©ä½“ç±»åˆ« $C$ æå–å‡ºæ¥çš„å¥½å¤„åœ¨äºï¼Œåé¢è¿›è¡Œç‰©ä½“åˆ†ç±»çš„æ—¶å€™å°±å¯ä»¥åªè€ƒè™‘è¾ƒå°‘çš„ç±»åˆ«ï¼Œä¸å®¹æ˜“åˆ†ç±»é”™è¯¯ï¼Œè€Œä¸”ä¸åŒçš„ç”¨æˆ·çš„ $C$ ä¹Ÿå¯ä»¥ä¸åŒï¼Œæ›´åŠ  flexibleã€‚</li><li>åšå¥½äº†å‰ç½®å·¥ä½œï¼Œå°±å¯ä»¥å°†ç³»ç»Ÿéƒ¨ç½²åˆ°çœŸå®çš„æœºå™¨äººä¸Šäº†ï¼Œç³»ç»Ÿä¼šè¿›å…¥ä»¥ä¸‹æ”¶çº³å¾ªç¯ï¼Œæ¯ä¸€å¾ªç¯æ”¶çº³ä¸€ä¸ªç‰©å“ï¼Œç›´åˆ°æ²¡æœ‰ç‰©å“å¯ä»¥æ”¶çº³ï¼š</li><li><ul><li>åˆ©ç”¨å¤–ç½®æ‘„åƒå¤´å¾—åˆ°åœ°æ¿çš„ä¿¯è§†å›¾ï¼Œé€šè¿‡ ViLD è¯†åˆ«å‡ºè·ç¦»æœºå™¨äººæœ€è¿‘çš„ç‰©ä½“ã€‚</li></ul></li><li><ul><li>æœºå™¨äººç§»åŠ¨åˆ°æ­¤ç‰©ä½“æ—ï¼Œé€šè¿‡å…¶è‡ªèº«çš„æ‘„åƒå¤´å¾—åˆ°ç‰©ä½“çš„è¿‘è·ç¦»ç…§ç‰‡ï¼Œå°†è¿‘è·ç¦»ç…§ç‰‡ä¸ $C$ å‘Šè¯‰ CLIPï¼Œè®©å…¶å¯¹ç‰©ä½“è¿›è¡Œåˆ†ç±»ï¼Œå¾—åˆ°ç±»åˆ« $c$ã€‚</li></ul></li><li><ul><li>è®© LLM æ ¹æ® $c, S_{receptacle}, S_{primitive}$ æ€»ç»“å‡ºç‰©ä½“è¯¥ <em>å¦‚ä½•</em> è¢«æ”¾ç½®åˆ° <em>ä½•å¤„</em>ã€‚</li></ul></li><li><ul><li>æœºå™¨äººæ‰§è¡Œç›¸åº”çš„æ”¶çº³åŠ¨ä½œã€‚</li></ul></li></ul><p><img src=/p/tidybot/real_system.png width=1525 height=377 srcset="/p/tidybot/real_system_hu18148537030116396249.png 480w, /p/tidybot/real_system_hu14811919430971523437.png 1024w" loading=lazy class=gallery-image data-flex-grow=404 data-flex-basis=970px></p><p>æ¶‰åŠåˆ° LLM çš„éƒ¨åˆ†ï¼Œå…·ä½“ prompt å¯ä»¥å‚é˜…åŸè®ºæ–‡ Appendix Aã€‚</p><h3 id=experiments>Experiments</h3><h4 id=benchmark-dataset>Benchmark Dataset</h4><p>ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„å¯é æ€§ï¼Œä½œè€…ä¸“é—¨åšäº†ä¸€ä¸ª benchmark datasetï¼Œå…¶ä¸­å…±åŒ…å« 96 ä¸ªä¸ªæ€§åŒ–åœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯é‡Œéƒ½æœ‰ä¸€äº›å®¹å™¨å’Œä¸€äº›ç‰©å“ï¼Œå…¶ä¸­æœ‰äº›ç‰©å“è¢«æ ‡æ³¨äº†åº”è¯¥è¢«æ”¾åˆ°ä»€ä¹ˆå®¹å™¨é‡Œï¼Œè€Œå¦ä¸€äº›ç‰©å“å¹¶æ²¡è¢«æ ‡æ³¨ã€‚æ³¨æ„ï¼Œæ¯ä¸ªåœºæ™¯å¯èƒ½ä»£è¡¨äº†ä¸åŒçš„æ”¶çº³å–œå¥½ï¼Œæ‰€ä»¥å¯¹äºåŒä¸€ä¸ªç‰©å“ï¼Œä¸åŒåœºæ™¯çš„æ”¶çº³å®¹å™¨å¯èƒ½å¤§ä¸ç›¸åŒã€‚ä»»åŠ¡çš„ç›®çš„å°±æ˜¯æ ¹æ®è¢«æ ‡æ³¨çš„ç‰©å“æ¥é¢„æµ‹æœªè¢«æ ‡æ³¨çš„ç‰©ä½“åº”è¯¥è¢«æ”¾åˆ°å“ªé‡Œã€‚</p><p>åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šï¼Œä½œè€…åšäº†ä¸€äº›å®éªŒï¼š<a class=link href=#baseline-comparisons># Baseline Comparisons</a>, <a class=link href=#ablation-studies># Ablation Studies</a>, <a class=link href=#human-evaluation># Human Evaluation</a>ã€‚</p><h4 id=baseline-comparisons>Baseline Comparisons</h4><p>è¿™éƒ¨åˆ†ï¼Œä½œè€…å°†è‡ªå·±çš„æ–¹æ³•ä¸ä¸€äº› baseline ä½œæ¯”è¾ƒï¼Œæ¯”å¦‚åªç»™ LLM æä¾›æ ‡æ³¨ç‰©ä½“ï¼Œç›´æ¥è®©å…¶é¢„æµ‹ä¸ºè¢«æ ‡æ³¨ç‰©ä½“è¯¥è¢«æ”¾åˆ°å“ªé‡Œï¼Œè€Œä¸ç»è¿‡ summarization è¿‡ç¨‹ï¼›å†æ¯”å¦‚åˆ©ç”¨ pre-trained text embeddingï¼Œå¯¹äºæœªæ ‡æ³¨çš„ç‰©ä½“ï¼Œç›´æ¥æ‰¾åˆ°ä¸å…¶ embedding è·ç¦»æœ€è¿‘çš„æ ‡æ³¨ç‰©ä½“ï¼Œè®¤ä¸ºäºŒè€…åº”è¯¥è¢«æ”¶çº³åˆ°åŒä¸€ä¸ªåœ°æ–¹ã€‚ç»“è®ºå°±æ˜¯ï¼Œä½œè€…çš„æ–¹æ³•èƒœè¿‡å…¶ä»– baselineã€‚</p><p><img src=/p/tidybot/baseline.png width=1041 height=534 srcset="/p/tidybot/baseline_hu2260679436813115871.png 480w, /p/tidybot/baseline_hu7433888565880427837.png 1024w" loading=lazy class=gallery-image data-flex-grow=194 data-flex-basis=467px></p><h4 id=ablation-studies>Ablation Studies</h4><p>ä½œè€…ä¸€å…±åšäº†ä¸‰ä¸ªæ–¹é¢çš„ ablation studiesï¼š</p><ul><li><ol><li>ä¸åˆ©ç”¨ user specific preferenceï¼Œç›´æ¥è®© LLM ä¾æ® commonsense æ¥æ¨æ–­ç‰©å“åº”è¯¥è¢«æ”¾åˆ°å“ªé‡Œã€‚</li></ol></li><li><ol start=2><li>è®©äººç±»æ¥è¿›è¡Œ summarizationï¼Œä¸ç”¨ LLM åšã€‚</li></ol></li><li><ol start=3><li>æ¯”è¾ƒé‡‡ç”¨ä¸åŒ LLM çš„å‡†ç¡®ç‡ã€‚</li></ol></li></ul><p><img src=/p/tidybot/ablation_studies_1.png width=658 height=291 srcset="/p/tidybot/ablation_studies_1_hu9791429553669372484.png 480w, /p/tidybot/ablation_studies_1_hu16712101575763368406.png 1024w" loading=lazy class=gallery-image data-flex-grow=226 data-flex-basis=542px> <img src=/p/tidybot/ablation_studies_2.png width=799 height=384 srcset="/p/tidybot/ablation_studies_2_hu4240727006035632664.png 480w, /p/tidybot/ablation_studies_2_hu11499249390621271082.png 1024w" loading=lazy class=gallery-image data-flex-grow=208 data-flex-basis=499px></p><p>ç»“è®ºæ˜¯ï¼Œè®© LLM è¿›è¡Œ summarization ä¼šæ¯”ç›´æ¥ç”¨ commonsense æœ‰éå¸¸å¤§çš„æå‡ï¼Œä½†ç›¸è¾ƒäºç›´æ¥è®©äººç±»è¿›è¡Œ summarization ä»æœ‰ä¸è¶³ã€‚è¿™ä¹Ÿè¯´æ˜é€šè¿‡æå‡ LLM çš„æ€»ç»“èƒ½åŠ›è¿˜èƒ½è¿›ä¸€æ­¥æå‡æ­¤ç³»ç»Ÿçš„èƒ½åŠ›ã€‚</p><p>å¦ä¸€æ–¹é¢ï¼Œåœ¨ä¸åŒçš„ LLM ä¸­ï¼Œtext-davinci-003 æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚</p><h4 id=human-evaluation>Human Evaluation</h4><p>ä½œè€…è¿˜æ‹›å‹Ÿäº†ä¸€äº›å¿—æ„¿è€…ï¼Œå‘ä»–ä»¬æä¾› user preferenceã€baseline ç»™å‡ºçš„æ”¶çº³å»ºè®®ã€è‡ªå·±æ–¹æ³•ç»™å‡ºçš„æ”¶çº³å»ºè®®ï¼Œè®©ä»–ä»¬æ¯”è¾ƒè‡ªå·±çš„æ–¹æ³•ä¸ baseline çš„ç»“æœï¼Œå“ªä¸ªæ›´ç¬¦åˆ user preferenceã€‚é¢˜ç›®å½¢å¼å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src=/p/tidybot/question.png width=744 height=547 srcset="/p/tidybot/question_hu15245168944330260646.png 480w, /p/tidybot/question_hu16038960857171736720.png 1024w" loading=lazy class=gallery-image data-flex-grow=136 data-flex-basis=326px></p><p>ç»“æœæ˜¾ç¤ºï¼Œä½œè€…çš„æ–¹æ³•æœ‰ 46.9% çš„æƒ…å†µè¢«è®¤ä¸ºæ›´å¥½ï¼Œè€Œ baseline åªæœ‰ 19.1% çš„æƒ…å†µè¢«è®¤ä¸ºæ›´å¥½ã€‚</p><p><img src=/p/tidybot/human_evaluation.png width=1604 height=275 srcset="/p/tidybot/human_evaluation_hu12242310108710729194.png 480w, /p/tidybot/human_evaluation_hu7342609013758737524.png 1024w" loading=lazy class=gallery-image data-flex-grow=583 data-flex-basis=1399px></p><h4 id=real-world-experiments>Real-world Experiments</h4><p>æ­£å¦‚ <a class=link href=#method># Method</a> ä¸­è¯´çš„ï¼Œä½œè€…è¿˜æ­å»ºäº†ä¸€ä¸ªçœŸå®çš„æœºå™¨äººå¹³å°ï¼Œè®©æ–‡ç« ä¸­æå‡ºçš„ä¸ªæ€§åŒ–æ”¶çº³æ–¹æ³•èƒ½å¤Ÿè½åœ°ã€‚ä½œè€…æ„é€ äº† 8 ä¸ªçœŸå®åœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å«ä¸€äº›æ•£è½åœ¨åœ°ä¸Šçš„ç‰©å“ä»¥åŠå‡ ä¸ªæ”¶çº³å®¹å™¨ï¼Œç„¶åè®©ç³»ç»Ÿæ ¹æ® <a class=link href=#method># Method</a> ä¸­çš„ pipeline è¿è¡Œã€‚ç»“æœæ˜¾ç¤ºï¼Œç³»ç»Ÿåœ¨ 85.0% æƒ…å†µä¸‹éƒ½èƒ½æ­£ç¡®å®Œæˆæ”¶çº³ä»»åŠ¡ã€‚</p><p><img src=/p/tidybot/real.png width=1671 height=592 srcset="/p/tidybot/real_hu16826072460319586654.png 480w, /p/tidybot/real_hu17600697350256475454.png 1024w" loading=lazy class=gallery-image data-flex-grow=282 data-flex-basis=677px></p><h2 id=language-conditioned-mobile-manipulation>Language-Conditioned Mobile Manipulation</h2><p>TidyBot å±äºä¸€ä¸ªæ›´å¹¿çš„ç ”ç©¶é¢†åŸŸ Language-Conditioned Mobile Manipulationã€‚è¿™ä¸ªé¢†åŸŸå°† CVã€NLPã€Robotics ç»“åˆäº†èµ·æ¥ï¼Œè¦æ±‚æœºå™¨äººèƒ½å¤Ÿæ ¹æ®äººç±»çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤å»åšå‡ºç›¸åº”çš„è¡Œä¸ºã€‚</p><div align=center><img src=cross_field.png width=50%></div><p>è¿™ç¯‡æ–‡ç« ä¸º Language-Conditioned Mobile Manipulation é¢†åŸŸåšäº†ä¸ªè¯¦ç»†çš„è°ƒæŸ¥ï¼š</p><ul><li>è®ºæ–‡åœ°å€ï¼š<a class=link href=https://arxiv.org/pdf/2312.10807 target=_blank rel=noopener>https://arxiv.org/pdf/2312.10807</a></li><li>å¼€æºä»£ç ï¼š<a class=link href=https://github.com/hk-zh/language-conditioned-robot-manipulation-models target=_blank rel=noopener>https://github.com/hk-zh/language-conditioned-robot-manipulation-models</a></li></ul><h3 id=architecture-framework>Architecture Framework</h3><p><img src=/p/tidybot/framework.png width=1425 height=934 srcset="/p/tidybot/framework_hu8973135417564341417.png 480w, /p/tidybot/framework_hu14937730675035186088.png 1024w" loading=lazy class=gallery-image data-flex-grow=152 data-flex-basis=366px></p><p>æ­¤é¢†åŸŸå·¥ä½œçš„æ€»ä½“æ¡†æ¶å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—</p><ul><li>è¯­è¨€æ¨¡å—ã€‚å…¶ä¸»è¦ä½œç”¨æ˜¯ç†è§£ç”¨æˆ·çš„è¯­è¨€è¾“å…¥ï¼Œå¹¶è½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œæŒ‡å¯¼ã€‚å¦‚ TidyBot ä¸­ï¼Œç”¨æˆ·å‘Šè¯‰ç³»ç»Ÿæ”¶çº³ preferenceï¼Œè¯­è¨€æ¨¡å—å°±ä¼šè¿›è¡Œå¤„ç†ï¼Œè¿›è¡Œ preference çš„æ€»ç»“ç­‰ã€‚</li><li>æ„ŸçŸ¥æ¨¡å—ã€‚å…¶ä¸»è¦ä½œç”¨æ˜¯æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒï¼Œä¾‹å¦‚ TidyBot ä¸­æœºå™¨äººåˆ©ç”¨è‡ªèº«çš„ç›¸æœºå»è¯†åˆ«ç‰©ä½“è¿›è¡Œåˆ†ç±»ã€‚</li><li>æ§åˆ¶æ¨¡å—ã€‚å…¶ä¸»è¦ä½œç”¨æ˜¯è®©æœºå™¨äººæ‰§è¡Œéœ€è¦æ‰§è¡Œçš„æŒ‡ä»¤ã€‚å¯¹åº”åˆ° TidyBot ä¸­ï¼Œå°±æ˜¯æœºå™¨äººå»æ‰§è¡Œ â€œç§»åŠ¨åˆ°æŸå¤„â€ã€â€œæ‹¿èµ·åœ°ä¸Šçš„ç‰©å“â€ã€â€œæŠŠç‰©å“æ”¾ç½®åˆ°æŸå¤„â€ ç­‰ã€‚åœ¨ TidyBot ä¸­ï¼Œè¿™æ ·çš„åŠ¨ä½œæ˜¯ hard-coded çš„ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥é‡‡ç”¨ reinforcement learning (RL), imitation learing (IL) ç­‰æ–¹æ³•å¾—åˆ°ã€‚</li></ul><h3 id=approaches-categorization>Approaches Categorization</h3><p>Language-Conditioned Mobile Manipulation çš„å·¥ä½œä¸»è¦å¯ä»¥è¢«ç²—ç•¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š</p><ul><li>Language-conditioned Reinforcement Learning</li><li>Language-conditioned Imitation Learning</li><li>Empowered by LLMs & VLMs</li></ul><p>å½“ç„¶ï¼Œæœ‰äº›å·¥ä½œå¯èƒ½å¯ä»¥è¢«åŒæ—¶åˆ’åˆ†åˆ°å¤šç§ç±»åˆ«ä¸­ã€‚å…¶ä¸­ï¼Œå‰ä¸¤ç§æ–¹æ³•è¾ƒä¸ºä¼ ç»Ÿï¼Œæ²¡æœ‰é‡‡ç”¨å¤§è¯­è¨€æ¨¡å‹ç­‰ç°æˆå·¥å…·ã€‚ç¬¬ä¸‰ç§æ–¹æ³•åˆ©ç”¨ç°æˆçš„ LLMs ä¸ VLMsï¼Œç®€åŒ–äº†ç³»ç»Ÿï¼Œæé«˜äº†èƒ½åŠ›ã€‚</p><p><img src=/p/tidybot/categories.png width=1955 height=1212 srcset="/p/tidybot/categories_hu14519433019373800883.png 480w, /p/tidybot/categories_hu12373372102478987912.png 1024w" loading=lazy class=gallery-image data-flex-grow=161 data-flex-basis=387px></p><h4 id=language-conditioned-reinforcement-learning>Language-conditioned Reinforcement Learning</h4><p>æ­¤ç±»å·¥ä½œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡äººä¸ºè®¾è®¡ç­‰æ–¹æ³•ï¼Œå»ºç«‹ä¸€ä¸ªä»è‡ªç„¶è¯­è¨€åˆ° reward çš„ä¸€ä¸ªæ˜ å°„ï¼Œå½“ agent è¾¾åˆ°è‡ªç„¶è¯­è¨€æè¿°çš„ç›®æ ‡æ—¶ï¼Œå®ƒå°±èƒ½å¾—åˆ°å¯¹åº”çš„ rewardã€‚Agent åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å¯ä»¥å­¦ä¹ åˆ°ä¸€ä¸ªä»è‡ªç„¶è¯­è¨€åˆ°å…·ä½“åŠ¨ä½œçš„æ˜ å°„ã€‚å…·ä½“å·¥ä½œæœ‰ï¼š</p><ul><li>Lancon-learn: Learning with language to enable generalization in multi-task manipulation <a class=link href=https://ieeexplore.ieee.org/document/9667188 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/hartikainen/metaworld/tree/reward-tweaks-rebase target=_blank rel=noopener>[code]</a></li><li>Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards <a class=link href=https://proceedings.mlr.press/v155/goyal21a.html target=_blank rel=noopener>[paper]</a><a class=link href=https://github.com/prasoongoyal/PixL2R target=_blank rel=noopener>[code]</a></li><li>Learning from symmetry: Meta-reinforcement learning with symmetrical behaviors and language instructions <a class=link href=https://arxiv.org/abs/2209.10656 target=_blank rel=noopener>[paper]</a><a class=link href=https://tumi6robot.wixsite.com/symmetry/ target=_blank rel=noopener>[website]</a></li><li>Meta-reinforcement learning via language instructions <a class=link href=https://arxiv.org/abs/2209.04924 target=_blank rel=noopener>[paper]</a><a class=link href=https://github.com/yaoxt3/MILLION target=_blank rel=noopener>[code]</a><a class=link href=https://tumi6robot.wixsite.com/million target=_blank rel=noopener>[website]</a></li><li>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation <a class=link href=https://proceedings.mlr.press/v164/nair22a/nair22a.pdf target=_blank rel=noopener>[paper]</a></li><li>Concept2robot: Learning manipulation concepts from instructions and human demonstrations <a class=link href=https://www.roboticsproceedings.org/rss16/p082.pdf target=_blank rel=noopener>[paper]</a></li></ul><h4 id=language-conditioned-imitation-learning>Language-conditioned Imitation Learning</h4><p>æ­¤ç±»å·¥ä½œåˆ©ç”¨æ¨¡ä»¿å­¦ä¹ çš„èŒƒå¼ï¼Œå…¶ä¸åƒå¼ºåŒ–å­¦ä¹ é‚£æ ·è¦æ±‚æä¾› rewardï¼Œä½†æ˜¯éœ€è¦æä¾›ä¸€äº›æ­£ç¡®çš„è¡Œä¸ºä¾‹å­ (expert demonstrations)ï¼Œagent ä¼šæ ¹æ®è¿™äº›æ­£ç¡®çš„è¡Œä¸ºè¿›è¡Œå­¦ä¹ ã€‚å…·ä½“å¯ä»¥å†è¢«ç»†åˆ†ä¸º behavior cloning (BC) å’Œ inverse reinforcement learning (IRL)ã€‚</p><p>BC å°±æ˜¯ç›´æ¥ä¾æ ·ç”»è‘«èŠ¦ï¼Œexpert demonstrations é‡Œæ€ä¹ˆåšï¼Œagent å°±æ€ä¹ˆåšã€‚å…·ä½“å·¥ä½œæœ‰ï¼š</p><ul><li>Language conditioned imitation learning over unstructured data <a class=link href=https://arxiv.org/abs/2005.07648 target=_blank rel=noopener>[paper]</a> <a class=link href>[code]</a> <a class=link href=https://language-play.github.io/ target=_blank rel=noopener>[website]</a></li><li>Bc-z: Zero-shot task generalization with robotic imitation learning <a class=link href=https://arxiv.org/abs/2202.02005 target=_blank rel=noopener>[paper]</a></li><li>What matters in language-conditioned robotic imitation learning over unstructured data <a class=link href=https://arxiv.org/abs/2204.06252 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/lukashermann/hulc target=_blank rel=noopener>[code]</a><a class=link href=http://hulc.cs.uni-freiburg.de/ target=_blank rel=noopener>[website]</a></li><li>Grounding language with visual affordances over unstructured data <a class=link href=https://arxiv.org/abs/2210.01911 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/mees/hulc2 target=_blank rel=noopener>[code]</a><a class=link href=http://hulc2.cs.uni-freiburg.de/ target=_blank rel=noopener>[website]</a></li><li>Language-conditioned imitation learning with base skill priors under unstructured data <a class=link href=https://arxiv.org/abs/2305.19075 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/hk-zh/spil target=_blank rel=noopener>[code]</a> <a class=link href=https://hk-zh.github.io/spil/ target=_blank rel=noopener>[website]</a></li><li>Pay attention!- robustifying a deep visuomotor policy through task-focused visual attention <a class=link href=https://arxiv.org/abs/1809.10093 target=_blank rel=noopener>[paper]</a></li><li>Language-conditioned imitation learning for robot manipulation tasks <a class=link href=https://arxiv.org/abs/2010.12083 target=_blank rel=noopener>[paper]</a></li></ul><p>IRL åˆ™éœ€è¦ç»å†ä¸¤ä¸ªæ­¥éª¤ï¼Œç¬¬ä¸€æ­¥å…ˆä» expert demonstrations å’Œè¯­è¨€å‘½ä»¤ä¸­å­¦ä¹ ä¸€ä¸ªä»è‡ªç„¶è¯­è¨€å‘½ä»¤åˆ° reward çš„æ˜ å°„ï¼Œå†é€šè¿‡ RL çš„æ–¹å¼å­¦ä¹ è¡Œä¸ºç­–ç•¥ï¼ˆè¿™æ ·çœ‹æ¥ï¼Œæ­¤éƒ¨åˆ†ä¸ <a class=link href=#language-conditioned-reinforcement-learning># Language-conditioned Reinforcement Learning</a> ä¹Ÿæœ‰äº¤é›†ï¼‰ã€‚å…·ä½“å·¥ä½œï¼š</p><ul><li>Grounding english commands to reward function <a class=link href=https://www.roboticsproceedings.org/rss11/p18.pdf target=_blank rel=noopener>[paper]</a></li><li>From language to goals: Inverse reinforcement learning for vision-based instruction following <a class=link href=https://arxiv.org/abs/1902.07742 target=_blank rel=noopener>[paper]</a></li></ul><h4 id=empowered-by-llms--vlms>Empowered by LLMs & VLMs</h4><p>å‰ä¸¤ç±»æ–¹æ³•å‡éœ€è¦å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œå­¦ä¹ ï¼Œè€Œæœ‰äº† LLM è¿™æ ·å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œå°±å¯ä»¥å¯¹ç³»ç»Ÿè¿›è¡Œç®€åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œå¯ä»¥åˆ©ç”¨å¥½å¤§è¯­è¨€æ¨¡å‹çš„ planning å’Œ reasoning èƒ½åŠ›ã€‚</p><p>å¤§è¯­è¨€æ¨¡å‹çš„ planning èƒ½åŠ›æŒ‡çš„æ˜¯å…¶å°†å¤æ‚ä»»åŠ¡è½¬åŒ–ä¸ºä¸€ç³»åˆ—ç®€å•çš„ã€æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚è­¬å¦‚è¦æ±‚æœºå™¨äººç‚’èœï¼Œç›´æ¥å­¦ä¹ ä¸€ä¸ªç‚’èœçš„ç­–ç•¥æ˜¯éå¸¸éš¾çš„ï¼Œä½†å¯ä»¥å…ˆè®© LLM å°†ç‚’èœçš„åŠ¨ä½œæ‹†åˆ†æˆ æ´—èœã€æ”¾æ²¹ã€æ”¾èœ ç­‰ä¸€ç³»åˆ—ç®€å•çš„ã€æœºå™¨äººèƒ½å¤Ÿå­¦ä¼šçš„åŠ¨ä½œï¼Œæ­¤æ—¶å†è®©æœºå™¨äººå»æ‰§è¡Œè¿™äº›åŠ¨ä½œå°±èƒ½å®Œæˆç‚’èœçš„ä»»åŠ¡äº†ã€‚å…·ä½“å·¥ä½œæœ‰ï¼š</p><ul><li>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning <a class=link href=https://arxiv.org/abs/2307.06135 target=_blank rel=noopener>[paper]</a></li><li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <a class=link href=https://arxiv.org/abs/2201.07207 target=_blank rel=noopener>[paper]</a></li><li>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents <a class=link href=https://arxiv.org/abs/2302.01560 target=_blank rel=noopener>[paper]</a></li><li>Progprompt: Generating situated robot task plans using large language models <a class=link href=https://arxiv.org/abs/2209.11302 target=_blank rel=noopener>[paper]</a></li><li>Robots that ask for help: Uncertainty alignment for large language model planners <a class=link href=https://arxiv.org/abs/2307.01928 target=_blank rel=noopener>[paper]</a></li><li>Task and motion planning with large language models for object rearrangement <a class=link href=https://arxiv.org/abs/2303.06247 target=_blank rel=noopener>[paper]</a></li><li>Do as i can, not as i say: Grounding language in robotic affordances <a class=link href=https://arxiv.org/abs/2204.01691 target=_blank rel=noopener>[paper]</a></li><li>The 2014 international planning competition: Progress and trends <a class=link href=https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2571 target=_blank rel=noopener>[paper]</a></li><li>Robot task planning via deep reinforcement learning: a tabletop object sorting application <a class=link href=https://ieeexplore.ieee.org/document/8914278 target=_blank rel=noopener>[paper]</a></li><li>Robot task planning and situation handling in open worlds <a class=link href=https://arxiv.org/abs/2210.01287 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/yding25/GPT-Planner target=_blank rel=noopener>[code]</a> <a class=link href=https://cowplanning.github.io/ target=_blank rel=noopener>[website]</a></li><li>Embodied Task Planning with Large Language Models <a class=link href=https://arxiv.org/abs/2307.01848 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/Gary3410/TaPA target=_blank rel=noopener>[code]</a> <a class=link href=https://gary3410.github.io/TaPA/ target=_blank rel=noopener>[website]</a></li><li>Text2motion: From natural language instructions to feasible plans <a class=link href=https://arxiv.org/abs/2303.12153 target=_blank rel=noopener>[paper]</a> <a class=link href=https://sites.google.com/stanford.edu/text2motion target=_blank rel=noopener>[website]</a></li><li>Large language models as commonsense knowledge for large-scale task planning <a class=link href=https://arxiv.org/abs/2305.14078 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/1989Ryan/llm-mcts target=_blank rel=noopener>[code]</a> <a class=link href=https://llm-mcts.github.io/ target=_blank rel=noopener>[website]</a></li><li>Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation <a class=link href=https://arxiv.org/abs/2305.18898 target=_blank rel=noopener>[paper]</a></li><li>Learning to reason over scene graphs: a case study of finetuning gpt-2 into a robot language model for grounded task planning <a class=link href=https://www.frontiersin.org/articles/10.3389/frobt.2023.1221739/full target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/dnandha/RobLM target=_blank rel=noopener>[code]</a></li><li>Scaling up and distilling down: Language-guided robot skill acquisition <a class=link href=https://arxiv.org/abs/2307.14535 target=_blank rel=noopener>[paper]</a><a class=link href=https://github.com/real-stanford/scalingup target=_blank rel=noopener>[code]</a> <a class=link href=https://www.cs.columbia.edu/~huy/scalingup/ target=_blank rel=noopener>[website]</a></li><li>Stap: Sequencing task-agnostic policies <a class=link href=https://ieeexplore.ieee.org/document/10160220 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/agiachris/STAP target=_blank rel=noopener>[code]</a><a class=link href=https://sites.google.com/stanford.edu/stap/home target=_blank rel=noopener>[website]</a></li><li>Inner monologue: Embodied reasoning through planning with language models <a class=link href=https://arxiv.org/abs/2207.05608 target=_blank rel=noopener>[paper]</a> <a class=link href=https://innermonologue.github.io/ target=_blank rel=noopener>[website]</a></li></ul><p>å¤§è¯­è¨€æ¨¡å‹çš„ reasoning èƒ½åŠ›å°±åƒ TidyBot ä¸­å±•ç¤ºçš„é‚£æ ·ï¼Œåˆ©ç”¨ LLM å»æ¨ç†æŸä¸ªç‰©å“åº”è¯¥è¢«æ”¾ç½®åˆ°ä½•å¤„ï¼Œå†è®©æœºå™¨äººå»æ‰§è¡Œç‰¹å®šçš„ç­–ç•¥ã€‚å…·ä½“å·¥ä½œæœ‰ï¼š</p><ul><li>Rearrangement:A challenge for embodied ai <a class=link href=https://arxiv.org/abs/2011.01975 target=_blank rel=noopener>[paper]</a></li><li>The threedworld transport challenge: A visually guided task and motion planning benchmark for physically realistic embodied ai <a class=link href=https://ieeexplore.ieee.org/document/9812329 target=_blank rel=noopener>[paper]</a></li><li>Tidy up my room: Multi-agent cooperation for service tasks in smart environments <a class=link href=https://dl.acm.org/doi/abs/10.3233/AIS-190524 target=_blank rel=noopener>[paper]</a></li><li>A quantifiable stratification strategy for tidy-up in service robotics <a class=link href=https://ieeexplore.ieee.org/document/9542842 target=_blank rel=noopener>[paper]</a></li><li>Tidybot: Personalized robot assistance with large language models <a class=link href=https://arxiv.org/abs/2305.05658 target=_blank rel=noopener>[paper]</a></li><li>Housekeep: Tidying virtual households using commonsense reasoning <a class=link href=https://arxiv.org/abs/2205.10712 target=_blank rel=noopener>[paper]</a></li><li>Building cooperative embodied agents modularly with large language models <a class=link href=https://arxiv.org/abs/2307.02485 target=_blank rel=noopener>[paper]</a></li><li>Socratic models: Composing zero-shot multimodal reasoning with language <a class=link href=https://arxiv.org/abs/2204.00598 target=_blank rel=noopener>[paper]</a></li><li>Voyager: An open-ended embodied agent with large language models <a class=link href=https://arxiv.org/abs/2305.16291 target=_blank rel=noopener>[paper]</a></li><li>Translating natural language to planning goals with large-language models <a class=link href=https://arxiv.org/abs/2302.05128 target=_blank rel=noopener>[paper]</a></li></ul><p>åœ¨è‡ªç„¶è¯­è¨€çš„åŸºç¡€ä¸Šï¼Œå¯ä»¥å†åŠ ä¸Šè§†è§‰å·¥å…·ï¼Œæ¯”å¦‚ TidyBot ä¸­è¯†åˆ«ç‰©ä½“çš„éƒ¨åˆ†ã€‚åˆ©ç”¨äº† VLMs çš„å…·ä½“å·¥ä½œæœ‰ï¼š</p><ul><li>Cliport: What and where pathways for robotic manipulation <a class=link href=https://arxiv.org/abs/2109.12098 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/cliport/cliport target=_blank rel=noopener>[code]</a> <a class=link href=https://cliport.github.io/ target=_blank rel=noopener>[website]</a></li><li>Transporter networks: Rearranging the visual world for robotic manipulation <a class=link href=https://proceedings.mlr.press/v155/zeng21a/zeng21a.pdf target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/google-research/ravens target=_blank rel=noopener>[code]</a> <a class=link href=https://transporternets.github.io/ target=_blank rel=noopener>[website]</a></li><li>Simple but effective: Clip embeddings for embodied ai <a class=link href=https://openaccess.thecvf.com/content/CVPR2022/papers/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.pdf target=_blank rel=noopener>[paper]</a></li><li>Instruct2act: Mapping multi-modality instructions to robotic actions with large language model <a class=link href=https://arxiv.org/abs/2305.11176 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/OpenGVLab/Instruct2Act target=_blank rel=noopener>[code]</a></li><li>Latte: Language trajectory transformer <a class=link href=https://arxiv.org/abs/2208.02918 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr target=_blank rel=noopener>[code]</a></li><li>Embodied Task Planning with Large Language Models <a class=link href=https://arxiv.org/abs/2307.01848 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/Gary3410/TaPA target=_blank rel=noopener>[code]</a> <a class=link href=https://gary3410.github.io/TaPA/ target=_blank rel=noopener>[website]</a></li><li>Palm-e: An embodied multimodal language model <a class=link href=https://arxiv.org/abs/2303.03378 target=_blank rel=noopener>[paper]</a> <a class=link href=https://palm-e.github.io/ target=_blank rel=noopener>[website]</a></li><li>Socratic models: Composing zero-shot multimodal reasoning with language <a class=link href=https://arxiv.org/abs/2204.00598 target=_blank rel=noopener>[paper]</a></li><li>Pretrained language models as visual planners for human assistance <a class=link href=https://openaccess.thecvf.com/content/ICCV2023/papers/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.pdf target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/facebookresearch/vlamp target=_blank rel=noopener>[code]</a></li><li>Open-world object manipulation using pre-trained vision-language models <a class=link href=https://arxiv.org/abs/2303.00905 target=_blank rel=noopener>[paper]</a> <a class=link href=https://robot-moo.github.io/ target=_blank rel=noopener>[website]</a></li><li>Robotic skill acquisition via instruction augmentation with vision-language models <a class=link href=https://arxiv.org/abs/2211.11736 target=_blank rel=noopener>[paper]</a> <a class=link href=https://instructionaugmentation.github.io/ target=_blank rel=noopener>[website]</a></li><li>Language reward modulation for pretraining reinforcement learning <a class=link href=https://arxiv.org/abs/2308.12270 target=_blank rel=noopener>[paper]</a> <a class=link href=https://github.com/ademiadeniji/lamp target=_blank rel=noopener>[code]</a></li><li>Vision-language models as success detectors <a class=link href=https://proceedings.mlr.press/v232/du23b.html target=_blank rel=noopener>[paper]</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/dl/>DL</a>
<a href=/tags/nlp/>NLP</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>æœ€åæ›´æ–°äº May 19, 2024 14:00 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/gedl-notes-1.7/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.7</h2></div></a></article><article><a href=/p/gedl-notes-1.6/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.6</h2></div></a></article><article><a href=/p/gedl-notes-1.5/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.5</h2></div></a></article><article><a href=/p/gedl-notes-1.4/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.4</h2></div></a></article><article><a href=/p/gedl-notes-1.3/><div class=article-details><h2 class=article-title>Group Equivariant Deep Learning Lecture 1.3</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=SUZ-tsinghua/SUZ-tsinghua.github.io issue-term=title crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2024 suz</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>